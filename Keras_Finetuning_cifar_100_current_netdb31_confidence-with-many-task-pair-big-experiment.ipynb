{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning `VGGNet` with `keras`\n",
    "\n",
    "This notebook demonstrates how to create a single pipeline to resize, preprocess, and augment images in real time during training with `keras`. This eliminates the need to preprocess images and write them back to disk before doing fine-tuning. This is accomplished by hooking together `keras.preprocessing.image.ImageDataGenerator` with `keras.applications.vgg16.preprocess_input()` via the former's (undocumented) `preprocessing_function` argument with a small wrapper function.  \n",
    "\n",
    "This notebook modifies [this tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) (full code [here](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)).\n",
    "\n",
    "This approach can be generalized to all keras [pretrained models](https://keras.io/applications/). For example, for `keras.applications.resnet50.ResNet50`, you would simply swap out `keras.applications.vgg16.preprocess_input()` for `keras.applications.resnet50.preprocess_input()`.\n",
    "\n",
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Image manipulation.\n",
    "import PIL.Image\n",
    "# import Image\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "# import inception5h\n",
    "# inception5h.data_dir = 'inception/5h/'\n",
    "# inception5h.maybe_download()\n",
    "# model \n",
    "def load_image(filename):\n",
    "    image = PIL.Image.open(filename)\n",
    "    return np.float32(image)\n",
    "\n",
    "def save_image(image, filename):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    \n",
    "    # Convert to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Write the image-file in jpeg-format.\n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')\n",
    "        \n",
    "def plot_image(image):\n",
    "    # Assume the pixel-values are scaled between 0 and 255.\n",
    "    \n",
    "    if False:\n",
    "        # Convert the pixel-values to the range between 0.0 and 1.0\n",
    "        image = np.clip(image/255.0, 0.0, 1.0)\n",
    "        \n",
    "        # Plot using matplotlib.\n",
    "        plt.imshow(image, interpolation='lanczos')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Ensure the pixel-values are between 0 and 255.\n",
    "        image = np.clip(image, 0.0, 255.0)\n",
    "        \n",
    "        # Convert pixels to bytes.\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # Convert to a PIL-image and display it.\n",
    "        display(PIL.Image.fromarray(image))\n",
    "        \n",
    "def normalize_image(x):\n",
    "    # Get the min and max values for all pixels in the input.\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "\n",
    "    # Normalize so all values are between 0.0 and 1.0\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "    \n",
    "    return x_norm\n",
    "\n",
    "def plot_gradient(gradient):\n",
    "    # Normalize the gradient so it is between 0.0 and 1.0\n",
    "    gradient_normalized = normalize_image(gradient)\n",
    "    \n",
    "    # Plot the normalized gradient.\n",
    "    plt.imshow(gradient_normalized, interpolation='bilinear')\n",
    "    plt.show()\n",
    "    \n",
    "def resize_image(image, size=None, factor=None):\n",
    "    # If a rescaling-factor is provided then use it.\n",
    "    if factor is not None:\n",
    "        # Scale the numpy array's shape for height and width.\n",
    "        size = np.array(image.shape[0:2]) * factor\n",
    "        \n",
    "        # The size is floating-point because it was scaled.\n",
    "        # PIL requires the size to be integers.\n",
    "        size = size.astype(int)\n",
    "    else:\n",
    "        # Ensure the size has length 2.\n",
    "        size = size[0:2]\n",
    "    \n",
    "    # The height and width is reversed in numpy vs. PIL.\n",
    "    size = tuple(reversed(size))\n",
    "\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    img = np.clip(image, 0.0, 255.0)\n",
    "    \n",
    "    # Convert the pixels to 8-bit bytes.\n",
    "    img = img.astype(np.uint8)\n",
    "    \n",
    "    # Create PIL-object from numpy array.\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    \n",
    "    # Resize the image.\n",
    "    img_resized = img.resize(size, PIL.Image.LANCZOS)\n",
    "    \n",
    "    # Convert 8-bit pixel values back to floating-point.\n",
    "    img_resized = np.float32(img_resized)\n",
    "\n",
    "    return img_resized\n",
    "\n",
    "def get_tile_size(num_pixels, tile_size=400):\n",
    "    \"\"\"\n",
    "    num_pixels is the number of pixels in a dimension of the image.\n",
    "    tile_size is the desired tile-size.\n",
    "    \"\"\"\n",
    "\n",
    "    # How many times can we repeat a tile of the desired size.\n",
    "    num_tiles = int(round(num_pixels / tile_size))\n",
    "    \n",
    "    # Ensure that there is at least 1 tile.\n",
    "    num_tiles = max(1, num_tiles)\n",
    "    \n",
    "    # The actual tile-size.\n",
    "    actual_tile_size = math.ceil(num_pixels / num_tiles)\n",
    "    \n",
    "    return actual_tile_size\n",
    "\n",
    "def tiled_gradient(gradient, image, tile_size=400):\n",
    "    # Allocate an array for the gradient of the entire image.\n",
    "    grad = np.zeros_like(image)\n",
    "\n",
    "    # Number of pixels for the x- and y-axes.\n",
    "    x_max, y_max, _ = image.shape\n",
    "\n",
    "    # Tile-size for the x-axis.\n",
    "    x_tile_size = get_tile_size(num_pixels=x_max, tile_size=tile_size)\n",
    "    # 1/4 of the tile-size.\n",
    "    x_tile_size4 = x_tile_size // 4\n",
    "\n",
    "    # Tile-size for the y-axis.\n",
    "    y_tile_size = get_tile_size(num_pixels=y_max, tile_size=tile_size)\n",
    "    # 1/4 of the tile-size\n",
    "    y_tile_size4 = y_tile_size // 4\n",
    "\n",
    "    # Random start-position for the tiles on the x-axis.\n",
    "    # The random value is between -3/4 and -1/4 of the tile-size.\n",
    "    # This is so the border-tiles are at least 1/4 of the tile-size,\n",
    "    # otherwise the tiles may be too small which creates noisy gradients.\n",
    "    x_start = random.randint(-3*x_tile_size4, -x_tile_size4)\n",
    "\n",
    "    while x_start < x_max:\n",
    "        # End-position for the current tile.\n",
    "        x_end = x_start + x_tile_size\n",
    "        \n",
    "        # Ensure the tile's start- and end-positions are valid.\n",
    "        x_start_lim = max(x_start, 0)\n",
    "        x_end_lim = min(x_end, x_max)\n",
    "\n",
    "        # Random start-position for the tiles on the y-axis.\n",
    "        # The random value is between -3/4 and -1/4 of the tile-size.\n",
    "        y_start = random.randint(-3*y_tile_size4, -y_tile_size4)\n",
    "\n",
    "        while y_start < y_max:\n",
    "            # End-position for the current tile.\n",
    "            y_end = y_start + y_tile_size\n",
    "\n",
    "            # Ensure the tile's start- and end-positions are valid.\n",
    "            y_start_lim = max(y_start, 0)\n",
    "            y_end_lim = min(y_end, y_max)\n",
    "\n",
    "            # Get the image-tile.\n",
    "            img_tile = image[x_start_lim:x_end_lim,\n",
    "                             y_start_lim:y_end_lim, :]\n",
    "\n",
    "            # Create a feed-dict with the image-tile.\n",
    "            feed_dict = model.create_feed_dict(image=img_tile)\n",
    "\n",
    "            # Use TensorFlow to calculate the gradient-value.\n",
    "            g = session.run(gradient, feed_dict=feed_dict)\n",
    "\n",
    "            # Normalize the gradient for the tile. This is\n",
    "            # necessary because the tiles may have very different\n",
    "            # values. Normalizing gives a more coherent gradient.\n",
    "            g /= (np.std(g) + 1e-8)\n",
    "\n",
    "            # Store the tile's gradient at the appropriate location.\n",
    "            grad[x_start_lim:x_end_lim,\n",
    "                 y_start_lim:y_end_lim, :] = g\n",
    "            \n",
    "            # Advance the start-position for the y-axis.\n",
    "            y_start = y_end\n",
    "\n",
    "        # Advance the start-position for the x-axis.\n",
    "        x_start = x_end\n",
    "\n",
    "    return grad\n",
    "\n",
    "def optimize_image(layer_tensor, image,\n",
    "                   num_iterations=10, step_size=3.0, tile_size=400,\n",
    "                   show_gradient=False):\n",
    "    \"\"\"\n",
    "    Use gradient ascent to optimize an image so it maximizes the\n",
    "    mean value of the given layer_tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    layer_tensor: Reference to a tensor that will be maximized.\n",
    "    image: Input image used as the starting point.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    step_size: Scale for each step of the gradient ascent.\n",
    "    tile_size: Size of the tiles when calculating the gradient.\n",
    "    show_gradient: Plot the gradient in each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the image so we don't overwrite the original image.\n",
    "    img = image.copy()\n",
    "    \n",
    "    # print(\"Image before:\")\n",
    "    # plot_image(img)\n",
    "\n",
    "    print(\"Processing image: \", end=\"\")\n",
    "\n",
    "    # Use TensorFlow to get the mathematical function for the\n",
    "    # gradient of the given layer-tensor with regard to the\n",
    "    # input image. This may cause TensorFlow to add the same\n",
    "    # math-expressions to the graph each time this function is called.\n",
    "    # It may use a lot of RAM and could be moved outside the function.\n",
    "    gradient = model.get_gradient(layer_tensor)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate the value of the gradient.\n",
    "        # This tells us how to change the image so as to\n",
    "        # maximize the mean of the given layer-tensor.\n",
    "        grad = tiled_gradient(gradient=gradient, image=img)\n",
    "        \n",
    "        # Blur the gradient with different amounts and add\n",
    "        # them together. The blur amount is also increased\n",
    "        # during the optimization. This was found to give\n",
    "        # nice, smooth images. You can try and change the formulas.\n",
    "        # The blur-amount is called sigma (0=no blur, 1=low blur, etc.)\n",
    "        # We could call gaussian_filter(grad, sigma=(sigma, sigma, 0.0))\n",
    "        # which would not blur the colour-channel. This tends to\n",
    "        # give psychadelic / pastel colours in the resulting images.\n",
    "        # When the colour-channel is also blurred the colours of the\n",
    "        # input image are mostly retained in the output image.\n",
    "        sigma = (i * 4.0) / num_iterations + 0.5\n",
    "        grad_smooth1 = gaussian_filter(grad, sigma=sigma)\n",
    "        grad_smooth2 = gaussian_filter(grad, sigma=sigma*2)\n",
    "        grad_smooth3 = gaussian_filter(grad, sigma=sigma*0.5)\n",
    "        grad = (grad_smooth1 + grad_smooth2 + grad_smooth3)\n",
    "\n",
    "        # Scale the step-size according to the gradient-values.\n",
    "        # This may not be necessary because the tiled-gradient\n",
    "        # is already normalized.\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "\n",
    "        # Update the image by following the gradient.\n",
    "        img += grad * step_size_scaled\n",
    "\n",
    "        if show_gradient:\n",
    "            # Print statistics for the gradient.\n",
    "            msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
    "            print(msg.format(grad.min(), grad.max(), step_size_scaled))\n",
    "\n",
    "            # Plot the gradient.\n",
    "            plot_gradient(grad)\n",
    "        else:\n",
    "            # Otherwise show a little progress-indicator.\n",
    "            print(\". \", end=\"\")\n",
    "\n",
    "    print()\n",
    "    # print(\"Image after:\")\n",
    "    # plot_image(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def recursive_optimize(layer_tensor, image,\n",
    "                       num_repeats=4, rescale_factor=0.7, blend=0.2,\n",
    "                       num_iterations=10, step_size=3.0,\n",
    "                       tile_size=400):\n",
    "    \"\"\"\n",
    "    Recursively blur and downscale the input image.\n",
    "    Each downscaled image is run through the optimize_image()\n",
    "    function to amplify the patterns that the Inception model sees.\n",
    "\n",
    "    Parameters:\n",
    "    image: Input image used as the starting point.\n",
    "    rescale_factor: Downscaling factor for the image.\n",
    "    num_repeats: Number of times to downscale the image.\n",
    "    blend: Factor for blending the original and processed images.\n",
    "\n",
    "    Parameters passed to optimize_image():\n",
    "    layer_tensor: Reference to a tensor that will be maximized.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    step_size: Scale for each step of the gradient ascent.\n",
    "    tile_size: Size of the tiles when calculating the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # Do a recursive step?\n",
    "    if num_repeats>0:\n",
    "        # Blur the input image to prevent artifacts when downscaling.\n",
    "        # The blur amount is controlled by sigma. Note that the\n",
    "        # colour-channel is not blurred as it would make the image gray.\n",
    "        sigma = 0.5\n",
    "        img_blur = gaussian_filter(image, sigma=(sigma, sigma, 0.0))\n",
    "\n",
    "        # Downscale the image.\n",
    "        img_downscaled = resize_image(image=img_blur,\n",
    "                                      factor=rescale_factor)\n",
    "            \n",
    "        # Recursive call to this function.\n",
    "        # Subtract one from num_repeats and use the downscaled image.\n",
    "        img_result = recursive_optimize(layer_tensor=layer_tensor,\n",
    "                                        image=img_downscaled,\n",
    "                                        num_repeats=num_repeats-1,\n",
    "                                        rescale_factor=rescale_factor,\n",
    "                                        blend=blend,\n",
    "                                        num_iterations=num_iterations,\n",
    "                                        step_size=step_size,\n",
    "                                        tile_size=tile_size)\n",
    "        \n",
    "        # Upscale the resulting image back to its original size.\n",
    "        img_upscaled = resize_image(image=img_result, size=image.shape)\n",
    "\n",
    "        # Blend the original and processed images.\n",
    "        image = blend * image + (1.0 - blend) * img_upscaled\n",
    "\n",
    "    print(\"Recursive level:\", num_repeats)\n",
    "\n",
    "    # Process the image using the DeepDream algorithm.\n",
    "    img_result = optimize_image(layer_tensor=layer_tensor,\n",
    "                                image=image,\n",
    "                                num_iterations=num_iterations,\n",
    "                                step_size=step_size,\n",
    "                                tile_size=tile_size)\n",
    "    \n",
    "    return img_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n",
      "tf\n",
      "(50000, 32, 32, 3)\n",
      "Loading class label: data/cifar-100-python/meta\n",
      "Loading class label: data/cifar-100-python/meta\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "def get_proper_images(raw):\n",
    "    raw_float = np.array(raw, dtype=float)\n",
    "    images = raw_float.reshape([-1, 3, 32, 32])\n",
    "    print(K.image_dim_ordering())\n",
    "    if K.image_dim_ordering() == 'tf':    \n",
    "        images = images.transpose([0, 2, 3, 1])\n",
    "    return images\n",
    "\n",
    "def onehot_labels(labels):\n",
    "    return np.eye(100)[labels]\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_class_names():\n",
    "    \"\"\"\n",
    "    Unpickle the given file and return the data.\n",
    "    extract label names from the data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create full path for the file.\n",
    "    file_path = 'data/cifar-100-python/meta'\n",
    "\n",
    "    print(\"Loading class label: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    # Load the class-names from the pickled file.\n",
    "    # @TODO what's the name of the label name is the meta file\n",
    "    raw = data[b'fine_label_names']\n",
    "\n",
    "    # Convert from binary strings.\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "    \n",
    "    return names\n",
    "\n",
    "def load_coarse_class_names():\n",
    "    \"\"\"\n",
    "    Unpickle the given file and return the data.\n",
    "    extract label names from the data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create full path for the file.\n",
    "    file_path = 'data/cifar-100-python/meta'\n",
    "\n",
    "    print(\"Loading class label: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    # Load the class-names from the pickled file.\n",
    "    # @TODO what's the name of the label name is the meta file\n",
    "    raw = data[b'coarse_label_names']\n",
    "\n",
    "    # Convert from binary strings.\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "    \n",
    "    return names\n",
    "\n",
    "X_train = get_proper_images(unpickle('data/cifar-100-python/train')[b'data'])\n",
    "Y_train = onehot_labels(unpickle('data/cifar-100-python/train')[b'fine_labels'])\n",
    "X_test = get_proper_images(unpickle('data/cifar-100-python/test')[b'data'])\n",
    "Y_test = onehot_labels(unpickle('data/cifar-100-python/test')[b'fine_labels'])\n",
    "print(X_train.shape)\n",
    "Y_test.shape\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "# import numpy as np\n",
    "# # load data\n",
    "\n",
    "# def sample_images(X_train):\n",
    "#     # create a grid of 3x3 images\n",
    "#     start = int(np.random.rand()*2500)-10\n",
    "#     for i in range(0, 9):\n",
    "#         plt.subplot(3,3,1 + i)\n",
    "#         # plt.imshow(toimage(X_train[i+start]))\n",
    "#         plt.axis('off')\n",
    "#     # show the plot\n",
    "#     plt.show()\n",
    "\n",
    "cls_names = load_class_names()\n",
    "# cls_names\n",
    "\n",
    "cls_coarse_names = load_coarse_class_names()\n",
    "# cls_coarse_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the data set and split them according to coarse label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n",
      "tf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = get_proper_images(unpickle('data/cifar-100-python/train')[b'data'])\n",
    "Y_train = onehot_labels(unpickle('data/cifar-100-python/train')[b'fine_labels'])\n",
    "Y_train_class = unpickle('data/cifar-100-python/train')[b'fine_labels']\n",
    "Y_train_coarse = unpickle('data/cifar-100-python/train')[b'coarse_labels']\n",
    "X_test = get_proper_images(unpickle('data/cifar-100-python/test')[b'data'])\n",
    "Y_test = onehot_labels(unpickle('data/cifar-100-python/test')[b'fine_labels'])\n",
    "\n",
    "# Y_test_class is the single number version of Y_test ex: [0, 0, 1] -> 2\n",
    "Y_test_class = unpickle('data/cifar-100-python/test')[b'fine_labels']\n",
    "Y_test_coarse = unpickle('data/cifar-100-python/test')[b'coarse_labels']\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "       Superclass\t                                 Classes\n",
    "00 0 aquatic mammals\t                beaver, dolphin, otter, seal, whale 5\n",
    "05 1 fish\t                        aquarium fish, flatfish, ray, shark, trout 5\n",
    "10 2 flowers\t                        orchids, poppies, roses, sunflowers, tulips\n",
    "15 3 food containers\t                bottles, bowls, cans, cups, plates\n",
    "20 4 fruit and vegetables\t        apples, mushrooms, oranges, pears, sweet peppers\n",
    "25 5 household electrical devices\tclock, computer keyboard, lamp, telephone, television\n",
    "30 6 household furniture\t            bed, chair, couch, table, wardrobe\n",
    "35 7 insects                         bee, beetle, butterfly, caterpillar, cockroach\n",
    "40 8 large carnivores\t            bear, leopard, lion, tiger, wolf\n",
    "45 9 large man-made outdoor things\tbridge, castle, house, road, skyscraper\n",
    "50 10 large natural outdoor scenes\tcloud, forest, mountain, plain, sea\n",
    "55 11 large omnivores and herbivores\tcamel, cattle, chimpanzee, elephant, kangaroo\n",
    "60 12 medium-sized mammals\t        fox, porcupine, possum, raccoon, skunk\n",
    "65 13 non-insect invertebrates\t    crab, lobster, snail, spider, worm\n",
    "70 14 people\t                        baby, boy, girl, man, woman\n",
    "75 15 reptiles\t                    crocodile, dinosaur, lizard, snake, turtle\n",
    "80 16 small mammals\t                hamster, mouse, rabbit, shrew, squirrel\n",
    "85 17 trees\t                        maple, oak, palm, pine, willow\n",
    "90 18 vehicles 1\t                    bicycle, bus, motorcycle, pickup truck, train\n",
    "95 19 vehicles 2\t                    lawn-mower, rocket, streetcar, tank, tractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_people = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==14]\n",
    "Y_train_people = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==14]\n",
    "\n",
    "X_train_small_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==16]\n",
    "Y_train_small_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==16]\n",
    "\n",
    "X_train_medium_sized_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==12]\n",
    "Y_train_medium_sized_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==12]\n",
    "\n",
    "X_train_aquatic_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==0]\n",
    "Y_train_aquatic_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==0]\n",
    "\n",
    "X_train_fish = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==1]\n",
    "Y_train_fish = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==1]\n",
    "\n",
    "X_train_reptiles = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==15]\n",
    "Y_train_reptiles = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==15]\n",
    "\n",
    "X_train_carnivores = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==8]\n",
    "Y_train_carnivores = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==8]\n",
    "\n",
    "# test data\n",
    "X_test_people = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==14]\n",
    "Y_test_people = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==14]\n",
    "\n",
    "X_test_small_mammals = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==16]\n",
    "Y_test_small_mammals = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==16]\n",
    "\n",
    "X_test_medium_sized_mammals = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==12]\n",
    "Y_test_medium_sized_mammals = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==12]\n",
    "\n",
    "X_test_aquatic_mammals = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==0]\n",
    "Y_test_aquatic_mammals = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==0]\n",
    "\n",
    "X_test_fish = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==1]\n",
    "Y_test_fish = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==1]\n",
    "\n",
    "X_test_reptiles = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==15]\n",
    "Y_test_reptiles = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==15]\n",
    "\n",
    "X_test_carnivores = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==8]\n",
    "Y_test_carnivores = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==8]\n",
    "\n",
    "# combines the five dataset\n",
    "X_train_five = np.concatenate((X_train_people,X_train_small_mammals,X_train_medium_sized_mammals,X_train_aquatic_mammals,X_train_fish), axis=0)\n",
    "Y_train_five = np.concatenate((Y_train_people,Y_train_small_mammals,Y_train_medium_sized_mammals,Y_train_aquatic_mammals,Y_train_fish), axis=0)\n",
    "X_test_five = np.concatenate((X_test_people,X_test_small_mammals,X_test_medium_sized_mammals,X_test_aquatic_mammals,X_test_fish), axis=0)\n",
    "Y_test_five = np.concatenate((Y_test_people,Y_test_small_mammals,Y_test_medium_sized_mammals,Y_test_aquatic_mammals,Y_test_fish), axis=0)\n",
    "\n",
    "# convert type\n",
    "X_train_people = np.asarray(X_train_people)\n",
    "X_train_small_mammals = np.asarray(X_train_small_mammals)\n",
    "X_train_medium_sized_mammals = np.asarray(X_train_medium_sized_mammals)\n",
    "X_train_aquatic_mammals = np.asarray(X_train_aquatic_mammals)\n",
    "X_train_fish = np.asarray(X_train_fish)\n",
    "X_train_five = np.asarray(X_train_five)\n",
    "X_train_reptiles = np.asarray(X_train_reptiles)\n",
    "X_train_carnivores = np.asarray(X_train_carnivores)\n",
    "\n",
    "\n",
    "X_test_people = np.asarray(X_test_people)\n",
    "X_test_small_mammals = np.asarray(X_test_small_mammals)\n",
    "X_test_medium_sized_mammals = np.asarray(X_test_medium_sized_mammals)\n",
    "X_test_aquatic_mammals = np.asarray(X_test_aquatic_mammals)\n",
    "X_test_fish = np.asarray(X_test_fish)\n",
    "X_test_five = np.asarray(X_test_five)\n",
    "X_test_reptiles = np.asarray(X_test_reptiles)\n",
    "X_test_carnivores = np.asarray(X_test_carnivores)\n",
    "\n",
    "Y_train_people = np.asarray(Y_train_people)\n",
    "Y_train_small_mammals = np.asarray(Y_train_small_mammals)\n",
    "Y_train_medium_sized_mammals = np.asarray(Y_train_medium_sized_mammals)\n",
    "Y_train_aquatic_mammals = np.asarray(Y_train_aquatic_mammals)\n",
    "Y_train_fish = np.asarray(Y_train_fish)\n",
    "Y_train_five = np.asarray(Y_train_five)\n",
    "Y_train_reptiles = np.asarray(Y_train_reptiles)\n",
    "Y_train_carnivores = np.asarray(Y_train_carnivores)\n",
    "\n",
    "Y_test_people = np.asarray(Y_test_people)\n",
    "Y_test_small_mammals = np.asarray(Y_test_small_mammals)\n",
    "Y_test_medium_sized_mammals = np.asarray(Y_test_medium_sized_mammals)\n",
    "Y_test_aquatic_mammals = np.asarray(Y_test_aquatic_mammals)\n",
    "Y_test_fish = np.asarray(Y_test_fish)\n",
    "Y_test_five = np.asarray(Y_test_five)\n",
    "Y_test_reptiles = np.asarray(Y_test_reptiles)\n",
    "Y_test_carnivores = np.asarray(Y_test_carnivores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using coarse label to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another way to split data with more data per episode\n",
    "X_test_1 = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 0 and Y_label <= 3]\n",
    "Y_test_1 = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 0 and Y_label <= 3]\n",
    "\n",
    "X_test_2 = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 4 and Y_label <= 7]\n",
    "Y_test_2 = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 4 and Y_label <= 7]\n",
    "\n",
    "X_test_3 = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 8 and Y_label <= 11]\n",
    "Y_test_3 = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 8 and Y_label <= 11]\n",
    "\n",
    "X_test_4 = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 12 and Y_label <= 15]\n",
    "Y_test_4 = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 12 and Y_label <= 15]\n",
    "\n",
    "X_test_5 = [X_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 16 and Y_label <= 19]\n",
    "Y_test_5 = [Y_test[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label >= 16 and Y_label <= 19]\n",
    "\n",
    "\n",
    "X_train_1 = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 0 and Y_label <= 3]\n",
    "Y_train_1 = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 0 and Y_label <= 3]\n",
    "\n",
    "X_train_2 = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 4 and Y_label <= 7]\n",
    "Y_train_2 = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 4 and Y_label <= 7]\n",
    "\n",
    "X_train_3 = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 8 and Y_label <= 11]\n",
    "Y_train_3 = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 8 and Y_label <= 11]\n",
    "\n",
    "X_train_4 = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 12 and Y_label <= 15]\n",
    "Y_train_4 = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 12 and Y_label <= 15]\n",
    "\n",
    "X_train_5 = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 16 and Y_label <= 19]\n",
    "Y_train_5 = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label >= 16 and Y_label <= 19]\n",
    "\n",
    "X_train_1 = np.asarray(X_train_1)\n",
    "X_train_2 = np.asarray(X_train_2)\n",
    "X_train_3 = np.asarray(X_train_3)\n",
    "X_train_4 = np.asarray(X_train_4)\n",
    "X_train_5 = np.asarray(X_train_5)\n",
    "\n",
    "X_test_1 = np.asarray(X_test_1)\n",
    "X_test_2 = np.asarray(X_test_2)\n",
    "X_test_3 = np.asarray(X_test_3)\n",
    "X_test_4 = np.asarray(X_test_4)\n",
    "X_test_5 = np.asarray(X_test_5)\n",
    "\n",
    "Y_train_1 = np.asarray(Y_train_1)\n",
    "Y_train_2 = np.asarray(Y_train_2)\n",
    "Y_train_3 = np.asarray(Y_train_3)\n",
    "Y_train_4 = np.asarray(Y_train_4)\n",
    "Y_train_5 = np.asarray(Y_train_5)\n",
    "\n",
    "Y_test_1 = np.asarray(Y_test_1)\n",
    "Y_test_2 = np.asarray(Y_test_2)\n",
    "Y_test_3 = np.asarray(Y_test_3)\n",
    "Y_test_4 = np.asarray(Y_test_4)\n",
    "Y_test_5 = np.asarray(Y_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 32, 32, 3)\n",
      "(2500, 32, 32, 3)\n",
      "(2500, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_easy = [X_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==0]\n",
    "X_train_easy = np.concatenate((X_train_easy, [X_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==20]), axis=0)\n",
    "X_train_easy = np.concatenate((X_train_easy, [X_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==40]), axis=0)\n",
    "X_train_easy = np.concatenate((X_train_easy, [X_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==60]), axis=0)\n",
    "X_train_easy = np.concatenate((X_train_easy, [X_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==80]), axis=0)\n",
    "X_train_easy = np.asarray(X_train_easy)\n",
    "\n",
    "Y_train_easy = [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==0]\n",
    "Y_train_easy = np.concatenate((Y_train_easy, [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==20]), axis=0)\n",
    "Y_train_easy = np.concatenate((Y_train_easy, [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==40]), axis=0)\n",
    "Y_train_easy = np.concatenate((Y_train_easy, [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==60]), axis=0)\n",
    "Y_train_easy = np.concatenate((Y_train_easy, [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==80]), axis=0)\n",
    "Y_train_easy = np.asarray(Y_train_easy)\n",
    "\n",
    "X_test_easy = [X_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==0]\n",
    "X_test_easy = np.concatenate((X_test_easy, [X_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==20]), axis=0)\n",
    "X_test_easy = np.concatenate((X_test_easy, [X_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==40]), axis=0)\n",
    "X_test_easy = np.concatenate((X_test_easy, [X_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==60]), axis=0)\n",
    "X_test_easy = np.concatenate((X_test_easy, [X_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==80]), axis=0)\n",
    "X_test_easy = np.asarray(X_test_easy)\n",
    "\n",
    "Y_test_easy = [Y_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==0]\n",
    "Y_test_easy = np.concatenate((Y_test_easy, [Y_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==20]), axis=0)\n",
    "Y_test_easy = np.concatenate((Y_test_easy, [Y_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==40]), axis=0)\n",
    "Y_test_easy = np.concatenate((Y_test_easy, [Y_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==60]), axis=0)\n",
    "Y_test_easy = np.concatenate((Y_test_easy, [Y_test[i] for i, Y_label in enumerate(Y_test_class) if Y_label==80]), axis=0)\n",
    "Y_test_easy = np.asarray(Y_test_easy)\n",
    "\n",
    "print(X_train_five.shape)\n",
    "print(X_train_easy.shape)\n",
    "print(X_train_people.shape)\n",
    "Y = [Y_train[i] for i, Y_label in enumerate(Y_train_class) if Y_label==20]\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_class_people = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==14]\n",
    "Y_test_class_small_mammals = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==16]\n",
    "Y_test_class_medium_sized_mammals = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==12]\n",
    "Y_test_class_aquatic_mammals = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==0]\n",
    "Y_test_class_fish = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==1]\n",
    "Y_test_class_reptiles = [Y_test_class[i] for i, Y_label in enumerate(Y_test_coarse) if Y_label==15]\n",
    "Y_test_class_five = np.concatenate((Y_test_class_people, Y_test_class_small_mammals, Y_test_class_medium_sized_mammals, Y_test_class_aquatic_mammals, Y_test_class_fish), axis=0)\n",
    "\n",
    "                                    \n",
    "Y_test_class_people = np.asarray(Y_test_class_people)\n",
    "Y_test_class_small_mammals = np.asarray(Y_test_class_small_mammals)\n",
    "Y_test_class_medium_sized_mammals = np.asarray(Y_test_class_medium_sized_mammals)\n",
    "Y_test_class_aquatic_mammals = np.asarray(Y_test_class_aquatic_mammals)\n",
    "Y_test_class_fish = np.asarray(Y_test_class_fish)\n",
    "Y_test_class_reptilse = np.asarray(Y_test_class_reptiles)\n",
    "Y_test_class_five = np.asarray(Y_test_class_five)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_name = cls_names\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "    # This function is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = images_test[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = cls_test[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])\n",
    "    \n",
    "def plot_confusion_matrix(cls_pred):\n",
    "    # This is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_test,  # True class for test-set.\n",
    "                          y_pred=cls_pred)  # Predicted class.\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    for i in range(num_classes):\n",
    "        # Append the class-name to each line.\n",
    "        class_name = \"({}) {}\".format(i, class_names[i])\n",
    "        print(cm[i, :], class_name)\n",
    "\n",
    "    # Print the class-numbers for easy reference.\n",
    "    class_numbers = [\" ({0})\".format(i) for i in range(num_classes)]\n",
    "    print(\"\".join(class_numbers))\n",
    "    \n",
    "# Split the data-set in batches of this size to limit RAM usage.\n",
    "batch_size = 256\n",
    "\n",
    "def predict_cls(images, labels, cls_true, model):\n",
    "    # Number of images.\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_images)\n",
    "\n",
    "        # Create a feed-dict with the images and labels\n",
    "        # between index i and j.\n",
    "        x = images[i:j, :]\n",
    "        y_true = labels[i:j, :]\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = model.predict(x)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    return correct, cls_pred\n",
    "\n",
    "def predict_cls_test():\n",
    "    return predict_cls(images = images_test,\n",
    "                       labels = labels_test,\n",
    "                       cls_true = cls_test)\n",
    "\n",
    "def classification_accuracy(correct):\n",
    "    # When averaging a boolean array, False means 0 and True means 1.\n",
    "    # So we are calculating: number of True / len(correct) which is\n",
    "    # the same as the classification accuracy.\n",
    "    \n",
    "    # Return the classification accuracy\n",
    "    # and the number of correct classifications.\n",
    "    return correct.mean(), correct.sum()\n",
    "\n",
    "def print_test_accuracy(images, labels, cls_true, show_example_errors=False, show_confusion_matrix=False):\n",
    "\n",
    "    # For all the images in the test-set,\n",
    "    # calculate the predicted classes and whether they are correct.\n",
    "    correct, cls_pred = predict_cls(images, labels, cls_true)\n",
    "    \n",
    "    # Classification accuracy and the number of correct classifications.\n",
    "    acc, num_correct = classification_accuracy(correct)\n",
    "    \n",
    "    # Number of images being classified.\n",
    "    num_images = len(correct)\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, num_correct, num_images))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)\n",
    "        \n",
    "def plot_conv_weights(weights, input_channel=0):\n",
    "    # Assume weights are TensorFlow ops for 4-dim variables\n",
    "    # e.g. weights_conv1 or weights_conv2.\n",
    "\n",
    "    # Retrieve the values of the weight-variables from TensorFlow.\n",
    "    # A feed-dict is not necessary because nothing is calculated.\n",
    "    w = session.run(weights)\n",
    "\n",
    "    # Print statistics for the weights.\n",
    "    print(\"Min:  {0:.5f}, Max:   {1:.5f}\".format(w.min(), w.max()))\n",
    "    print(\"Mean: {0:.5f}, Stdev: {1:.5f}\".format(w.mean(), w.std()))\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "    abs_max = max(abs(w_min), abs(w_max))\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = w.shape[3]\n",
    "\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid filter-weights.\n",
    "        if i<num_filters:\n",
    "            # Get the weights for the i'th filter of the input channel.\n",
    "            # The format of this 4-dim tensor is determined by the\n",
    "            # TensorFlow API. See Tutorial #02 for more details.\n",
    "            img = w[:, :, input_channel, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=-abs_max, vmax=abs_max,\n",
    "                      interpolation='nearest', cmap='seismic')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    \n",
    "def plot_layer_output(layer_output, image):\n",
    "    # Assume layer_output is a 4-dim tensor\n",
    "    # e.g. output_conv1 or output_conv2.\n",
    "\n",
    "    # Create a feed-dict which holds the single input image.\n",
    "    # Note that TensorFlow needs a list of images,\n",
    "    # so we just create a list with this one image.\n",
    "    feed_dict = {x: [image]}\n",
    "    \n",
    "    # Retrieve the output of the layer after inputting this image.\n",
    "    values = session.run(layer_output, feed_dict=feed_dict)\n",
    "\n",
    "    # Get the lowest and highest values.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    values_min = np.min(values)\n",
    "    values_max = np.max(values)\n",
    "\n",
    "    # Number of image channels output by the conv. layer.\n",
    "    num_images = values.shape[3]\n",
    "\n",
    "    # Number of grid-cells to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_images))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid image-channels.\n",
    "        if i<num_images:\n",
    "            # Get the images for the i'th output channel.\n",
    "            img = values[0, :, :, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=values_min, vmax=values_max,\n",
    "                      interpolation='nearest', cmap='binary')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "# modified by Katy\n",
    "def random_batch(images_train, labels_train):\n",
    "    # Number of images in the training-set.\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def get_image(i, images, images_class):\n",
    "    return images[i, :, :, :], images_class[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### early-stoping helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalab pretrained model without frozen weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K._LEARNING_PHASE = tf.constant(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datalab pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# K._LEARNING_PHASE = tf.constant(0)\n",
    "# K.keras_learning_phase = tf.constant(0)\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import applications\n",
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "def load_model_whole():\n",
    "    # HOME = expanduser(\"~\")\n",
    "    # MODEL_PATH = os.path.join(HOME, 'katy/good_model.h5')\n",
    "    # vgg16 = load_model(MODEL_PATH)\n",
    "    vgg16 = load_model('models/good_model.h5')\n",
    "    top = vgg16.get_layer('dropout_56').output\n",
    "    # print(top)\n",
    "    # vgg16 = VGG16(weights='imagenet', include_top=False)\n",
    "    # top = vgg16.output\n",
    "    prediction = Dense(output_dim=100, activation='softmax', name='softmax')(top)\n",
    "    model_whole = Model(input=vgg16.input, output=prediction)\n",
    "    print(vgg16.input.shape)\n",
    "    # model_whole.summary()\n",
    "    print(\"datalab pretrained model loaded\")\n",
    "    return model_whole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_input_1 (InputLaye (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (None, 32, 32, 64)    1792        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_45 (BatchNorm (None, 32, 32, 64)    256         convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 32, 32, 64)    0           batchnormalization_45[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)             (None, 32, 32, 64)    0           activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (None, 32, 32, 64)    36928       dropout_45[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_46 (BatchNorm (None, 32, 32, 64)    256         convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 32, 32, 64)    0           batchnormalization_46[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)             (None, 32, 32, 64)    0           activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_17 (MaxPooling2D)   (None, 16, 16, 64)    0           dropout_46[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 16, 16, 128)   73856       maxpooling2d_17[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_47 (BatchNorm (None, 16, 16, 128)   512         convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 16, 16, 128)   0           batchnormalization_47[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)             (None, 16, 16, 128)   0           activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_44 (Convolution2D) (None, 16, 16, 128)   147584      dropout_47[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_48 (BatchNorm (None, 16, 16, 128)   512         convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 16, 16, 128)   0           batchnormalization_48[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)             (None, 16, 16, 128)   0           activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_18 (MaxPooling2D)   (None, 8, 8, 128)     0           dropout_48[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_45 (Convolution2D) (None, 8, 8, 256)     295168      maxpooling2d_18[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_49 (BatchNorm (None, 8, 8, 256)     1024        convolution2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 8, 8, 256)     0           batchnormalization_49[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)             (None, 8, 8, 256)     0           activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 8, 8, 256)     590080      dropout_49[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_50 (BatchNorm (None, 8, 8, 256)     1024        convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 8, 8, 256)     0           batchnormalization_50[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)             (None, 8, 8, 256)     0           activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 8, 8, 256)     590080      dropout_50[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_51 (BatchNorm (None, 8, 8, 256)     1024        convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 8, 8, 256)     0           batchnormalization_51[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)             (None, 8, 8, 256)     0           activation_51[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_19 (MaxPooling2D)   (None, 4, 4, 256)     0           dropout_51[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_48 (Convolution2D) (None, 4, 4, 256)     590080      maxpooling2d_19[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_52 (BatchNorm (None, 4, 4, 256)     1024        convolution2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 4, 4, 256)     0           batchnormalization_52[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)             (None, 4, 4, 256)     0           activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_49 (Convolution2D) (None, 4, 4, 256)     590080      dropout_52[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_53 (BatchNorm (None, 4, 4, 256)     1024        convolution2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 4, 4, 256)     0           batchnormalization_53[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)             (None, 4, 4, 256)     0           activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 4, 4, 256)     590080      dropout_53[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_54 (BatchNorm (None, 4, 4, 256)     1024        convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 4, 4, 256)     0           batchnormalization_54[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)             (None, 4, 4, 256)     0           activation_54[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_20 (MaxPooling2D)   (None, 2, 2, 256)     0           dropout_54[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 1024)          0           maxpooling2d_20[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1024)          1049600     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_55 (BatchNorm (None, 1024)          4096        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, 1024)          0           batchnormalization_55[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)             (None, 1024)          0           activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1024)          1049600     dropout_55[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_56 (BatchNorm (None, 1024)          4096        dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, 1024)          0           batchnormalization_56[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)             (None, 1024)          0           activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "softmax (Dense)                  (None, 100)           102500      dropout_56[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 5,723,300\n",
      "Trainable params: 5,715,364\n",
      "Non-trainable params: 7,936\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_whole = load_model_whole()\n",
    "model_whole.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with confidence reinforcement(remember to reload model when necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6d667df0be9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_people\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_small_mammals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_medium_sized_mammals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_aquatic_mammals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_images' is not defined"
     ]
    }
   ],
   "source": [
    "sample_images(X_train_people)\n",
    "sample_images(X_train_small_mammals)\n",
    "sample_images(X_train_medium_sized_mammals)\n",
    "sample_images(X_train_aquatic_mammals)\n",
    "sample_images(X_train_fish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train_people = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==14]\n",
    "# Y_train_people = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==14]\n",
    "\n",
    "# X_train_small_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==16]\n",
    "# Y_train_small_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==16]\n",
    "\n",
    "# X_train_medium_sized_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==12]\n",
    "# Y_train_medium_sized_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==12]\n",
    "\n",
    "# X_train_aquatic_mammals = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==0]\n",
    "# Y_train_aquatic_mammals = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==0]\n",
    "\n",
    "# X_train_fish = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==1]\n",
    "# Y_train_fish = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==1]\n",
    "\n",
    "# X_train_reptiles = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==15]\n",
    "# Y_train_reptiles = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==15]\n",
    "\n",
    "# X_train_carnivores = [X_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==8]\n",
    "# Y_train_carnivores = [Y_train[i] for i, Y_label in enumerate(Y_train_coarse) if Y_label==8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training a model and save it to .h5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the pre-calculated variance list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('variance/X_train_people_var.txt', 'rb') as f:\n",
    "    X_train_people_var = pickle.load(f)\n",
    "X_train_people_var= np.asarray(X_train_people_var)\n",
    "with open('variance/X_train_small_mammals_var.txt', 'rb') as f:\n",
    "    X_train_small_mammals_var = pickle.load(f)\n",
    "X_train_small_mammals_var= np.asarray(X_train_small_mammals_var)\n",
    "with open('variance/X_train_medium_sized_mammals_var.txt', 'rb') as f:\n",
    "    X_train_medium_sized_mammals_var = pickle.load(f)\n",
    "X_train_medium_sized_mammals_var = np.asarray(X_train_medium_sized_mammals_var)\n",
    "with open('variance/X_train_aquatic_mammals_var.txt', 'rb') as f:\n",
    "    X_train_aquatic_mammals_var = pickle.load(f)\n",
    "X_train_aquatic_mammals_var = np.asarray(X_train_aquatic_mammals_var)\n",
    "with open('variance/X_train_fish_var.txt', 'rb') as f:\n",
    "    X_train_fish_var = pickle.load(f)\n",
    "X_train_fish_var = np.asarray(X_train_fish_var)\n",
    "with open('variance/X_train_reptiles_var.txt', 'rb') as f:\n",
    "    X_train_reptiles_var = pickle.load(f)\n",
    "X_train_reptiles_var = np.asarray(X_train_reptiles_var)\n",
    "with open('variance/X_train_carnivores_var.txt', 'rb') as f:\n",
    "    X_train_carnivores_var = pickle.load(f)\n",
    "X_train_carnivores_var = np.asarray(X_train_carnivores_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partialy train on high-condidence image\n",
    "def dream(model, X_train_var, X_train, Y_train, X_test, Y_test, confidence_rate =0.5):\n",
    "    X_train_var_tuple = enumerate(X_train_var)\n",
    "    # samll variance -> high confidence\n",
    "    X_train_var_tuple_sorted = sorted(X_train_var_tuple, key = lambda x: x[1], reverse=False)\n",
    "    X_train_var_tuple_sorted_with_high_confidence_ = X_train_var_tuple_sorted[: int(confidence_rate * len(X_train))]\n",
    "    X_train_var_index_sorted_with_high_confidence = [x[0] for x in X_train_var_tuple_sorted_with_high_confidence_]\n",
    "    X_train_high_confidence = [X_train_small_mammals[i] for i in X_train_var_index_sorted_with_high_confidence]\n",
    "    Y_train_high_confidence = [Y_train_small_mammals[i] for i in X_train_var_index_sorted_with_high_confidence]\n",
    "    print(\"train with: \")\n",
    "    print(len(X_train_high_confidence))\n",
    "    X_train_high_confidence = np.asarray(X_train_high_confidence)\n",
    "    Y_train_high_confidence = np.asarray(Y_train_high_confidence)\n",
    "    sgd = SGD(lr=0.0001)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model.fit(X_train_high_confidence, Y_train_high_confidence, \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              validation_split=0.2, \\\n",
    "              callbacks=[early_stop], \\\n",
    "              verbose=0, \\\n",
    "              shuffle=True)\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=1, batch_size=batch_size)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "    # acc on small mammals: 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with dreaming, do 20 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "0\n",
      "1 shot leaning, day time\n",
      "1 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('models/small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"models/small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000]\n",
    "all_shots_acc_A = {1:[], \n",
    "                   5:[], \n",
    "                   10:[], \n",
    "                   15:[], \n",
    "                   20:[], \n",
    "                   100:[],\n",
    "                   200:[], \n",
    "                   300:[], \n",
    "                   400:[], \n",
    "                   500:[],\n",
    "                   1000:[]}\n",
    "all_shots_acc_B = {1:[], \n",
    "                   5:[], \n",
    "                   10:[], \n",
    "                   15:[], \n",
    "                   20:[], \n",
    "                   100:[],\n",
    "                   200:[], \n",
    "                   300:[], \n",
    "                   400:[], \n",
    "                   500:[],\n",
    "                   1000:[]}\n",
    "for i in range(20):\n",
    "    for num_train_images in nums_train_images:\n",
    "        # adjust training epoch\n",
    "        if num_train_images < 20:\n",
    "            nb_epoch = 6\n",
    "        else:\n",
    "            nb_epoch = 50\n",
    "\n",
    "        # first initialize the model and let in train on the day time task (task A)\n",
    "        print(str(i))\n",
    "        print(str(num_train_images) + \" shot leaning, day time\")\n",
    "\n",
    "        # load json and create model\n",
    "        json_file = open('models/small_mammals_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model_whole = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model_whole.load_weights(\"models/small_mammals_model.h5\")\n",
    "        #print(\"Loaded model from disk\")\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning, night time\")\n",
    "        print(\"dreaming\")\n",
    "        dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "        model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "                  batch_size=batch_size, \\\n",
    "                  nb_epoch=nb_epoch, \\\n",
    "                  shuffle=True)\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "        score_B = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "        print('\\nTest loss: %.3f' % score_B[0])\n",
    "        print('Test accuracy: %.3f' % score_B[1])\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "        # test on yesterday episode -> totally forget\n",
    "        score_A = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "        print('\\nTest loss: %.3f' % score_A[0])\n",
    "        print('Test accuracy: %.3f' % score_A[1])\n",
    "\n",
    "        all_shots_acc_A[num_train_images].append(score_A[1])\n",
    "        all_shots_acc_B[num_train_images].append(score_B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save result to file\n",
    "with open('accuracy/all_shots_acc_A_with_dream_small_mammals_to_medium_sized_mammals_100.txt', 'wb') as f:\n",
    "    pickle.dump(all_shots_acc_A, f)\n",
    "with open('accuracy/all_shots_acc_B_with_dream_small_mammals_to_medium_sized_mammals_100.txt', 'wb') as f:\n",
    "    pickle.dump(all_shots_acc_B, f)    \n",
    "with open('accuracy/all_shots_acc_A_without_dream.txt', 'rb') as f:\n",
    "    all_shots_acc_A = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('models/small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"models/small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000]\n",
    "all_shots_acc_A = {1:[], \n",
    "                   5:[], \n",
    "                   10:[], \n",
    "                   15:[], \n",
    "                   20:[], \n",
    "                   100:[],\n",
    "                   200:[], \n",
    "                   300:[], \n",
    "                   400:[], \n",
    "                   500:[],\n",
    "                   1000:[]}\n",
    "all_shots_acc_B = {1:[], \n",
    "                   5:[], \n",
    "                   10:[], \n",
    "                   15:[], \n",
    "                   20:[], \n",
    "                   100:[],\n",
    "                   200:[], \n",
    "                   300:[], \n",
    "                   400:[], \n",
    "                   500:[],\n",
    "                   1000:[]}\n",
    "for i in range(100):\n",
    "    for num_train_images in nums_train_images:\n",
    "        print(\"no dream \" + str(i) + \" th test \" + str(num_train_images) + \"shot\")\n",
    "        # adjust training epoch\n",
    "        if num_train_images < 20:\n",
    "            nb_epoch = 6\n",
    "        else:\n",
    "            nb_epoch = 50\n",
    "\n",
    "        # first initialize the model and let in train on the day time task (task A)\n",
    "        # print(str(num_train_images) + \" shot leaning, day time\")\n",
    "\n",
    "        # load json and create model\n",
    "        json_file = open('models/small_mammals_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model_whole = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model_whole.load_weights(\"models/small_mammals_model.h5\")\n",
    "        #print(\"Loaded model from disk\")\n",
    "\n",
    "        # print(str(num_train_images) + \" shot leaning, night time\")\n",
    "        ### dreaming\n",
    "        ###dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "        model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "                  batch_size=batch_size, \\\n",
    "                  nb_epoch=nb_epoch, \\\n",
    "                  shuffle=True)\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "        score_B = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "        print('\\nTest loss: %.3f' % score_B[0])\n",
    "        print('Test accuracy: %.3f' % score_B[1])\n",
    "\n",
    "        print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "        # test on yesterday episode -> totally forget\n",
    "        score_A = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "        print('\\nTest loss: %.3f' % score_A[0])\n",
    "        print('Test accuracy: %.3f' % score_A[1])\n",
    "        all_shots_acc_A[num_train_images].append(score_A[1])\n",
    "        all_shots_acc_B[num_train_images].append(score_B[1])\n",
    "        K.clear_session()\n",
    "\n",
    "# save result to file\n",
    "with open('accuracy/all_shots_acc_A_without_dream_small_mammals_to_medium_sized_mammals.txt', 'wb') as f:\n",
    "    pickle.dump(all_shots_acc_A, f)\n",
    "with open('accuracy/all_shots_acc_B_without_dream_small_mammals_to_medium_sized_mammals.txt', 'wb') as f:\n",
    "    pickle.dump(all_shots_acc_B, f)    \n",
    "with open('accuracy/all_shots_acc_A_without_dream.txt', 'rb') as f:\n",
    "    all_shots_acc_A = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-fd8a76bd6a04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_h5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_h5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         raise TypeError('the JSON object must be str, not {!r}'.format(\n\u001b[0;32m--> 312\u001b[0;31m                             s.__class__.__name__))\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'\\ufeff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/davecg/396a65abde32590fbb43c15951de41f4\n",
    "# Usage: fix_keras_model.py old_model.h5 new_model.h5\n",
    "import h5py\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# shutil.copyfile(input_model_path, output_model_path)\n",
    "output_model_path = 'models/small_mammals_model-Copy1.h5'\n",
    "with h5py.File(output_model_path, \"r+\") as out_h5:\n",
    "    v = out_h5.attrs.get(\"model_config\")\n",
    "    config = json.loads(v)\n",
    "    for i, l in enumerate(config[\"config\"][\"layers\"]):\n",
    "        dtype = l[\"config\"].pop(\"input_dtype\", None)\n",
    "        if dtype is not None:\n",
    "            l[\"config\"][\"dtype\"] = dtype\n",
    "    new_config_str = json.dumps(config)\n",
    "    out_h5.attrs.modify(\"model_config\", new_config_str)\n",
    "\n",
    "# Check that it worked.\n",
    "from keras.models import load_model\n",
    "load_model(output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.files.File'>\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_model_path, \"r+\") as out_h5:\n",
    "    print(type(out_h5))\n",
    "    v = out_h5.attrs.get(\"model_config\")\n",
    "    print(type(v))\n",
    "    # config = json.loads(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium -> aquatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 3s - loss: 15.8675 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8461 - acc: 0.0000e+00 - val_loss: 15.8204 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8444 - acc: 0.0000e+00 - val_loss: 15.8170 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8559 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8543 - acc: 0.0000e+00 - val_loss: 15.8118 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8377 - acc: 0.0000e+00 - val_loss: 15.8103 - val_acc: 0.0000e+00\n",
      "CPU times: user 6.87 s, sys: 749 ms, total: 7.62 s\n",
      "Wall time: 9.41 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.306\n",
      "Test accuracy: 0.786\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s - loss: 14.3445 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 12.5719 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 10.6435 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.6627 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.7157 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 5.0624 - acc: 0.0000e+00\n",
      "CPU times: user 4.7 s, sys: 0 ns, total: 4.7 s\n",
      "Wall time: 4.76 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 12.583\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.109\n",
      "Test accuracy: 0.634\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 2s - loss: 15.8661 - acc: 0.0000e+00 - val_loss: 15.8250 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8586 - acc: 0.0000e+00 - val_loss: 15.8218 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8437 - acc: 0.0000e+00 - val_loss: 15.8185 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8360 - acc: 0.0000e+00 - val_loss: 15.8150 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8298 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8389 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "CPU times: user 6.6 s, sys: 533 ms, total: 7.13 s\n",
      "Wall time: 9.04 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.308\n",
      "Test accuracy: 0.786\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 2s - loss: 15.8197 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 15.7662 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 14.8916 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 14.1092 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 11.3654 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 9.1215 - acc: 0.0000e+00\n",
      "CPU times: user 4.62 s, sys: 0 ns, total: 4.62 s\n",
      "Wall time: 4.53 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 10.239\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.084\n",
      "Test accuracy: 0.564\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 3s - loss: 15.8436 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8504 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8579 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8234 - acc: 0.0000e+00 - val_loss: 15.8139 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8568 - acc: 0.0000e+00 - val_loss: 15.8114 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8507 - acc: 0.0000e+00 - val_loss: 15.8095 - val_acc: 0.0000e+00\n",
      "CPU times: user 6.94 s, sys: 564 ms, total: 7.5 s\n",
      "Wall time: 9.37 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.308\n",
      "Test accuracy: 0.786\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 2s - loss: 15.7519 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.3821 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 14.7365 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 12.4243 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 10.5431 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 9.0805 - acc: 0.0000e+00\n",
      "CPU times: user 4.9 s, sys: 0 ns, total: 4.9 s\n",
      "Wall time: 4.82 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 8.102\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.755\n",
      "Test accuracy: 0.414\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 3s - loss: 15.8581 - acc: 0.0000e+00 - val_loss: 15.8242 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8453 - acc: 0.0000e+00 - val_loss: 15.8207 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8611 - acc: 0.0000e+00 - val_loss: 15.8170 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8373 - acc: 0.0000e+00 - val_loss: 15.8130 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8442 - acc: 0.0000e+00 - val_loss: 15.8108 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8235 - acc: 0.0000e+00 - val_loss: 15.8087 - val_acc: 0.0000e+00\n",
      "CPU times: user 7.8 s, sys: 605 ms, total: 8.4 s\n",
      "Wall time: 10.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.303\n",
      "Test accuracy: 0.786\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 2s - loss: 15.8946 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.3050 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 13.9033 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 12.0932 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 9.4911 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 7.2760 - acc: 0.0000e+00\n",
      "CPU times: user 5.13 s, sys: 18.4 ms, total: 5.15 s\n",
      "Wall time: 5.08 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 6.450\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.878\n",
      "Test accuracy: 0.452\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s - loss: 15.8562 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8253 - acc: 0.0000e+00 - val_loss: 15.8211 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8430 - acc: 0.0000e+00 - val_loss: 15.8176 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8449 - acc: 0.0000e+00 - val_loss: 15.8139 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8467 - acc: 0.0000e+00 - val_loss: 15.8110 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8769 - acc: 0.0000e+00 - val_loss: 15.8097 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8235 - acc: 0.0000e+00 - val_loss: 15.8083 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8394 - acc: 0.0000e+00 - val_loss: 15.8072 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8198 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8297 - acc: 0.0000e+00 - val_loss: 15.8041 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7990 - acc: 0.0000e+00 - val_loss: 15.8028 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8072 - acc: 0.0000e+00 - val_loss: 15.8002 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8265 - acc: 0.0000e+00 - val_loss: 15.7963 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8144 - acc: 0.0000e+00 - val_loss: 15.7935 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7940 - acc: 0.0000e+00 - val_loss: 15.7907 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7724 - acc: 0.0000e+00 - val_loss: 15.7881 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7862 - acc: 0.0000e+00 - val_loss: 15.7862 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7892 - acc: 0.0000e+00 - val_loss: 15.7836 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7929 - acc: 0.0000e+00 - val_loss: 15.7817 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7912 - acc: 0.0000e+00 - val_loss: 15.7805 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7854 - acc: 0.0000e+00 - val_loss: 15.7771 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7791 - acc: 0.0000e+00 - val_loss: 15.7738 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7502 - acc: 0.0000e+00 - val_loss: 15.7698 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7419 - acc: 0.0000e+00 - val_loss: 15.7659 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7569 - acc: 0.0000e+00 - val_loss: 15.7610 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7482 - acc: 0.0000e+00 - val_loss: 15.7555 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7163 - acc: 0.0000e+00 - val_loss: 15.7498 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7479 - acc: 0.0000e+00 - val_loss: 15.7444 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7506 - acc: 0.0000e+00 - val_loss: 15.7388 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7217 - acc: 0.0000e+00 - val_loss: 15.7346 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7081 - acc: 0.0000e+00 - val_loss: 15.7293 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7080 - acc: 0.0000e+00 - val_loss: 15.7244 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6837 - acc: 0.0000e+00 - val_loss: 15.7184 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6943 - acc: 0.0000e+00 - val_loss: 15.7126 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6780 - acc: 0.0000e+00 - val_loss: 15.7063 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6797 - acc: 0.0000e+00 - val_loss: 15.6999 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6488 - acc: 0.0000e+00 - val_loss: 15.6936 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6204 - acc: 0.0000e+00 - val_loss: 15.6873 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6907 - acc: 0.0000e+00 - val_loss: 15.6815 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6308 - acc: 0.0000e+00 - val_loss: 15.6751 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5978 - acc: 0.0000e+00 - val_loss: 15.6689 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6265 - acc: 0.0000e+00 - val_loss: 15.6629 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6352 - acc: 0.0000e+00 - val_loss: 15.6564 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6257 - acc: 0.0000e+00 - val_loss: 15.6503 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6111 - acc: 0.0000e+00 - val_loss: 15.6440 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5684 - acc: 0.0000e+00 - val_loss: 15.6367 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5606 - acc: 0.0000e+00 - val_loss: 15.6301 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6238 - acc: 0.0000e+00 - val_loss: 15.6235 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5995 - acc: 0.0000e+00 - val_loss: 15.6162 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5797 - acc: 0.0000e+00 - val_loss: 15.6092 - val_acc: 0.0000e+00\n",
      "CPU times: user 27.1 s, sys: 5.49 s, total: 32.6 s\n",
      "Wall time: 48.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.228\n",
      "Test accuracy: 0.804\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s - loss: 15.8465 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.5110 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 13.6742 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 11.6302 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 9.1157 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 7.1710 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 5.3584 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 3.8297 - acc: 0.0500\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 2.8913 - acc: 0.3000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.4000 - acc: 0.3500\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 2.1188 - acc: 0.3500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.6903 - acc: 0.4000\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.6478 - acc: 0.2000\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.6978 - acc: 0.2500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.3532 - acc: 0.5500\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.4545 - acc: 0.4000\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.4721 - acc: 0.2500\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.4966 - acc: 0.2500\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.4177 - acc: 0.2000\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.3756 - acc: 0.4000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.0784 - acc: 0.7000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.2541 - acc: 0.5000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.2186 - acc: 0.4500\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 1.0311 - acc: 0.6000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.0036 - acc: 0.6500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.9924 - acc: 0.6500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.8474 - acc: 0.7000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.7267 - acc: 0.7500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.6003 - acc: 0.9000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.6543 - acc: 0.8500\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.6344 - acc: 0.7000\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.6229 - acc: 0.7500\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.5708 - acc: 0.8500\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.4553 - acc: 0.9000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.4072 - acc: 0.9000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.3576 - acc: 0.9000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.3510 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.2186 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.2609 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.1577 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.1412 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.1372 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.0790 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.0747 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.0487 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.0599 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.0392 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.0260 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.0354 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.0242 - acc: 1.0000\n",
      "CPU times: user 6.98 s, sys: 147 ms, total: 7.13 s\n",
      "Wall time: 6.88 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 3.720\n",
      "Test accuracy: 0.328\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.711\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s - loss: 15.8709 - acc: 0.0000e+00 - val_loss: 15.8253 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8767 - acc: 0.0000e+00 - val_loss: 15.8219 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8620 - acc: 0.0000e+00 - val_loss: 15.8182 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8471 - acc: 0.0000e+00 - val_loss: 15.8149 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7714 - acc: 0.0000e+00 - val_loss: 15.8124 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8314 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8158 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8206 - acc: 0.0000e+00 - val_loss: 15.8082 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8301 - acc: 0.0000e+00 - val_loss: 15.8067 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8084 - acc: 0.0000e+00 - val_loss: 15.8050 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8257 - acc: 0.0000e+00 - val_loss: 15.8033 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8221 - acc: 0.0000e+00 - val_loss: 15.8006 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8064 - acc: 0.0000e+00 - val_loss: 15.7970 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8266 - acc: 0.0000e+00 - val_loss: 15.7940 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8209 - acc: 0.0000e+00 - val_loss: 15.7917 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8113 - acc: 0.0000e+00 - val_loss: 15.7903 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7825 - acc: 0.0000e+00 - val_loss: 15.7883 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7712 - acc: 0.0000e+00 - val_loss: 15.7865 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7850 - acc: 0.0000e+00 - val_loss: 15.7849 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7596 - acc: 0.0000e+00 - val_loss: 15.7823 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7626 - acc: 0.0000e+00 - val_loss: 15.7792 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7769 - acc: 0.0000e+00 - val_loss: 15.7751 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7717 - acc: 0.0000e+00 - val_loss: 15.7714 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7790 - acc: 0.0000e+00 - val_loss: 15.7673 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7603 - acc: 0.0000e+00 - val_loss: 15.7628 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7289 - acc: 0.0000e+00 - val_loss: 15.7579 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7553 - acc: 0.0000e+00 - val_loss: 15.7520 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7318 - acc: 0.0000e+00 - val_loss: 15.7466 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7485 - acc: 0.0000e+00 - val_loss: 15.7422 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7053 - acc: 0.0000e+00 - val_loss: 15.7356 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7294 - acc: 0.0000e+00 - val_loss: 15.7304 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7462 - acc: 0.0000e+00 - val_loss: 15.7256 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6882 - acc: 0.0000e+00 - val_loss: 15.7196 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6925 - acc: 0.0000e+00 - val_loss: 15.7141 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6873 - acc: 0.0000e+00 - val_loss: 15.7080 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7019 - acc: 0.0000e+00 - val_loss: 15.7019 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6926 - acc: 0.0000e+00 - val_loss: 15.6962 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6878 - acc: 0.0000e+00 - val_loss: 15.6902 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6463 - acc: 0.0000e+00 - val_loss: 15.6846 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6653 - acc: 0.0000e+00 - val_loss: 15.6781 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6760 - acc: 0.0000e+00 - val_loss: 15.6718 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6003 - acc: 0.0000e+00 - val_loss: 15.6658 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6232 - acc: 0.0000e+00 - val_loss: 15.6597 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6056 - acc: 0.0000e+00 - val_loss: 15.6530 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6327 - acc: 0.0000e+00 - val_loss: 15.6468 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6181 - acc: 0.0000e+00 - val_loss: 15.6405 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5841 - acc: 0.0000e+00 - val_loss: 15.6335 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5455 - acc: 0.0000e+00 - val_loss: 15.6263 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5777 - acc: 0.0000e+00 - val_loss: 15.6194 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5755 - acc: 0.0000e+00 - val_loss: 15.6122 - val_acc: 0.0000e+00\n",
      "CPU times: user 28.6 s, sys: 5.4 s, total: 34 s\n",
      "Wall time: 49.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.232\n",
      "Test accuracy: 0.802\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 3s - loss: 15.7391 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 14.3258 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 11.7911 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 9.0557 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 6.4907 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 4.6064 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.3324 - acc: 0.1900\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.7060 - acc: 0.2400\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 2.1270 - acc: 0.2400\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.7132 - acc: 0.3400\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.6372 - acc: 0.3300\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.5883 - acc: 0.3700\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.7138 - acc: 0.2600\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.6555 - acc: 0.3300\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.6003 - acc: 0.3300\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.5550 - acc: 0.3600\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.4058 - acc: 0.3700\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.3735 - acc: 0.3700\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.3323 - acc: 0.4000\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.3381 - acc: 0.4600\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.1994 - acc: 0.4700\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.2936 - acc: 0.4300\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.1390 - acc: 0.5000\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.1494 - acc: 0.4400\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.1405 - acc: 0.4400\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.0954 - acc: 0.4700\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.0161 - acc: 0.4700\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 0.9793 - acc: 0.6300\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 0.9361 - acc: 0.5800\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 0.7997 - acc: 0.7100\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.8870 - acc: 0.5300\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.7889 - acc: 0.6200\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.7769 - acc: 0.6400\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.8008 - acc: 0.5800\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.7113 - acc: 0.7200\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.7424 - acc: 0.6600\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.6854 - acc: 0.7300\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.6502 - acc: 0.7600\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.5943 - acc: 0.8100\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.5660 - acc: 0.7900\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.4980 - acc: 0.8200\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.4936 - acc: 0.8100\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.4609 - acc: 0.8400\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.4522 - acc: 0.8700\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.4114 - acc: 0.8300\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.3423 - acc: 0.9000\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.3551 - acc: 0.9300\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.3382 - acc: 0.9300\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.3335 - acc: 0.9400\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.2937 - acc: 0.9200\n",
      "CPU times: user 8.74 s, sys: 608 ms, total: 9.35 s\n",
      "Wall time: 10.2 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 4.099\n",
      "Test accuracy: 0.306\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.313\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s - loss: 15.8968 - acc: 0.0000e+00 - val_loss: 15.8251 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8541 - acc: 0.0000e+00 - val_loss: 15.8217 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8625 - acc: 0.0000e+00 - val_loss: 15.8184 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8569 - acc: 0.0000e+00 - val_loss: 15.8148 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8417 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8432 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8117 - acc: 0.0000e+00 - val_loss: 15.8085 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8205 - acc: 0.0000e+00 - val_loss: 15.8078 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8259 - acc: 0.0000e+00 - val_loss: 15.8063 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8442 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8171 - acc: 0.0000e+00 - val_loss: 15.8038 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8247 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7929 - acc: 0.0000e+00 - val_loss: 15.7979 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8137 - acc: 0.0000e+00 - val_loss: 15.7947 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7942 - acc: 0.0000e+00 - val_loss: 15.7915 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7944 - acc: 0.0000e+00 - val_loss: 15.7897 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7731 - acc: 0.0000e+00 - val_loss: 15.7873 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7760 - acc: 0.0000e+00 - val_loss: 15.7849 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7814 - acc: 0.0000e+00 - val_loss: 15.7827 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7790 - acc: 0.0000e+00 - val_loss: 15.7817 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7449 - acc: 0.0000e+00 - val_loss: 15.7796 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7692 - acc: 0.0000e+00 - val_loss: 15.7758 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7493 - acc: 0.0000e+00 - val_loss: 15.7712 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7858 - acc: 0.0000e+00 - val_loss: 15.7668 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7666 - acc: 0.0000e+00 - val_loss: 15.7614 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7569 - acc: 0.0000e+00 - val_loss: 15.7560 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7462 - acc: 0.0000e+00 - val_loss: 15.7503 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7172 - acc: 0.0000e+00 - val_loss: 15.7452 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7077 - acc: 0.0000e+00 - val_loss: 15.7403 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7526 - acc: 0.0000e+00 - val_loss: 15.7353 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7359 - acc: 0.0000e+00 - val_loss: 15.7303 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6842 - acc: 0.0000e+00 - val_loss: 15.7245 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6859 - acc: 0.0000e+00 - val_loss: 15.7185 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6668 - acc: 0.0000e+00 - val_loss: 15.7124 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7232 - acc: 0.0000e+00 - val_loss: 15.7064 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6787 - acc: 0.0000e+00 - val_loss: 15.7005 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6721 - acc: 0.0000e+00 - val_loss: 15.6945 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6311 - acc: 0.0000e+00 - val_loss: 15.6882 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6618 - acc: 0.0000e+00 - val_loss: 15.6820 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6506 - acc: 0.0000e+00 - val_loss: 15.6759 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6305 - acc: 0.0000e+00 - val_loss: 15.6699 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6567 - acc: 0.0000e+00 - val_loss: 15.6639 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6405 - acc: 0.0000e+00 - val_loss: 15.6580 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5884 - acc: 0.0000e+00 - val_loss: 15.6513 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6044 - acc: 0.0000e+00 - val_loss: 15.6449 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5905 - acc: 0.0000e+00 - val_loss: 15.6379 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5543 - acc: 0.0000e+00 - val_loss: 15.6299 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5920 - acc: 0.0000e+00 - val_loss: 15.6226 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5790 - acc: 0.0000e+00 - val_loss: 15.6154 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5717 - acc: 0.0000e+00 - val_loss: 15.6079 - val_acc: 0.0000e+00\n",
      "CPU times: user 28.3 s, sys: 5.42 s, total: 33.7 s\n",
      "Wall time: 49.7 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.227\n",
      "Test accuracy: 0.804\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 3s - loss: 15.6478 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.2605 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 11.5321 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 8.7033 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.1665 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.4041 - acc: 0.0250\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.2423 - acc: 0.2250\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.6099 - acc: 0.2400\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 2.0801 - acc: 0.2650\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.7598 - acc: 0.3650\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6475 - acc: 0.3200\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.7523 - acc: 0.2850\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.7048 - acc: 0.2750\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.7555 - acc: 0.2950\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.6138 - acc: 0.3050\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.5160 - acc: 0.3100\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.4194 - acc: 0.3250\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.3967 - acc: 0.4000\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.3696 - acc: 0.3900\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.3915 - acc: 0.4150\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.3470 - acc: 0.4250\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.3142 - acc: 0.4200\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.2429 - acc: 0.4250\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.2038 - acc: 0.4200\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.1983 - acc: 0.4100\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.1906 - acc: 0.3850\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.1346 - acc: 0.4400\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.0855 - acc: 0.4850\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.1226 - acc: 0.4500\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.0349 - acc: 0.4950\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 0.9915 - acc: 0.4900\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 1.0307 - acc: 0.4950\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 1.0160 - acc: 0.4350\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 0.9473 - acc: 0.4650\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 0.9332 - acc: 0.5100\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 0.9267 - acc: 0.5000\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 0.8404 - acc: 0.6000\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 0.8483 - acc: 0.5650\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 0.8349 - acc: 0.5550\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 0.8415 - acc: 0.5550\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 0.7706 - acc: 0.6300\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 0.7915 - acc: 0.5800\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 0.7476 - acc: 0.5950\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.7219 - acc: 0.6650\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.7465 - acc: 0.5900\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.7368 - acc: 0.6100\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.7084 - acc: 0.6050\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.6400 - acc: 0.7050\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.6651 - acc: 0.6650\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.6419 - acc: 0.6350\n",
      "CPU times: user 10.9 s, sys: 1.16 s, total: 12.1 s\n",
      "Wall time: 14.5 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 2.754\n",
      "Test accuracy: 0.350\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.251\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s - loss: 15.8677 - acc: 0.0000e+00 - val_loss: 15.8242 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8498 - acc: 0.0000e+00 - val_loss: 15.8208 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8293 - acc: 0.0000e+00 - val_loss: 15.8173 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8700 - acc: 0.0000e+00 - val_loss: 15.8132 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8452 - acc: 0.0000e+00 - val_loss: 15.8107 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8183 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8237 - acc: 0.0000e+00 - val_loss: 15.8082 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8106 - acc: 0.0000e+00 - val_loss: 15.8066 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8254 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8198 - acc: 0.0000e+00 - val_loss: 15.8036 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7993 - acc: 0.0000e+00 - val_loss: 15.8023 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7979 - acc: 0.0000e+00 - val_loss: 15.7999 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8006 - acc: 0.0000e+00 - val_loss: 15.7963 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8127 - acc: 0.0000e+00 - val_loss: 15.7933 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7787 - acc: 0.0000e+00 - val_loss: 15.7896 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8177 - acc: 0.0000e+00 - val_loss: 15.7866 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8078 - acc: 0.0000e+00 - val_loss: 15.7847 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7730 - acc: 0.0000e+00 - val_loss: 15.7822 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7773 - acc: 0.0000e+00 - val_loss: 15.7804 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7648 - acc: 0.0000e+00 - val_loss: 15.7787 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7667 - acc: 0.0000e+00 - val_loss: 15.7759 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7854 - acc: 0.0000e+00 - val_loss: 15.7729 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7845 - acc: 0.0000e+00 - val_loss: 15.7684 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7486 - acc: 0.0000e+00 - val_loss: 15.7630 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7731 - acc: 0.0000e+00 - val_loss: 15.7586 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7551 - acc: 0.0000e+00 - val_loss: 15.7532 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7227 - acc: 0.0000e+00 - val_loss: 15.7483 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7437 - acc: 0.0000e+00 - val_loss: 15.7432 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7286 - acc: 0.0000e+00 - val_loss: 15.7380 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7225 - acc: 0.0000e+00 - val_loss: 15.7331 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7245 - acc: 0.0000e+00 - val_loss: 15.7283 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7117 - acc: 0.0000e+00 - val_loss: 15.7233 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6896 - acc: 0.0000e+00 - val_loss: 15.7178 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6850 - acc: 0.0000e+00 - val_loss: 15.7115 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6775 - acc: 0.0000e+00 - val_loss: 15.7055 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6735 - acc: 0.0000e+00 - val_loss: 15.6992 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7268 - acc: 0.0000e+00 - val_loss: 15.6932 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6710 - acc: 0.0000e+00 - val_loss: 15.6873 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6216 - acc: 0.0000e+00 - val_loss: 15.6806 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6309 - acc: 0.0000e+00 - val_loss: 15.6747 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6210 - acc: 0.0000e+00 - val_loss: 15.6684 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5683 - acc: 0.0000e+00 - val_loss: 15.6618 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6364 - acc: 0.0000e+00 - val_loss: 15.6554 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5980 - acc: 0.0000e+00 - val_loss: 15.6487 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6329 - acc: 0.0000e+00 - val_loss: 15.6426 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6019 - acc: 0.0000e+00 - val_loss: 15.6362 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5916 - acc: 0.0000e+00 - val_loss: 15.6291 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5895 - acc: 0.0000e+00 - val_loss: 15.6221 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5632 - acc: 0.0000e+00 - val_loss: 15.6148 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5534 - acc: 0.0000e+00 - val_loss: 15.6078 - val_acc: 0.0000e+00\n",
      "CPU times: user 29.5 s, sys: 5.55 s, total: 35 s\n",
      "Wall time: 51.1 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.227\n",
      "Test accuracy: 0.804\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 4s - loss: 15.3853 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 11.2855 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 6.0637 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.0845 - acc: 0.1600     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 2.0451 - acc: 0.3033     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.6785 - acc: 0.2667     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.6986 - acc: 0.2500     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.6619 - acc: 0.2633     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.5320 - acc: 0.2767     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.4958 - acc: 0.3700     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.5171 - acc: 0.3867     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.3967 - acc: 0.3867     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.3836 - acc: 0.3600     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.3496 - acc: 0.3533     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.2930 - acc: 0.3867     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.2641 - acc: 0.4233     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.2136 - acc: 0.4333     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.1809 - acc: 0.4367     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.1617 - acc: 0.4233     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.1383 - acc: 0.4433     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.1071 - acc: 0.4467     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.0554 - acc: 0.4933     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.0063 - acc: 0.5500     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 1.0211 - acc: 0.4933     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 1.0091 - acc: 0.4967     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 1.0223 - acc: 0.4633     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 0.9551 - acc: 0.5400     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.8908 - acc: 0.5900     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.9410 - acc: 0.5333     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.8996 - acc: 0.5467     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.8839 - acc: 0.5567     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.8496 - acc: 0.5400     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.7780 - acc: 0.6067     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.7514 - acc: 0.6267     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.7853 - acc: 0.6100     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.7202 - acc: 0.6567     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.7124 - acc: 0.6267     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.6510 - acc: 0.7167     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.6696 - acc: 0.6867     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.6188 - acc: 0.7000     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.5902 - acc: 0.7433     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.5965 - acc: 0.7033     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.6249 - acc: 0.7100     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.6564 - acc: 0.6833     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.5641 - acc: 0.7300     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.5956 - acc: 0.7200     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.6105 - acc: 0.6867     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.5825 - acc: 0.7433     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.6501 - acc: 0.7233     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.6028 - acc: 0.7300     \n",
      "CPU times: user 14 s, sys: 1.8 s, total: 15.8 s\n",
      "Wall time: 19.3 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 2.246\n",
      "Test accuracy: 0.410\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.591\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8340 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8324 - acc: 0.0000e+00 - val_loss: 15.8217 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8470 - acc: 0.0000e+00 - val_loss: 15.8182 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8580 - acc: 0.0000e+00 - val_loss: 15.8145 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8092 - acc: 0.0000e+00 - val_loss: 15.8122 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8562 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8488 - acc: 0.0000e+00 - val_loss: 15.8091 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8651 - acc: 0.0000e+00 - val_loss: 15.8084 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8283 - acc: 0.0000e+00 - val_loss: 15.8067 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8169 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8521 - acc: 0.0000e+00 - val_loss: 15.8041 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8074 - acc: 0.0000e+00 - val_loss: 15.8018 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7940 - acc: 0.0000e+00 - val_loss: 15.7983 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8007 - acc: 0.0000e+00 - val_loss: 15.7954 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8085 - acc: 0.0000e+00 - val_loss: 15.7925 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7907 - acc: 0.0000e+00 - val_loss: 15.7908 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7679 - acc: 0.0000e+00 - val_loss: 15.7886 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7635 - acc: 0.0000e+00 - val_loss: 15.7863 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7078 - acc: 0.0000e+00 - val_loss: 15.7842 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7880 - acc: 0.0000e+00 - val_loss: 15.7822 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7542 - acc: 0.0000e+00 - val_loss: 15.7797 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7965 - acc: 0.0000e+00 - val_loss: 15.7758 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7629 - acc: 0.0000e+00 - val_loss: 15.7711 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8078 - acc: 0.0000e+00 - val_loss: 15.7670 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7456 - acc: 0.0000e+00 - val_loss: 15.7622 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7162 - acc: 0.0000e+00 - val_loss: 15.7569 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7491 - acc: 0.0000e+00 - val_loss: 15.7510 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7108 - acc: 0.0000e+00 - val_loss: 15.7455 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7097 - acc: 0.0000e+00 - val_loss: 15.7399 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6975 - acc: 0.0000e+00 - val_loss: 15.7345 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7066 - acc: 0.0000e+00 - val_loss: 15.7289 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7284 - acc: 0.0000e+00 - val_loss: 15.7231 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7088 - acc: 0.0000e+00 - val_loss: 15.7177 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6805 - acc: 0.0000e+00 - val_loss: 15.7118 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6831 - acc: 0.0000e+00 - val_loss: 15.7055 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6466 - acc: 0.0000e+00 - val_loss: 15.6988 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6958 - acc: 0.0000e+00 - val_loss: 15.6925 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6344 - acc: 0.0000e+00 - val_loss: 15.6868 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6417 - acc: 0.0000e+00 - val_loss: 15.6811 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6418 - acc: 0.0000e+00 - val_loss: 15.6744 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6555 - acc: 0.0000e+00 - val_loss: 15.6683 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6125 - acc: 0.0000e+00 - val_loss: 15.6624 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6110 - acc: 0.0000e+00 - val_loss: 15.6551 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6198 - acc: 0.0000e+00 - val_loss: 15.6483 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5826 - acc: 0.0000e+00 - val_loss: 15.6417 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6025 - acc: 0.0000e+00 - val_loss: 15.6347 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5726 - acc: 0.0000e+00 - val_loss: 15.6278 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5529 - acc: 0.0000e+00 - val_loss: 15.6203 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5346 - acc: 0.0000e+00 - val_loss: 15.6126 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5376 - acc: 0.0000e+00 - val_loss: 15.6055 - val_acc: 0.0000e+00\n",
      "CPU times: user 29 s, sys: 5.57 s, total: 34.6 s\n",
      "Wall time: 50.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.226\n",
      "Test accuracy: 0.802\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 4s - loss: 15.0310 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 10.5088 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 5.4596 - acc: 0.0100         \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 2.9295 - acc: 0.2175     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.9295 - acc: 0.3100     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.6083 - acc: 0.2725     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.6803 - acc: 0.2525     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.5153 - acc: 0.3325     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.3652 - acc: 0.4200     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.3993 - acc: 0.4025     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.3319 - acc: 0.4300     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.2694 - acc: 0.4200     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.2238 - acc: 0.4025     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.2219 - acc: 0.4350     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.2004 - acc: 0.4350     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.1234 - acc: 0.4725     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.0670 - acc: 0.4950     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.0563 - acc: 0.5150     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.0015 - acc: 0.5050     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 0.9672 - acc: 0.5675     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 0.9450 - acc: 0.5425     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 0.8745 - acc: 0.5950     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.8673 - acc: 0.5850     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.8268 - acc: 0.6050     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.7682 - acc: 0.6325     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.7166 - acc: 0.6600     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.6850 - acc: 0.6850     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.6614 - acc: 0.6900     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.6459 - acc: 0.7050     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.5808 - acc: 0.7775     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.5593 - acc: 0.7825     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.5292 - acc: 0.8025     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.4571 - acc: 0.8550     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.4139 - acc: 0.8525     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.4021 - acc: 0.8600     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.3503 - acc: 0.8525     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.2961 - acc: 0.9150     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.2709 - acc: 0.9200     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.2394 - acc: 0.9325     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.1784 - acc: 0.9550     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.2182 - acc: 0.9200     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.1163 - acc: 0.9725     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.1647 - acc: 0.9425     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.1422 - acc: 0.9550     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.1688 - acc: 0.9425     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1245 - acc: 0.9525     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.1470 - acc: 0.9475     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.0894 - acc: 0.9750     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.1041 - acc: 0.9600     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.1095 - acc: 0.9550     \n",
      "CPU times: user 16.2 s, sys: 2.36 s, total: 18.6 s\n",
      "Wall time: 23.6 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 3.815\n",
      "Test accuracy: 0.456\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.698\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8499 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8525 - acc: 0.0000e+00 - val_loss: 15.8208 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8525 - acc: 0.0000e+00 - val_loss: 15.8174 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8747 - acc: 0.0000e+00 - val_loss: 15.8144 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8302 - acc: 0.0000e+00 - val_loss: 15.8117 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8049 - acc: 0.0000e+00 - val_loss: 15.8101 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8228 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8334 - acc: 0.0000e+00 - val_loss: 15.8078 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8396 - acc: 0.0000e+00 - val_loss: 15.8061 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8312 - acc: 0.0000e+00 - val_loss: 15.8046 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8203 - acc: 0.0000e+00 - val_loss: 15.8031 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8035 - acc: 0.0000e+00 - val_loss: 15.8005 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7666 - acc: 0.0000e+00 - val_loss: 15.7969 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8175 - acc: 0.0000e+00 - val_loss: 15.7939 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8002 - acc: 0.0000e+00 - val_loss: 15.7911 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7820 - acc: 0.0000e+00 - val_loss: 15.7886 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7894 - acc: 0.0000e+00 - val_loss: 15.7862 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7947 - acc: 0.0000e+00 - val_loss: 15.7841 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7734 - acc: 0.0000e+00 - val_loss: 15.7819 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7821 - acc: 0.0000e+00 - val_loss: 15.7804 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7493 - acc: 0.0000e+00 - val_loss: 15.7777 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7770 - acc: 0.0000e+00 - val_loss: 15.7740 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7737 - acc: 0.0000e+00 - val_loss: 15.7702 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7359 - acc: 0.0000e+00 - val_loss: 15.7658 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7465 - acc: 0.0000e+00 - val_loss: 15.7609 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7554 - acc: 0.0000e+00 - val_loss: 15.7543 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7261 - acc: 0.0000e+00 - val_loss: 15.7491 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7183 - acc: 0.0000e+00 - val_loss: 15.7444 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7411 - acc: 0.0000e+00 - val_loss: 15.7392 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6852 - acc: 0.0000e+00 - val_loss: 15.7332 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7044 - acc: 0.0000e+00 - val_loss: 15.7274 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7268 - acc: 0.0000e+00 - val_loss: 15.7225 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6721 - acc: 0.0000e+00 - val_loss: 15.7165 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7086 - acc: 0.0000e+00 - val_loss: 15.7108 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6951 - acc: 0.0000e+00 - val_loss: 15.7049 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6818 - acc: 0.0000e+00 - val_loss: 15.6987 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6528 - acc: 0.0000e+00 - val_loss: 15.6923 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6377 - acc: 0.0000e+00 - val_loss: 15.6859 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6579 - acc: 0.0000e+00 - val_loss: 15.6799 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6661 - acc: 0.0000e+00 - val_loss: 15.6735 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6310 - acc: 0.0000e+00 - val_loss: 15.6677 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6281 - acc: 0.0000e+00 - val_loss: 15.6615 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6188 - acc: 0.0000e+00 - val_loss: 15.6551 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5791 - acc: 0.0000e+00 - val_loss: 15.6489 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5554 - acc: 0.0000e+00 - val_loss: 15.6419 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5926 - acc: 0.0000e+00 - val_loss: 15.6351 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5769 - acc: 0.0000e+00 - val_loss: 15.6283 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5699 - acc: 0.0000e+00 - val_loss: 15.6204 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6064 - acc: 0.0000e+00 - val_loss: 15.6135 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5724 - acc: 0.0000e+00 - val_loss: 15.6061 - val_acc: 0.0000e+00\n",
      "CPU times: user 29.4 s, sys: 5.48 s, total: 34.9 s\n",
      "Wall time: 50.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.802\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 4s - loss: 14.9471 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.2175 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.3650 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.8436 - acc: 0.1980     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.8750 - acc: 0.2580     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.7221 - acc: 0.2280     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.7232 - acc: 0.2600     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6081 - acc: 0.2720     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.5237 - acc: 0.2940     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.4684 - acc: 0.3680     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.4006 - acc: 0.3680     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.3112 - acc: 0.3840     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.2969 - acc: 0.3800     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.2623 - acc: 0.4000     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.1982 - acc: 0.4360     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.1683 - acc: 0.4900     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.1324 - acc: 0.4440     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.0844 - acc: 0.4680     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.0684 - acc: 0.4780     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.0352 - acc: 0.4880     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.9961 - acc: 0.4900     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.9859 - acc: 0.5180     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.9592 - acc: 0.5180     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.8986 - acc: 0.5620     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.8392 - acc: 0.5880     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.8426 - acc: 0.5800     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.7894 - acc: 0.6500     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.7495 - acc: 0.6660     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.6990 - acc: 0.6880     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.6616 - acc: 0.6980     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.6274 - acc: 0.7480     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.5988 - acc: 0.7700     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.5137 - acc: 0.8440     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.4796 - acc: 0.8320     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.4239 - acc: 0.8660     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.3680 - acc: 0.8800     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.3663 - acc: 0.8540     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.3114 - acc: 0.8880     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.2428 - acc: 0.9240     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.2464 - acc: 0.9300     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.2476 - acc: 0.9100     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.2218 - acc: 0.9220     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.1822 - acc: 0.9480     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.1781 - acc: 0.9480     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.1548 - acc: 0.9460     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.1241 - acc: 0.9560     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.1138 - acc: 0.9600     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.1295 - acc: 0.9580     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0995 - acc: 0.9780     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0658 - acc: 0.9880     \n",
      "CPU times: user 18.1 s, sys: 2.85 s, total: 21 s\n",
      "Wall time: 27.6 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 3.346\n",
      "Test accuracy: 0.520\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.989\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_aquatic_mammals[:num_train_images], Y_train_aquatic_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium_sized -> reptilesm with dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 7s - loss: 15.8698 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8677 - acc: 0.0000e+00 - val_loss: 15.8214 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8645 - acc: 0.0000e+00 - val_loss: 15.8182 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8709 - acc: 0.0000e+00 - val_loss: 15.8146 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.7955 - acc: 0.0000e+00 - val_loss: 15.8120 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8047 - acc: 0.0000e+00 - val_loss: 15.8101 - val_acc: 0.0000e+00\n",
      "CPU times: user 11.8 s, sys: 647 ms, total: 12.4 s\n",
      "Wall time: 14.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.312\n",
      "Test accuracy: 0.784\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 6s - loss: 14.1292 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 13.4150 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 10.2112 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 8.5721 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.4097 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 3.7624 - acc: 0.0000e+00\n",
      "CPU times: user 10.7 s, sys: 40.1 ms, total: 10.8 s\n",
      "Wall time: 10.6 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 11.662\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.936\n",
      "Test accuracy: 0.604\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 8s - loss: 15.8442 - acc: 0.0000e+00 - val_loss: 15.8243 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8678 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8553 - acc: 0.0000e+00 - val_loss: 15.8173 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8309 - acc: 0.0000e+00 - val_loss: 15.8142 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8515 - acc: 0.0000e+00 - val_loss: 15.8117 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8381 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "CPU times: user 12.2 s, sys: 663 ms, total: 12.9 s\n",
      "Wall time: 14.7 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.307\n",
      "Test accuracy: 0.786\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 6s - loss: 15.8000 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 16.0797 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 15.5808 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 14.5588 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 12.5530 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 10.9279 - acc: 0.0000e+00\n",
      "CPU times: user 9.5 s, sys: 43.3 ms, total: 9.55 s\n",
      "Wall time: 9.44 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 10.771\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.831\n",
      "Test accuracy: 0.550\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 8s - loss: 15.8486 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8678 - acc: 0.0000e+00 - val_loss: 15.8216 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8418 - acc: 0.0000e+00 - val_loss: 15.8180 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8723 - acc: 0.0000e+00 - val_loss: 15.8149 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8308 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8157 - acc: 0.0000e+00 - val_loss: 15.8106 - val_acc: 0.0000e+00\n",
      "CPU times: user 12.5 s, sys: 680 ms, total: 13.2 s\n",
      "Wall time: 15 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.310\n",
      "Test accuracy: 0.784\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 7s - loss: 15.8474 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.8226 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 15.5031 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 13.3339 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 11.5512 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 9.0417 - acc: 0.0000e+00\n",
      "CPU times: user 9.83 s, sys: 48.3 ms, total: 9.88 s\n",
      "Wall time: 9.77 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 8.504\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.276\n",
      "Test accuracy: 0.448\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 9s - loss: 15.8373 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8776 - acc: 0.0000e+00 - val_loss: 15.8210 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8677 - acc: 0.0000e+00 - val_loss: 15.8179 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8384 - acc: 0.0000e+00 - val_loss: 15.8142 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8358 - acc: 0.0000e+00 - val_loss: 15.8118 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8395 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "CPU times: user 12.9 s, sys: 702 ms, total: 13.6 s\n",
      "Wall time: 15.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.308\n",
      "Test accuracy: 0.784\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 7s - loss: 15.9831 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.7374 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 14.9254 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 13.2056 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 11.1487 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 8.7462 - acc: 0.0000e+00\n",
      "CPU times: user 10.1 s, sys: 59.6 ms, total: 10.1 s\n",
      "Wall time: 10 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 6.644\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.161\n",
      "Test accuracy: 0.370\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 9s - loss: 15.8675 - acc: 0.0000e+00 - val_loss: 15.8247 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8228 - acc: 0.0000e+00 - val_loss: 15.8210 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8559 - acc: 0.0000e+00 - val_loss: 15.8175 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8515 - acc: 0.0000e+00 - val_loss: 15.8139 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8351 - acc: 0.0000e+00 - val_loss: 15.8116 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8501 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8500 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8351 - acc: 0.0000e+00 - val_loss: 15.8086 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8088 - acc: 0.0000e+00 - val_loss: 15.8073 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8228 - acc: 0.0000e+00 - val_loss: 15.8059 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8032 - acc: 0.0000e+00 - val_loss: 15.8050 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8444 - acc: 0.0000e+00 - val_loss: 15.8022 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8012 - acc: 0.0000e+00 - val_loss: 15.7981 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7980 - acc: 0.0000e+00 - val_loss: 15.7948 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7991 - acc: 0.0000e+00 - val_loss: 15.7917 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7890 - acc: 0.0000e+00 - val_loss: 15.7894 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7857 - acc: 0.0000e+00 - val_loss: 15.7881 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7916 - acc: 0.0000e+00 - val_loss: 15.7858 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7589 - acc: 0.0000e+00 - val_loss: 15.7839 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7948 - acc: 0.0000e+00 - val_loss: 15.7829 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7618 - acc: 0.0000e+00 - val_loss: 15.7798 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7677 - acc: 0.0000e+00 - val_loss: 15.7760 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7625 - acc: 0.0000e+00 - val_loss: 15.7710 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7491 - acc: 0.0000e+00 - val_loss: 15.7668 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7707 - acc: 0.0000e+00 - val_loss: 15.7621 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7636 - acc: 0.0000e+00 - val_loss: 15.7565 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7304 - acc: 0.0000e+00 - val_loss: 15.7510 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7192 - acc: 0.0000e+00 - val_loss: 15.7455 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7184 - acc: 0.0000e+00 - val_loss: 15.7397 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7200 - acc: 0.0000e+00 - val_loss: 15.7342 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7081 - acc: 0.0000e+00 - val_loss: 15.7288 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6996 - acc: 0.0000e+00 - val_loss: 15.7221 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7037 - acc: 0.0000e+00 - val_loss: 15.7160 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6678 - acc: 0.0000e+00 - val_loss: 15.7105 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6838 - acc: 0.0000e+00 - val_loss: 15.7048 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6700 - acc: 0.0000e+00 - val_loss: 15.6988 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6710 - acc: 0.0000e+00 - val_loss: 15.6934 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6388 - acc: 0.0000e+00 - val_loss: 15.6871 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6578 - acc: 0.0000e+00 - val_loss: 15.6805 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6546 - acc: 0.0000e+00 - val_loss: 15.6746 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6389 - acc: 0.0000e+00 - val_loss: 15.6682 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6850 - acc: 0.0000e+00 - val_loss: 15.6624 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6153 - acc: 0.0000e+00 - val_loss: 15.6562 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6151 - acc: 0.0000e+00 - val_loss: 15.6496 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6002 - acc: 0.0000e+00 - val_loss: 15.6434 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6179 - acc: 0.0000e+00 - val_loss: 15.6374 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5946 - acc: 0.0000e+00 - val_loss: 15.6309 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6053 - acc: 0.0000e+00 - val_loss: 15.6241 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5948 - acc: 0.0000e+00 - val_loss: 15.6172 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5824 - acc: 0.0000e+00 - val_loss: 15.6097 - val_acc: 0.0000e+00\n",
      "CPU times: user 33.3 s, sys: 5.45 s, total: 38.8 s\n",
      "Wall time: 54.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.233\n",
      "Test accuracy: 0.802\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 7s - loss: 15.5906 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.0095 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 13.6123 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 11.1075 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 8.4408 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 6.1071 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 4.1650 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 3.1476 - acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 2.2162 - acc: 0.2500\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 1.9380 - acc: 0.2000\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.7452 - acc: 0.3000\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.6943 - acc: 0.3500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.6590 - acc: 0.4000\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.4768 - acc: 0.4000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.4216 - acc: 0.4500\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.4463 - acc: 0.3500\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.5587 - acc: 0.4000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.3962 - acc: 0.3000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.5661 - acc: 0.2500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.4022 - acc: 0.4500\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.3952 - acc: 0.4500\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.3404 - acc: 0.4000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.3999 - acc: 0.3000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 1.4332 - acc: 0.3000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.3448 - acc: 0.4000\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 1.2096 - acc: 0.5000\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 1.2805 - acc: 0.5000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 1.3320 - acc: 0.5000\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 1.3118 - acc: 0.5000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 1.1178 - acc: 0.5000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.9927 - acc: 0.7000\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.8205 - acc: 0.7000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.8689 - acc: 0.7000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.9179 - acc: 0.6500\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.8699 - acc: 0.7500\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.7506 - acc: 0.7500\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.9020 - acc: 0.6500\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.8566 - acc: 0.6500\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.8049 - acc: 0.6500\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.7402 - acc: 0.6000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.6829 - acc: 0.8000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.6168 - acc: 0.7000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.6182 - acc: 0.8000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.5972 - acc: 0.7500\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.4514 - acc: 0.8000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.5266 - acc: 0.8500\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.4317 - acc: 0.8500\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.3580 - acc: 0.9000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.4460 - acc: 0.8000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.3497 - acc: 0.9000\n",
      "CPU times: user 12 s, sys: 209 ms, total: 12.3 s\n",
      "Wall time: 12 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 3.398\n",
      "Test accuracy: 0.284\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.213\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 9s - loss: 15.8546 - acc: 0.0000e+00 - val_loss: 15.8249 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8468 - acc: 0.0000e+00 - val_loss: 15.8217 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8537 - acc: 0.0000e+00 - val_loss: 15.8180 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8241 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8207 - acc: 0.0000e+00 - val_loss: 15.8114 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8287 - acc: 0.0000e+00 - val_loss: 15.8095 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8130 - acc: 0.0000e+00 - val_loss: 15.8081 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8125 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8032 - acc: 0.0000e+00 - val_loss: 15.8054 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8154 - acc: 0.0000e+00 - val_loss: 15.8035 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8264 - acc: 0.0000e+00 - val_loss: 15.8021 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8082 - acc: 0.0000e+00 - val_loss: 15.7992 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8084 - acc: 0.0000e+00 - val_loss: 15.7961 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7869 - acc: 0.0000e+00 - val_loss: 15.7928 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7893 - acc: 0.0000e+00 - val_loss: 15.7902 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8022 - acc: 0.0000e+00 - val_loss: 15.7880 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7822 - acc: 0.0000e+00 - val_loss: 15.7860 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7875 - acc: 0.0000e+00 - val_loss: 15.7840 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7915 - acc: 0.0000e+00 - val_loss: 15.7827 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7615 - acc: 0.0000e+00 - val_loss: 15.7805 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7654 - acc: 0.0000e+00 - val_loss: 15.7774 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7698 - acc: 0.0000e+00 - val_loss: 15.7744 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7385 - acc: 0.0000e+00 - val_loss: 15.7696 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7655 - acc: 0.0000e+00 - val_loss: 15.7654 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7785 - acc: 0.0000e+00 - val_loss: 15.7601 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7355 - acc: 0.0000e+00 - val_loss: 15.7546 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7205 - acc: 0.0000e+00 - val_loss: 15.7487 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7263 - acc: 0.0000e+00 - val_loss: 15.7433 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7049 - acc: 0.0000e+00 - val_loss: 15.7379 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6799 - acc: 0.0000e+00 - val_loss: 15.7328 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7245 - acc: 0.0000e+00 - val_loss: 15.7271 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7053 - acc: 0.0000e+00 - val_loss: 15.7217 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6938 - acc: 0.0000e+00 - val_loss: 15.7158 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6794 - acc: 0.0000e+00 - val_loss: 15.7105 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6872 - acc: 0.0000e+00 - val_loss: 15.7045 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6799 - acc: 0.0000e+00 - val_loss: 15.6984 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6519 - acc: 0.0000e+00 - val_loss: 15.6915 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6700 - acc: 0.0000e+00 - val_loss: 15.6851 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6801 - acc: 0.0000e+00 - val_loss: 15.6787 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6379 - acc: 0.0000e+00 - val_loss: 15.6723 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6228 - acc: 0.0000e+00 - val_loss: 15.6664 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6216 - acc: 0.0000e+00 - val_loss: 15.6597 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6284 - acc: 0.0000e+00 - val_loss: 15.6534 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6273 - acc: 0.0000e+00 - val_loss: 15.6467 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6172 - acc: 0.0000e+00 - val_loss: 15.6399 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6224 - acc: 0.0000e+00 - val_loss: 15.6336 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5954 - acc: 0.0000e+00 - val_loss: 15.6266 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5761 - acc: 0.0000e+00 - val_loss: 15.6197 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5668 - acc: 0.0000e+00 - val_loss: 15.6122 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5496 - acc: 0.0000e+00 - val_loss: 15.6052 - val_acc: 0.0000e+00\n",
      "CPU times: user 34 s, sys: 5.48 s, total: 39.5 s\n",
      "Wall time: 55.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.228\n",
      "Test accuracy: 0.804\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 8s - loss: 15.7281 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 14.6803 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 12.2689 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 9.5241 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 6.9326 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 4.9247 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.3071 - acc: 0.0100\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.4174 - acc: 0.2100\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 1.9815 - acc: 0.2500\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.7101 - acc: 0.2600\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.7237 - acc: 0.2300\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.6799 - acc: 0.1900\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.6224 - acc: 0.2800\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.6381 - acc: 0.2300\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.5691 - acc: 0.3500\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.5601 - acc: 0.3100\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.5181 - acc: 0.4300\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.5586 - acc: 0.3700\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.5669 - acc: 0.3400\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.4279 - acc: 0.3700\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.5324 - acc: 0.3300\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.3390 - acc: 0.4500\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.4013 - acc: 0.3500\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.3781 - acc: 0.3500\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.2656 - acc: 0.4700\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.2297 - acc: 0.4900\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.2435 - acc: 0.4500\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.1950 - acc: 0.5100\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 1.1128 - acc: 0.5200\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 1.0523 - acc: 0.5600\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.9794 - acc: 0.6000\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.9373 - acc: 0.6200\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.9327 - acc: 0.5900\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.8752 - acc: 0.6700\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.8042 - acc: 0.7400\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.8023 - acc: 0.7000\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.7365 - acc: 0.7300\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.7294 - acc: 0.7100\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.6814 - acc: 0.7700\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.6295 - acc: 0.7700\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.5849 - acc: 0.8000\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.5244 - acc: 0.8500\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.5218 - acc: 0.8300\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.4755 - acc: 0.8300\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.4581 - acc: 0.8300\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.3718 - acc: 0.9300\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.3802 - acc: 0.9000\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.3386 - acc: 0.9000\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.2575 - acc: 0.9500\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.2586 - acc: 0.9200\n",
      "CPU times: user 15.7 s, sys: 610 ms, total: 16.3 s\n",
      "Wall time: 17.2 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 3.490\n",
      "Test accuracy: 0.266\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.868\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8386 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8346 - acc: 0.0000e+00 - val_loss: 15.8203 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8339 - acc: 0.0000e+00 - val_loss: 15.8165 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8621 - acc: 0.0000e+00 - val_loss: 15.8131 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8258 - acc: 0.0000e+00 - val_loss: 15.8105 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8479 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8314 - acc: 0.0000e+00 - val_loss: 15.8078 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8227 - acc: 0.0000e+00 - val_loss: 15.8068 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8012 - acc: 0.0000e+00 - val_loss: 15.8053 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8330 - acc: 0.0000e+00 - val_loss: 15.8037 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8294 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8073 - acc: 0.0000e+00 - val_loss: 15.7990 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7827 - acc: 0.0000e+00 - val_loss: 15.7954 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8075 - acc: 0.0000e+00 - val_loss: 15.7928 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7799 - acc: 0.0000e+00 - val_loss: 15.7906 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7909 - acc: 0.0000e+00 - val_loss: 15.7889 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7951 - acc: 0.0000e+00 - val_loss: 15.7870 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7707 - acc: 0.0000e+00 - val_loss: 15.7849 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7726 - acc: 0.0000e+00 - val_loss: 15.7837 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7842 - acc: 0.0000e+00 - val_loss: 15.7825 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7497 - acc: 0.0000e+00 - val_loss: 15.7797 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7765 - acc: 0.0000e+00 - val_loss: 15.7756 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7897 - acc: 0.0000e+00 - val_loss: 15.7715 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7573 - acc: 0.0000e+00 - val_loss: 15.7673 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7469 - acc: 0.0000e+00 - val_loss: 15.7622 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7390 - acc: 0.0000e+00 - val_loss: 15.7563 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7680 - acc: 0.0000e+00 - val_loss: 15.7502 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7425 - acc: 0.0000e+00 - val_loss: 15.7450 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7330 - acc: 0.0000e+00 - val_loss: 15.7413 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7472 - acc: 0.0000e+00 - val_loss: 15.7366 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7190 - acc: 0.0000e+00 - val_loss: 15.7305 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6733 - acc: 0.0000e+00 - val_loss: 15.7252 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6980 - acc: 0.0000e+00 - val_loss: 15.7188 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6659 - acc: 0.0000e+00 - val_loss: 15.7137 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7225 - acc: 0.0000e+00 - val_loss: 15.7076 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6941 - acc: 0.0000e+00 - val_loss: 15.7009 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6555 - acc: 0.0000e+00 - val_loss: 15.6941 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6391 - acc: 0.0000e+00 - val_loss: 15.6870 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6961 - acc: 0.0000e+00 - val_loss: 15.6808 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6238 - acc: 0.0000e+00 - val_loss: 15.6741 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6173 - acc: 0.0000e+00 - val_loss: 15.6682 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6186 - acc: 0.0000e+00 - val_loss: 15.6618 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6086 - acc: 0.0000e+00 - val_loss: 15.6556 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5891 - acc: 0.0000e+00 - val_loss: 15.6492 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6088 - acc: 0.0000e+00 - val_loss: 15.6425 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5437 - acc: 0.0000e+00 - val_loss: 15.6357 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6273 - acc: 0.0000e+00 - val_loss: 15.6289 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5543 - acc: 0.0000e+00 - val_loss: 15.6220 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5791 - acc: 0.0000e+00 - val_loss: 15.6149 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5683 - acc: 0.0000e+00 - val_loss: 15.6075 - val_acc: 0.0000e+00\n",
      "CPU times: user 34.1 s, sys: 5.56 s, total: 39.7 s\n",
      "Wall time: 55.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.802\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 8s - loss: 15.8636 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.6480 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 11.9906 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 9.0340 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.5197 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.4339 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.0416 - acc: 0.0050\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.1822 - acc: 0.2950\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 1.7955 - acc: 0.2950\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.6863 - acc: 0.2700\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6113 - acc: 0.2900\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.5741 - acc: 0.2850\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.5473 - acc: 0.2850\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.5737 - acc: 0.2550\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.4707 - acc: 0.3850\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.4886 - acc: 0.3800\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.4227 - acc: 0.4050\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.3869 - acc: 0.4150\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.3512 - acc: 0.4100\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.2696 - acc: 0.5000\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.2790 - acc: 0.4200\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.2769 - acc: 0.4300\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.1952 - acc: 0.4950\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.1918 - acc: 0.4850\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.1508 - acc: 0.5350\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.0885 - acc: 0.5050\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.0906 - acc: 0.5200\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.0019 - acc: 0.5600\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.0481 - acc: 0.4800\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.0009 - acc: 0.5200\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 0.9962 - acc: 0.5000\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 0.9144 - acc: 0.5850\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 0.8910 - acc: 0.6350\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 0.8585 - acc: 0.6250\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 0.8008 - acc: 0.6450\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 0.8002 - acc: 0.6650\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 0.7182 - acc: 0.7350\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 0.7185 - acc: 0.7100\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 0.7015 - acc: 0.7000\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 0.6162 - acc: 0.7600\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 0.5828 - acc: 0.7850\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 0.5814 - acc: 0.7450\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 0.5448 - acc: 0.7750\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.4803 - acc: 0.8100\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.4482 - acc: 0.8100\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.4311 - acc: 0.8250\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.4056 - acc: 0.8450\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.3899 - acc: 0.8700\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.3705 - acc: 0.8950\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.3363 - acc: 0.9100\n",
      "CPU times: user 15.6 s, sys: 1.15 s, total: 16.7 s\n",
      "Wall time: 19.2 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.593\n",
      "Test accuracy: 0.316\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.074\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8545 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8617 - acc: 0.0000e+00 - val_loss: 15.8208 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8479 - acc: 0.0000e+00 - val_loss: 15.8172 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8233 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8364 - acc: 0.0000e+00 - val_loss: 15.8113 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8234 - acc: 0.0000e+00 - val_loss: 15.8093 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8223 - acc: 0.0000e+00 - val_loss: 15.8081 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8109 - acc: 0.0000e+00 - val_loss: 15.8071 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8116 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8208 - acc: 0.0000e+00 - val_loss: 15.8037 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7995 - acc: 0.0000e+00 - val_loss: 15.8019 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8105 - acc: 0.0000e+00 - val_loss: 15.7991 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8054 - acc: 0.0000e+00 - val_loss: 15.7963 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8008 - acc: 0.0000e+00 - val_loss: 15.7927 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7894 - acc: 0.0000e+00 - val_loss: 15.7898 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7953 - acc: 0.0000e+00 - val_loss: 15.7879 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8050 - acc: 0.0000e+00 - val_loss: 15.7861 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7856 - acc: 0.0000e+00 - val_loss: 15.7839 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7485 - acc: 0.0000e+00 - val_loss: 15.7820 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7516 - acc: 0.0000e+00 - val_loss: 15.7801 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7929 - acc: 0.0000e+00 - val_loss: 15.7779 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7425 - acc: 0.0000e+00 - val_loss: 15.7744 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7804 - acc: 0.0000e+00 - val_loss: 15.7699 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7527 - acc: 0.0000e+00 - val_loss: 15.7659 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7445 - acc: 0.0000e+00 - val_loss: 15.7613 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7176 - acc: 0.0000e+00 - val_loss: 15.7555 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7538 - acc: 0.0000e+00 - val_loss: 15.7499 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7312 - acc: 0.0000e+00 - val_loss: 15.7443 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7306 - acc: 0.0000e+00 - val_loss: 15.7385 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6955 - acc: 0.0000e+00 - val_loss: 15.7327 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7149 - acc: 0.0000e+00 - val_loss: 15.7274 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7387 - acc: 0.0000e+00 - val_loss: 15.7226 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6636 - acc: 0.0000e+00 - val_loss: 15.7166 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7190 - acc: 0.0000e+00 - val_loss: 15.7108 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6896 - acc: 0.0000e+00 - val_loss: 15.7052 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6759 - acc: 0.0000e+00 - val_loss: 15.6990 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6990 - acc: 0.0000e+00 - val_loss: 15.6928 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6616 - acc: 0.0000e+00 - val_loss: 15.6866 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6332 - acc: 0.0000e+00 - val_loss: 15.6800 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6467 - acc: 0.0000e+00 - val_loss: 15.6735 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6217 - acc: 0.0000e+00 - val_loss: 15.6677 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6176 - acc: 0.0000e+00 - val_loss: 15.6614 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6600 - acc: 0.0000e+00 - val_loss: 15.6553 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6250 - acc: 0.0000e+00 - val_loss: 15.6489 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5486 - acc: 0.0000e+00 - val_loss: 15.6418 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6105 - acc: 0.0000e+00 - val_loss: 15.6352 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6193 - acc: 0.0000e+00 - val_loss: 15.6286 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5826 - acc: 0.0000e+00 - val_loss: 15.6217 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5265 - acc: 0.0000e+00 - val_loss: 15.6139 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5843 - acc: 0.0000e+00 - val_loss: 15.6059 - val_acc: 0.0000e+00\n",
      "CPU times: user 34.6 s, sys: 5.59 s, total: 40.2 s\n",
      "Wall time: 55.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.804\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 8s - loss: 15.6161 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 12.0008 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 6.6802 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.3074 - acc: 0.0167         \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 1.9939 - acc: 0.2133     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.7102 - acc: 0.2133     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.7057 - acc: 0.2367     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.6659 - acc: 0.2500     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.6267 - acc: 0.2233     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.6576 - acc: 0.1800     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.6299 - acc: 0.2700     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.5947 - acc: 0.2933     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.5939 - acc: 0.2867     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.5354 - acc: 0.3033     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.5273 - acc: 0.2767     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.4500 - acc: 0.3467     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.3838 - acc: 0.4200     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.3655 - acc: 0.4133     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.2896 - acc: 0.4267     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.2322 - acc: 0.4733     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.1559 - acc: 0.5233     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.1098 - acc: 0.5367     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.0158 - acc: 0.6167     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 0.9698 - acc: 0.6300     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 0.9037 - acc: 0.6500     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 0.8812 - acc: 0.6333     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 0.8099 - acc: 0.6733     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.7459 - acc: 0.7133     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.6799 - acc: 0.7467     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.6205 - acc: 0.7733     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.5911 - acc: 0.7867     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.5959 - acc: 0.7633     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.5597 - acc: 0.7667     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.4734 - acc: 0.8167     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.4194 - acc: 0.8600     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.4216 - acc: 0.8667     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.3504 - acc: 0.8967     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.3307 - acc: 0.8900     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.3476 - acc: 0.8733     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.3952 - acc: 0.8600     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.2776 - acc: 0.9067     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.2791 - acc: 0.9100     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.1868 - acc: 0.9367     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.1901 - acc: 0.9433     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.2878 - acc: 0.9067     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.2182 - acc: 0.9367     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.1856 - acc: 0.9333     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.1536 - acc: 0.9400     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.1885 - acc: 0.9233     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.1464 - acc: 0.9600     \n",
      "CPU times: user 19.1 s, sys: 1.71 s, total: 20.8 s\n",
      "Wall time: 24.2 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.637\n",
      "Test accuracy: 0.474\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.410\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8413 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8878 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8302 - acc: 0.0000e+00 - val_loss: 15.8175 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8433 - acc: 0.0000e+00 - val_loss: 15.8143 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8439 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8429 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8124 - acc: 0.0000e+00 - val_loss: 15.8095 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8570 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8130 - acc: 0.0000e+00 - val_loss: 15.8073 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8443 - acc: 0.0000e+00 - val_loss: 15.8058 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8221 - acc: 0.0000e+00 - val_loss: 15.8040 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8259 - acc: 0.0000e+00 - val_loss: 15.8013 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8036 - acc: 0.0000e+00 - val_loss: 15.7975 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8274 - acc: 0.0000e+00 - val_loss: 15.7948 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8008 - acc: 0.0000e+00 - val_loss: 15.7923 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7909 - acc: 0.0000e+00 - val_loss: 15.7902 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7992 - acc: 0.0000e+00 - val_loss: 15.7890 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7998 - acc: 0.0000e+00 - val_loss: 15.7874 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7551 - acc: 0.0000e+00 - val_loss: 15.7859 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7660 - acc: 0.0000e+00 - val_loss: 15.7837 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7754 - acc: 0.0000e+00 - val_loss: 15.7806 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7684 - acc: 0.0000e+00 - val_loss: 15.7770 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7639 - acc: 0.0000e+00 - val_loss: 15.7729 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7710 - acc: 0.0000e+00 - val_loss: 15.7687 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7726 - acc: 0.0000e+00 - val_loss: 15.7642 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7664 - acc: 0.0000e+00 - val_loss: 15.7584 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7235 - acc: 0.0000e+00 - val_loss: 15.7525 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7202 - acc: 0.0000e+00 - val_loss: 15.7472 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6786 - acc: 0.0000e+00 - val_loss: 15.7427 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7014 - acc: 0.0000e+00 - val_loss: 15.7367 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6883 - acc: 0.0000e+00 - val_loss: 15.7314 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7333 - acc: 0.0000e+00 - val_loss: 15.7264 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7238 - acc: 0.0000e+00 - val_loss: 15.7213 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6763 - acc: 0.0000e+00 - val_loss: 15.7155 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6636 - acc: 0.0000e+00 - val_loss: 15.7087 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6960 - acc: 0.0000e+00 - val_loss: 15.7022 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6584 - acc: 0.0000e+00 - val_loss: 15.6960 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6515 - acc: 0.0000e+00 - val_loss: 15.6905 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6664 - acc: 0.0000e+00 - val_loss: 15.6844 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6523 - acc: 0.0000e+00 - val_loss: 15.6782 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6615 - acc: 0.0000e+00 - val_loss: 15.6726 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6278 - acc: 0.0000e+00 - val_loss: 15.6665 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6300 - acc: 0.0000e+00 - val_loss: 15.6603 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6249 - acc: 0.0000e+00 - val_loss: 15.6540 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6147 - acc: 0.0000e+00 - val_loss: 15.6477 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5964 - acc: 0.0000e+00 - val_loss: 15.6404 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5933 - acc: 0.0000e+00 - val_loss: 15.6341 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5911 - acc: 0.0000e+00 - val_loss: 15.6277 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5769 - acc: 0.0000e+00 - val_loss: 15.6204 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5520 - acc: 0.0000e+00 - val_loss: 15.6132 - val_acc: 0.0000e+00\n",
      "CPU times: user 34.8 s, sys: 5.73 s, total: 40.6 s\n",
      "Wall time: 56.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.800\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 9s - loss: 15.4045 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 11.2120 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 6.0861 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 2.9857 - acc: 0.0775     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.9468 - acc: 0.2150     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.6779 - acc: 0.2550     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.7075 - acc: 0.2425     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.6232 - acc: 0.2450     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.5700 - acc: 0.2675     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.6104 - acc: 0.2900     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.5534 - acc: 0.2975     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.5158 - acc: 0.3050     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.5061 - acc: 0.2975     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.4472 - acc: 0.3400     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.3556 - acc: 0.3925     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.3859 - acc: 0.3900     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.3407 - acc: 0.4250     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.2426 - acc: 0.4525     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.1928 - acc: 0.4775     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 1.1452 - acc: 0.4750     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 1.0616 - acc: 0.5150     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 1.0159 - acc: 0.5800     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.9503 - acc: 0.6075     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.9018 - acc: 0.6325     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.7784 - acc: 0.7275     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.7349 - acc: 0.7075     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.6654 - acc: 0.7525     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.5804 - acc: 0.8000     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.5840 - acc: 0.7925     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.4632 - acc: 0.8475     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.4421 - acc: 0.8500     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.3534 - acc: 0.8900     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.3400 - acc: 0.9075     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.2816 - acc: 0.9050     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.2587 - acc: 0.9250     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.2466 - acc: 0.9200     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.1332 - acc: 0.9650     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.2257 - acc: 0.9375     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.0973 - acc: 0.9725     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.0951 - acc: 0.9775     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.0873 - acc: 0.9750     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.0648 - acc: 0.9800     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.0510 - acc: 0.9800     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.0535 - acc: 0.9825     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.0607 - acc: 0.9750     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.0237 - acc: 0.9975     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.0272 - acc: 0.9950     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.0305 - acc: 0.9925     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.0218 - acc: 0.9950     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.0260 - acc: 0.9900     \n",
      "CPU times: user 21.2 s, sys: 2.22 s, total: 23.4 s\n",
      "Wall time: 28.4 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 5.311\n",
      "Test accuracy: 0.398\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.917\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s - loss: 15.8322 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8418 - acc: 0.0000e+00 - val_loss: 15.8212 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8549 - acc: 0.0000e+00 - val_loss: 15.8179 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8838 - acc: 0.0000e+00 - val_loss: 15.8145 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8474 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8289 - acc: 0.0000e+00 - val_loss: 15.8106 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8162 - acc: 0.0000e+00 - val_loss: 15.8099 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8235 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8206 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8090 - acc: 0.0000e+00 - val_loss: 15.8053 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8104 - acc: 0.0000e+00 - val_loss: 15.8038 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8090 - acc: 0.0000e+00 - val_loss: 15.8005 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8162 - acc: 0.0000e+00 - val_loss: 15.7970 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8325 - acc: 0.0000e+00 - val_loss: 15.7945 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7737 - acc: 0.0000e+00 - val_loss: 15.7919 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8222 - acc: 0.0000e+00 - val_loss: 15.7905 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7939 - acc: 0.0000e+00 - val_loss: 15.7894 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7896 - acc: 0.0000e+00 - val_loss: 15.7878 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7740 - acc: 0.0000e+00 - val_loss: 15.7857 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7775 - acc: 0.0000e+00 - val_loss: 15.7836 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7754 - acc: 0.0000e+00 - val_loss: 15.7802 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7412 - acc: 0.0000e+00 - val_loss: 15.7757 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7609 - acc: 0.0000e+00 - val_loss: 15.7707 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7303 - acc: 0.0000e+00 - val_loss: 15.7662 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7720 - acc: 0.0000e+00 - val_loss: 15.7616 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7499 - acc: 0.0000e+00 - val_loss: 15.7561 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7519 - acc: 0.0000e+00 - val_loss: 15.7512 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7121 - acc: 0.0000e+00 - val_loss: 15.7461 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7416 - acc: 0.0000e+00 - val_loss: 15.7411 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7434 - acc: 0.0000e+00 - val_loss: 15.7366 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7455 - acc: 0.0000e+00 - val_loss: 15.7310 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7058 - acc: 0.0000e+00 - val_loss: 15.7251 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6775 - acc: 0.0000e+00 - val_loss: 15.7196 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6771 - acc: 0.0000e+00 - val_loss: 15.7141 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6734 - acc: 0.0000e+00 - val_loss: 15.7078 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6933 - acc: 0.0000e+00 - val_loss: 15.7019 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6846 - acc: 0.0000e+00 - val_loss: 15.6953 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6475 - acc: 0.0000e+00 - val_loss: 15.6890 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6732 - acc: 0.0000e+00 - val_loss: 15.6826 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6463 - acc: 0.0000e+00 - val_loss: 15.6761 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6471 - acc: 0.0000e+00 - val_loss: 15.6708 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6425 - acc: 0.0000e+00 - val_loss: 15.6649 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6368 - acc: 0.0000e+00 - val_loss: 15.6588 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6328 - acc: 0.0000e+00 - val_loss: 15.6531 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5979 - acc: 0.0000e+00 - val_loss: 15.6463 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6064 - acc: 0.0000e+00 - val_loss: 15.6395 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6049 - acc: 0.0000e+00 - val_loss: 15.6327 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5855 - acc: 0.0000e+00 - val_loss: 15.6257 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5625 - acc: 0.0000e+00 - val_loss: 15.6189 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5128 - acc: 0.0000e+00 - val_loss: 15.6110 - val_acc: 0.0000e+00\n",
      "CPU times: user 35.6 s, sys: 5.5 s, total: 41.1 s\n",
      "Wall time: 57 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.232\n",
      "Test accuracy: 0.802\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 9s - loss: 15.2736 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.8298 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.6182 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.6965 - acc: 0.1160     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.8283 - acc: 0.2360     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6696 - acc: 0.2660     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.7055 - acc: 0.2040     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6578 - acc: 0.2240     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.6280 - acc: 0.2840     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.5987 - acc: 0.2500     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.5678 - acc: 0.2920     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.5461 - acc: 0.2900     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.5241 - acc: 0.3180     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.4563 - acc: 0.3840     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.4011 - acc: 0.3760     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.3801 - acc: 0.3920     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.3001 - acc: 0.4660     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.2541 - acc: 0.4740     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.1966 - acc: 0.4920     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.1444 - acc: 0.5040     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 1.1190 - acc: 0.4880     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 1.0544 - acc: 0.5580     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.9917 - acc: 0.5720     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.9745 - acc: 0.5820     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.8894 - acc: 0.6080     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.8458 - acc: 0.6300     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.8010 - acc: 0.6380     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.7451 - acc: 0.6900     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.7148 - acc: 0.7100     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.6130 - acc: 0.7720     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.5603 - acc: 0.8240     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.4521 - acc: 0.8640     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.4247 - acc: 0.8620     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.3570 - acc: 0.9020     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.2934 - acc: 0.9140     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.2427 - acc: 0.9300     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.2073 - acc: 0.9400     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.1538 - acc: 0.9600     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.1259 - acc: 0.9720     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.1147 - acc: 0.9760     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0955 - acc: 0.9840     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0903 - acc: 0.9740     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.0758 - acc: 0.9800     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.0581 - acc: 0.9880     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0368 - acc: 0.9900     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0614 - acc: 0.9780     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0478 - acc: 0.9860     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0574 - acc: 0.9820     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0411 - acc: 0.9940     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0526 - acc: 0.9820     \n",
      "CPU times: user 23.1 s, sys: 2.83 s, total: 26 s\n",
      "Wall time: 32.5 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.361\n",
      "Test accuracy: 0.512\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.909\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_reptiles, Y_test_reptiles, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium -> reptiles with dream 50 -> 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 15s - loss: 15.8691 - acc: 0.0000e+00 - val_loss: 15.8250 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8757 - acc: 0.0000e+00 - val_loss: 15.8220 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8381 - acc: 0.0000e+00 - val_loss: 15.8182 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8470 - acc: 0.0000e+00 - val_loss: 15.8145 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8216 - acc: 0.0000e+00 - val_loss: 15.8121 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8346 - acc: 0.0000e+00 - val_loss: 15.8097 - val_acc: 0.0000e+00\n",
      "CPU times: user 19 s, sys: 867 ms, total: 19.8 s\n",
      "Wall time: 21.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.304\n",
      "Test accuracy: 0.786\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 13s - loss: 13.7646 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 11.7367 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 10.4806 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.9155 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.4827 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 3.2794 - acc: 0.0000e+00\n",
      "CPU times: user 15.5 s, sys: 195 ms, total: 15.7 s\n",
      "Wall time: 15.6 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 11.283\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.663\n",
      "Test accuracy: 0.644\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 15s - loss: 15.8590 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8565 - acc: 0.0000e+00 - val_loss: 15.8206 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8360 - acc: 0.0000e+00 - val_loss: 15.8169 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8535 - acc: 0.0000e+00 - val_loss: 15.8134 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8349 - acc: 0.0000e+00 - val_loss: 15.8111 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8300 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "CPU times: user 19.4 s, sys: 849 ms, total: 20.3 s\n",
      "Wall time: 22 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.305\n",
      "Test accuracy: 0.788\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 13s - loss: 15.8700 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 15.7094 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 15.0033 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 12.9678 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 10.5200 - acc: 0.0000e+00\n",
      "CPU times: user 15.8 s, sys: 221 ms, total: 16.1 s\n",
      "Wall time: 15.9 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 10.087\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.142\n",
      "Test accuracy: 0.694\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 15s - loss: 15.8692 - acc: 0.0000e+00 - val_loss: 15.8252 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8745 - acc: 0.0000e+00 - val_loss: 15.8219 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8556 - acc: 0.0000e+00 - val_loss: 15.8185 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8464 - acc: 0.0000e+00 - val_loss: 15.8152 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8555 - acc: 0.0000e+00 - val_loss: 15.8129 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8316 - acc: 0.0000e+00 - val_loss: 15.8109 - val_acc: 0.0000e+00\n",
      "CPU times: user 19.7 s, sys: 892 ms, total: 20.6 s\n",
      "Wall time: 22.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.307\n",
      "Test accuracy: 0.786\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 14s - loss: 15.6909 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.5432 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 14.5102 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 12.6406 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 10.5031 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 7.9557 - acc: 0.0000e+00\n",
      "CPU times: user 16.5 s, sys: 208 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 7.755\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.118\n",
      "Test accuracy: 0.234\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 16s - loss: 15.8538 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8537 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8341 - acc: 0.0000e+00 - val_loss: 15.8178 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8285 - acc: 0.0000e+00 - val_loss: 15.8144 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8599 - acc: 0.0000e+00 - val_loss: 15.8120 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8567 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "CPU times: user 20.4 s, sys: 869 ms, total: 21.3 s\n",
      "Wall time: 23.1 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.313\n",
      "Test accuracy: 0.784\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 14s - loss: 15.8328 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.3791 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 14.3886 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 12.6721 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 10.2414 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 7.8673 - acc: 0.0000e+00\n",
      "CPU times: user 16.7 s, sys: 181 ms, total: 16.9 s\n",
      "Wall time: 16.9 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 6.749\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.328\n",
      "Test accuracy: 0.384\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 17s - loss: 15.8519 - acc: 0.0000e+00 - val_loss: 15.8247 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8511 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8278 - acc: 0.0000e+00 - val_loss: 15.8176 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8374 - acc: 0.0000e+00 - val_loss: 15.8138 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8160 - acc: 0.0000e+00 - val_loss: 15.8109 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8609 - acc: 0.0000e+00 - val_loss: 15.8094 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8132 - acc: 0.0000e+00 - val_loss: 15.8083 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8517 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8285 - acc: 0.0000e+00 - val_loss: 15.8060 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8114 - acc: 0.0000e+00 - val_loss: 15.8048 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.8150 - acc: 0.0000e+00 - val_loss: 15.8026 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8103 - acc: 0.0000e+00 - val_loss: 15.8003 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7977 - acc: 0.0000e+00 - val_loss: 15.7972 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8175 - acc: 0.0000e+00 - val_loss: 15.7940 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8141 - acc: 0.0000e+00 - val_loss: 15.7911 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.7960 - acc: 0.0000e+00 - val_loss: 15.7891 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7926 - acc: 0.0000e+00 - val_loss: 15.7870 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7882 - acc: 0.0000e+00 - val_loss: 15.7853 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7426 - acc: 0.0000e+00 - val_loss: 15.7830 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7867 - acc: 0.0000e+00 - val_loss: 15.7813 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7222 - acc: 0.0000e+00 - val_loss: 15.7772 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7567 - acc: 0.0000e+00 - val_loss: 15.7734 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7483 - acc: 0.0000e+00 - val_loss: 15.7690 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7616 - acc: 0.0000e+00 - val_loss: 15.7645 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7708 - acc: 0.0000e+00 - val_loss: 15.7588 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7296 - acc: 0.0000e+00 - val_loss: 15.7533 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7297 - acc: 0.0000e+00 - val_loss: 15.7476 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7216 - acc: 0.0000e+00 - val_loss: 15.7427 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.6986 - acc: 0.0000e+00 - val_loss: 15.7376 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7181 - acc: 0.0000e+00 - val_loss: 15.7324 - val_acc: 0.0000e+00\n",
      "CPU times: user 32.2 s, sys: 3.4 s, total: 35.6 s\n",
      "Wall time: 45.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.219\n",
      "Test accuracy: 0.798\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 14s - loss: 15.7733 - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s - loss: 15.4783 - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s - loss: 14.1316 - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s - loss: 11.7778 - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s - loss: 9.2210 - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s - loss: 6.7848 - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s - loss: 4.7162 - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s - loss: 3.4329 - acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s - loss: 2.3884 - acc: 0.2000\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s - loss: 1.9569 - acc: 0.2500\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s - loss: 1.7050 - acc: 0.3500\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s - loss: 1.6060 - acc: 0.3500\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s - loss: 1.6480 - acc: 0.3000\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s - loss: 1.4896 - acc: 0.4000\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s - loss: 1.6479 - acc: 0.3000\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s - loss: 1.3272 - acc: 0.4500\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s - loss: 1.3225 - acc: 0.4500\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s - loss: 1.4141 - acc: 0.4500\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s - loss: 1.3624 - acc: 0.4000\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s - loss: 1.1189 - acc: 0.7000\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s - loss: 1.5057 - acc: 0.3000\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s - loss: 1.2826 - acc: 0.4500\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s - loss: 1.0503 - acc: 0.7000\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s - loss: 1.1724 - acc: 0.4500\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s - loss: 1.0204 - acc: 0.6000\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s - loss: 0.9869 - acc: 0.5500\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s - loss: 1.0035 - acc: 0.6000\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s - loss: 0.9566 - acc: 0.6000\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s - loss: 0.8427 - acc: 0.7000\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s - loss: 0.8726 - acc: 0.6500\n",
      "CPU times: user 18 s, sys: 274 ms, total: 18.3 s\n",
      "Wall time: 18.2 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 2.027\n",
      "Test accuracy: 0.216\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 10.947\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 17s - loss: 15.8399 - acc: 0.0000e+00 - val_loss: 15.8238 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8302 - acc: 0.0000e+00 - val_loss: 15.8199 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8429 - acc: 0.0000e+00 - val_loss: 15.8165 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8651 - acc: 0.0000e+00 - val_loss: 15.8130 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8525 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8617 - acc: 0.0000e+00 - val_loss: 15.8091 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8450 - acc: 0.0000e+00 - val_loss: 15.8084 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.8585 - acc: 0.0000e+00 - val_loss: 15.8075 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8284 - acc: 0.0000e+00 - val_loss: 15.8059 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8289 - acc: 0.0000e+00 - val_loss: 15.8045 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8230 - acc: 0.0000e+00 - val_loss: 15.8027 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8291 - acc: 0.0000e+00 - val_loss: 15.7999 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7911 - acc: 0.0000e+00 - val_loss: 15.7961 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.7933 - acc: 0.0000e+00 - val_loss: 15.7928 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8112 - acc: 0.0000e+00 - val_loss: 15.7905 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8027 - acc: 0.0000e+00 - val_loss: 15.7892 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8113 - acc: 0.0000e+00 - val_loss: 15.7877 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7882 - acc: 0.0000e+00 - val_loss: 15.7862 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7694 - acc: 0.0000e+00 - val_loss: 15.7843 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7573 - acc: 0.0000e+00 - val_loss: 15.7820 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7690 - acc: 0.0000e+00 - val_loss: 15.7787 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7544 - acc: 0.0000e+00 - val_loss: 15.7739 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7454 - acc: 0.0000e+00 - val_loss: 15.7695 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7496 - acc: 0.0000e+00 - val_loss: 15.7652 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7363 - acc: 0.0000e+00 - val_loss: 15.7597 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7403 - acc: 0.0000e+00 - val_loss: 15.7544 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7205 - acc: 0.0000e+00 - val_loss: 15.7486 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7352 - acc: 0.0000e+00 - val_loss: 15.7430 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7314 - acc: 0.0000e+00 - val_loss: 15.7380 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7335 - acc: 0.0000e+00 - val_loss: 15.7327 - val_acc: 0.0000e+00\n",
      "CPU times: user 32.4 s, sys: 3.43 s, total: 35.8 s\n",
      "Wall time: 45.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.221\n",
      "Test accuracy: 0.798\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 14s - loss: 15.8931 - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 0s - loss: 15.1046 - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 0s - loss: 12.6854 - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 0s - loss: 9.6565 - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 0s - loss: 7.2789 - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 0s - loss: 5.0312 - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 0s - loss: 3.4471 - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 0s - loss: 2.3796 - acc: 0.1700\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 0s - loss: 1.9679 - acc: 0.3100\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 0s - loss: 1.8237 - acc: 0.2600\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 0s - loss: 1.6721 - acc: 0.2300\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 0s - loss: 1.5684 - acc: 0.2800\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 0s - loss: 1.6035 - acc: 0.2400\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 0s - loss: 1.5891 - acc: 0.3100\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 0s - loss: 1.5126 - acc: 0.3600\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 0s - loss: 1.5503 - acc: 0.3100\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 0s - loss: 1.5071 - acc: 0.3400\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 0s - loss: 1.4831 - acc: 0.3700\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 0s - loss: 1.4354 - acc: 0.3300\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 0s - loss: 1.3719 - acc: 0.4200\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 0s - loss: 1.4148 - acc: 0.3200\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 0s - loss: 1.3147 - acc: 0.4100\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 0s - loss: 1.2370 - acc: 0.4900\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 0s - loss: 1.1640 - acc: 0.5100\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 0s - loss: 1.1985 - acc: 0.4800\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 0s - loss: 1.1798 - acc: 0.5100\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 0s - loss: 1.1715 - acc: 0.5000\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 0s - loss: 1.0982 - acc: 0.5900\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 0s - loss: 1.0712 - acc: 0.4900\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 0s - loss: 1.0321 - acc: 0.5600\n",
      "CPU times: user 19.1 s, sys: 545 ms, total: 19.6 s\n",
      "Wall time: 20.1 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 2.389\n",
      "Test accuracy: 0.238\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.475\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 17s - loss: 15.8768 - acc: 0.0000e+00 - val_loss: 15.8252 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8610 - acc: 0.0000e+00 - val_loss: 15.8220 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8591 - acc: 0.0000e+00 - val_loss: 15.8188 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8638 - acc: 0.0000e+00 - val_loss: 15.8156 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8326 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8532 - acc: 0.0000e+00 - val_loss: 15.8106 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8727 - acc: 0.0000e+00 - val_loss: 15.8098 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8423 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8125 - acc: 0.0000e+00 - val_loss: 15.8069 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8200 - acc: 0.0000e+00 - val_loss: 15.8051 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8243 - acc: 0.0000e+00 - val_loss: 15.8035 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8025 - acc: 0.0000e+00 - val_loss: 15.8004 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7843 - acc: 0.0000e+00 - val_loss: 15.7959 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7829 - acc: 0.0000e+00 - val_loss: 15.7927 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7924 - acc: 0.0000e+00 - val_loss: 15.7903 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7827 - acc: 0.0000e+00 - val_loss: 15.7883 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8197 - acc: 0.0000e+00 - val_loss: 15.7861 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8079 - acc: 0.0000e+00 - val_loss: 15.7839 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7719 - acc: 0.0000e+00 - val_loss: 15.7821 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7519 - acc: 0.0000e+00 - val_loss: 15.7804 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7620 - acc: 0.0000e+00 - val_loss: 15.7784 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.7482 - acc: 0.0000e+00 - val_loss: 15.7750 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7158 - acc: 0.0000e+00 - val_loss: 15.7716 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7358 - acc: 0.0000e+00 - val_loss: 15.7660 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7659 - acc: 0.0000e+00 - val_loss: 15.7617 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.7273 - acc: 0.0000e+00 - val_loss: 15.7561 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7128 - acc: 0.0000e+00 - val_loss: 15.7503 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.7178 - acc: 0.0000e+00 - val_loss: 15.7447 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7137 - acc: 0.0000e+00 - val_loss: 15.7388 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7161 - acc: 0.0000e+00 - val_loss: 15.7333 - val_acc: 0.0000e+00\n",
      "CPU times: user 33 s, sys: 3.41 s, total: 36.4 s\n",
      "Wall time: 46.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.224\n",
      "Test accuracy: 0.798\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 15s - loss: 15.8481 - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 0s - loss: 14.9147 - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 0s - loss: 12.6113 - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 0s - loss: 9.5884 - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 0s - loss: 6.9666 - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 0s - loss: 4.8278 - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 0s - loss: 3.3077 - acc: 0.0050\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 0s - loss: 2.3326 - acc: 0.2750\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 0s - loss: 1.9134 - acc: 0.3050\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 0s - loss: 1.6885 - acc: 0.3300\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 0s - loss: 1.6221 - acc: 0.2800\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 0s - loss: 1.6026 - acc: 0.2700\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 0s - loss: 1.6126 - acc: 0.2700\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 0s - loss: 1.5188 - acc: 0.3050\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 0s - loss: 1.5476 - acc: 0.3000\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 0s - loss: 1.4629 - acc: 0.4000\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 0s - loss: 1.4377 - acc: 0.3950\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 0s - loss: 1.4509 - acc: 0.4150\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 0s - loss: 1.3713 - acc: 0.4000\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 0s - loss: 1.3483 - acc: 0.4050\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 0s - loss: 1.2688 - acc: 0.4500\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 0s - loss: 1.2808 - acc: 0.4300\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 0s - loss: 1.2150 - acc: 0.4800\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 0s - loss: 1.1929 - acc: 0.4900\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 0s - loss: 1.1482 - acc: 0.4950\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 0s - loss: 1.0700 - acc: 0.5650\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 0s - loss: 1.1093 - acc: 0.5250\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 0s - loss: 1.0269 - acc: 0.5750\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 0s - loss: 1.0085 - acc: 0.5650\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 0s - loss: 0.9764 - acc: 0.5850\n",
      "CPU times: user 20.3 s, sys: 874 ms, total: 21.2 s\n",
      "Wall time: 22.8 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 2.350\n",
      "Test accuracy: 0.270\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.233\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 18s - loss: 15.8715 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.8445 - acc: 0.0000e+00 - val_loss: 15.8212 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8459 - acc: 0.0000e+00 - val_loss: 15.8178 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8308 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8672 - acc: 0.0000e+00 - val_loss: 15.8121 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8332 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8605 - acc: 0.0000e+00 - val_loss: 15.8094 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8172 - acc: 0.0000e+00 - val_loss: 15.8087 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8277 - acc: 0.0000e+00 - val_loss: 15.8069 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8314 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8232 - acc: 0.0000e+00 - val_loss: 15.8031 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8410 - acc: 0.0000e+00 - val_loss: 15.8008 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7947 - acc: 0.0000e+00 - val_loss: 15.7972 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8126 - acc: 0.0000e+00 - val_loss: 15.7935 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7903 - acc: 0.0000e+00 - val_loss: 15.7913 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7604 - acc: 0.0000e+00 - val_loss: 15.7895 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7725 - acc: 0.0000e+00 - val_loss: 15.7878 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7746 - acc: 0.0000e+00 - val_loss: 15.7856 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7592 - acc: 0.0000e+00 - val_loss: 15.7836 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7679 - acc: 0.0000e+00 - val_loss: 15.7814 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8081 - acc: 0.0000e+00 - val_loss: 15.7792 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7868 - acc: 0.0000e+00 - val_loss: 15.7756 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7339 - acc: 0.0000e+00 - val_loss: 15.7706 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7493 - acc: 0.0000e+00 - val_loss: 15.7662 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7790 - acc: 0.0000e+00 - val_loss: 15.7610 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7331 - acc: 0.0000e+00 - val_loss: 15.7555 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7686 - acc: 0.0000e+00 - val_loss: 15.7504 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7398 - acc: 0.0000e+00 - val_loss: 15.7460 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7299 - acc: 0.0000e+00 - val_loss: 15.7411 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.6909 - acc: 0.0000e+00 - val_loss: 15.7350 - val_acc: 0.0000e+00\n",
      "CPU times: user 33.2 s, sys: 3.51 s, total: 36.7 s\n",
      "Wall time: 46.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.224\n",
      "Test accuracy: 0.796\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 15s - loss: 15.6803 - acc: 0.0000e+00    \n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 0s - loss: 12.3098 - acc: 0.0000e+00     \n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 0s - loss: 6.8736 - acc: 0.0000e+00     \n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 0s - loss: 3.2633 - acc: 0.0167         \n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 0s - loss: 1.9981 - acc: 0.2267     \n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 0s - loss: 1.6738 - acc: 0.2467     \n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 0s - loss: 1.5759 - acc: 0.2733     \n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 0s - loss: 1.6564 - acc: 0.2400     \n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 0s - loss: 1.5622 - acc: 0.3000     \n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 0s - loss: 1.5856 - acc: 0.2733     \n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 0s - loss: 1.6075 - acc: 0.2533     \n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 0s - loss: 1.5029 - acc: 0.3267     \n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 0s - loss: 1.4771 - acc: 0.3433     \n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 0s - loss: 1.5103 - acc: 0.3400     \n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 0s - loss: 1.4722 - acc: 0.3300     \n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 0s - loss: 1.3651 - acc: 0.3700     \n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 0s - loss: 1.3501 - acc: 0.4100     \n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 0s - loss: 1.3378 - acc: 0.4000     \n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 0s - loss: 1.2868 - acc: 0.4200     \n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 0s - loss: 1.1987 - acc: 0.4800     \n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 0s - loss: 1.1944 - acc: 0.5000     \n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 0s - loss: 1.1023 - acc: 0.5333     \n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 0s - loss: 1.0612 - acc: 0.5533     \n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 0s - loss: 1.0447 - acc: 0.5767     \n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 0s - loss: 0.9990 - acc: 0.5767     \n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 0s - loss: 1.0016 - acc: 0.5533     \n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 0s - loss: 0.9023 - acc: 0.6067     \n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 0s - loss: 0.8350 - acc: 0.6700     \n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 0s - loss: 0.8077 - acc: 0.6700     \n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 0s - loss: 0.7945 - acc: 0.6533     \n",
      "CPU times: user 22.7 s, sys: 1.22 s, total: 23.9 s\n",
      "Wall time: 26.1 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 13s    \n",
      "\n",
      "Test loss: 3.008\n",
      "Test accuracy: 0.338\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.797\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 18s - loss: 15.8678 - acc: 0.0000e+00 - val_loss: 15.8253 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8436 - acc: 0.0000e+00 - val_loss: 15.8221 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.8302 - acc: 0.0000e+00 - val_loss: 15.8188 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8530 - acc: 0.0000e+00 - val_loss: 15.8154 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7995 - acc: 0.0000e+00 - val_loss: 15.8130 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 1s - loss: 15.8193 - acc: 0.0000e+00 - val_loss: 15.8107 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8224 - acc: 0.0000e+00 - val_loss: 15.8087 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8337 - acc: 0.0000e+00 - val_loss: 15.8082 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8350 - acc: 0.0000e+00 - val_loss: 15.8065 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8193 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8351 - acc: 0.0000e+00 - val_loss: 15.8041 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8202 - acc: 0.0000e+00 - val_loss: 15.8007 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8302 - acc: 0.0000e+00 - val_loss: 15.7976 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7890 - acc: 0.0000e+00 - val_loss: 15.7939 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7956 - acc: 0.0000e+00 - val_loss: 15.7912 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8034 - acc: 0.0000e+00 - val_loss: 15.7895 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7679 - acc: 0.0000e+00 - val_loss: 15.7875 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7857 - acc: 0.0000e+00 - val_loss: 15.7854 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.8043 - acc: 0.0000e+00 - val_loss: 15.7841 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7542 - acc: 0.0000e+00 - val_loss: 15.7823 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7579 - acc: 0.0000e+00 - val_loss: 15.7789 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7352 - acc: 0.0000e+00 - val_loss: 15.7753 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7457 - acc: 0.0000e+00 - val_loss: 15.7708 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7306 - acc: 0.0000e+00 - val_loss: 15.7663 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7457 - acc: 0.0000e+00 - val_loss: 15.7605 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7649 - acc: 0.0000e+00 - val_loss: 15.7546 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7457 - acc: 0.0000e+00 - val_loss: 15.7492 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7115 - acc: 0.0000e+00 - val_loss: 15.7440 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7021 - acc: 0.0000e+00 - val_loss: 15.7377 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 15.7215 - acc: 0.0000e+00 - val_loss: 15.7322 - val_acc: 0.0000e+00\n",
      "CPU times: user 33.8 s, sys: 3.43 s, total: 37.2 s\n",
      "Wall time: 47.1 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.216\n",
      "Test accuracy: 0.798\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "400/400 [==============================] - 16s - loss: 15.4970 - acc: 0.0000e+00    \n",
      "Epoch 2/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,1024]\n\t [[Node: mul_18795 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_10587, gradients_91/MatMul_214_grad/MatMul_1)]]\n\nCaused by op 'mul_18795', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-8ef9290caf3a>\", line 58, in <module>\n    get_ipython().magic('time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2158, in magic\n    return self.run_line_magic(magic_name, magic_arg_s)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2079, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1180, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 1, in <module>\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 1115, in fit\n    self._make_train_function()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 713, in _make_train_function\n    self.total_loss)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 391, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,1024]\n\t [[Node: mul_18795 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_10587, gradients_91/MatMul_214_grad/MatMul_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,1024]\n\t [[Node: mul_18795 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_10587, gradients_91/MatMul_214_grad/MatMul_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8ef9290caf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel_whole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,1024]\n\t [[Node: mul_18795 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_10587, gradients_91/MatMul_214_grad/MatMul_1)]]\n\nCaused by op 'mul_18795', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-8ef9290caf3a>\", line 58, in <module>\n    get_ipython().magic('time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2158, in magic\n    return self.run_line_magic(magic_name, magic_arg_s)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2079, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1180, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 1, in <module>\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 1115, in fit\n    self._make_train_function()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 713, in _make_train_function\n    self.total_loss)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 391, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 794, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1015, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1625, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,1024]\n\t [[Node: mul_18795 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_10587, gradients_91/MatMul_214_grad/MatMul_1)]]\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 30\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    print(\"at night, it dreams\")\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_reptiles, Y_test_reptiles, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium sized -> reptiles **without dream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 9s - loss: 13.1299 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 11.7355 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 9.9921 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.9823 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 6.3763 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 3.4783 - acc: 0.0000e+00\n",
      "CPU times: user 12 s, sys: 72.5 ms, total: 12.1 s\n",
      "Wall time: 11.9 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 11.044\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.256\n",
      "Test accuracy: 0.726\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 9s - loss: 15.4415 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 15.9445 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 14.5188 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 13.4483 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 12.2001 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 9.1323 - acc: 0.0000e+00\n",
      "CPU times: user 12.1 s, sys: 60.7 ms, total: 12.2 s\n",
      "Wall time: 12.1 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 8.920\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.319\n",
      "Test accuracy: 0.622\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 9s - loss: 15.5768 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.8043 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 14.9358 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 13.3438 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 11.3424 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 8.4923 - acc: 0.0000e+00\n",
      "CPU times: user 12.4 s, sys: 77.9 ms, total: 12.5 s\n",
      "Wall time: 12.3 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 6.861\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.746\n",
      "Test accuracy: 0.502\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 9s - loss: 16.0380 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.7219 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 15.1038 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 13.0076 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 11.0961 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 8.6776 - acc: 0.0000e+00\n",
      "CPU times: user 15 s, sys: 94.6 ms, total: 15.1 s\n",
      "Wall time: 15 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 7.437\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.877\n",
      "Test accuracy: 0.522\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 10s - loss: 16.0513 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.6732 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 14.7707 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 12.6417 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 10.0620 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 7.3136 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 5.5641 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 3.6893 - acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 2.5996 - acc: 0.1500\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.0178 - acc: 0.1500\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.7278 - acc: 0.3500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.5797 - acc: 0.2500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.5412 - acc: 0.4500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.5292 - acc: 0.3500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.5289 - acc: 0.4500\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.4647 - acc: 0.4500\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.2658 - acc: 0.6000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.4601 - acc: 0.5000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.2686 - acc: 0.5500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.2964 - acc: 0.5000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.1835 - acc: 0.5000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.2322 - acc: 0.6000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.1818 - acc: 0.5000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 1.1152 - acc: 0.7000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.0141 - acc: 0.6500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 1.0279 - acc: 0.5500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 1.0108 - acc: 0.5500\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 1.0071 - acc: 0.5500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.8590 - acc: 0.6500\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.7038 - acc: 0.6500\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.7673 - acc: 0.7500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.6903 - acc: 0.7000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.7839 - acc: 0.7500\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.6516 - acc: 0.8500\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.5280 - acc: 0.8000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.5654 - acc: 0.8000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.4999 - acc: 0.9000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.3886 - acc: 0.9000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.4654 - acc: 0.8000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.3691 - acc: 0.9000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.3687 - acc: 0.8500\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.3332 - acc: 0.8500\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.2641 - acc: 0.9500\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.2215 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.2550 - acc: 0.9000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.1740 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.1269 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.1343 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.0796 - acc: 1.0000\n",
      "CPU times: user 14.4 s, sys: 242 ms, total: 14.7 s\n",
      "Wall time: 14.3 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 4.333\n",
      "Test accuracy: 0.272\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.091\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 10s - loss: 15.8941 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 15.0239 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 13.1451 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 10.0565 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 7.3730 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 5.1642 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.4973 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.4679 - acc: 0.1500\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 1.9878 - acc: 0.3300\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.7438 - acc: 0.2900\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.5973 - acc: 0.3000\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.5847 - acc: 0.3200\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.5265 - acc: 0.2500\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.5391 - acc: 0.3200\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.5153 - acc: 0.3100\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.4083 - acc: 0.4000\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.4891 - acc: 0.3800\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.3695 - acc: 0.4600\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.3177 - acc: 0.5100\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.3604 - acc: 0.4500\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.2213 - acc: 0.5600\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.2978 - acc: 0.5200\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.1510 - acc: 0.5800\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.2374 - acc: 0.5000\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.1112 - acc: 0.5600\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.0959 - acc: 0.5700\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.0080 - acc: 0.6500\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.0243 - acc: 0.5800\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 0.9183 - acc: 0.6300\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 0.8641 - acc: 0.6700\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.8989 - acc: 0.6700\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.8083 - acc: 0.6900\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.8304 - acc: 0.6500\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.7229 - acc: 0.7400\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.7051 - acc: 0.7700\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.6162 - acc: 0.8000\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.6555 - acc: 0.7600\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.5925 - acc: 0.7500\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.5217 - acc: 0.8200\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.4928 - acc: 0.8300\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.4440 - acc: 0.8600\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.4185 - acc: 0.8700\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.3695 - acc: 0.9000\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.3516 - acc: 0.8800\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.3433 - acc: 0.8800\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.2740 - acc: 0.9000\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.2600 - acc: 0.9200\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.1700 - acc: 0.9900\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.1645 - acc: 0.9900\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.1437 - acc: 0.9700\n",
      "CPU times: user 15.9 s, sys: 655 ms, total: 16.6 s\n",
      "Wall time: 17.4 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 3.402\n",
      "Test accuracy: 0.378\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.147\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 10s - loss: 15.8880 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 15.1668 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 13.0408 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 10.1372 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 7.3607 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 5.1820 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.4828 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.5703 - acc: 0.1300\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 2.0016 - acc: 0.2650\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.7845 - acc: 0.2450\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.7302 - acc: 0.2150\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.6795 - acc: 0.2450\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.7155 - acc: 0.2300\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.6521 - acc: 0.2050\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.6506 - acc: 0.2550\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.5970 - acc: 0.2550\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.5652 - acc: 0.3550\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.5850 - acc: 0.3050\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.5770 - acc: 0.4050\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.5217 - acc: 0.3450\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.5169 - acc: 0.2950\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.4717 - acc: 0.3550\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.4736 - acc: 0.3500\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.4536 - acc: 0.3450\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.4198 - acc: 0.3700\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.3760 - acc: 0.3950\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.3078 - acc: 0.4400\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.2852 - acc: 0.4650\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.2536 - acc: 0.4850\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.2457 - acc: 0.4200\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 1.2215 - acc: 0.4850\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 1.1841 - acc: 0.4150\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 1.1250 - acc: 0.4750\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 1.1699 - acc: 0.4250\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 1.0803 - acc: 0.4950\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 1.0231 - acc: 0.5600\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 1.0460 - acc: 0.5250\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 1.0162 - acc: 0.5350\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 0.9576 - acc: 0.6050\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 0.8891 - acc: 0.6600\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 0.8689 - acc: 0.6500\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 0.9278 - acc: 0.6100\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 0.8847 - acc: 0.6100\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.8265 - acc: 0.6550\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.8030 - acc: 0.6850\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.7402 - acc: 0.7250\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.7368 - acc: 0.7000\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.6666 - acc: 0.7750\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.6135 - acc: 0.7950\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.5453 - acc: 0.8350\n",
      "CPU times: user 17.7 s, sys: 1.12 s, total: 18.9 s\n",
      "Wall time: 21.2 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 3.180\n",
      "Test accuracy: 0.254\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.067\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 10s - loss: 15.7711 - acc: 0.0000e+00    \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 12.5268 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 7.0510 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.4254 - acc: 0.0233         \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 1.9977 - acc: 0.2400     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.7254 - acc: 0.2133     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.6801 - acc: 0.2567     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.6350 - acc: 0.2833     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.5778 - acc: 0.3100     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.5824 - acc: 0.2467     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.5280 - acc: 0.3233     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.5084 - acc: 0.3500     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.4700 - acc: 0.3600     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.4524 - acc: 0.3800     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.3574 - acc: 0.4133     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.3369 - acc: 0.3967     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.2789 - acc: 0.4200     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.2147 - acc: 0.4167     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.1854 - acc: 0.4867     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.1722 - acc: 0.4767     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.0842 - acc: 0.4667     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.0214 - acc: 0.5267     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.0754 - acc: 0.5000     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 0.9648 - acc: 0.5433     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 0.9666 - acc: 0.5800     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 1.0096 - acc: 0.5500     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 0.9012 - acc: 0.6233     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.8662 - acc: 0.6567     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.8924 - acc: 0.6100     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.7911 - acc: 0.6533     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.7919 - acc: 0.6333     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.7111 - acc: 0.6833     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.7327 - acc: 0.7033     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.7715 - acc: 0.6867     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.6775 - acc: 0.7600     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.6424 - acc: 0.6867     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.6008 - acc: 0.7367     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.5870 - acc: 0.7500     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.5600 - acc: 0.7500     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.5198 - acc: 0.7933     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.4784 - acc: 0.8133     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.4422 - acc: 0.8200     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.4297 - acc: 0.8333     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.3988 - acc: 0.8733     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.3460 - acc: 0.8733     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.3699 - acc: 0.8933     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.2996 - acc: 0.9067     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.2636 - acc: 0.9233     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.2336 - acc: 0.9233     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.2095 - acc: 0.9500     \n",
      "CPU times: user 21.1 s, sys: 1.7 s, total: 22.8 s\n",
      "Wall time: 26.2 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 3.510\n",
      "Test accuracy: 0.314\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.332\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 11s - loss: 15.6057 - acc: 0.0000e+00    \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 11.7861 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 6.5893 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 3.1755 - acc: 0.0700     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.9963 - acc: 0.2075     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.7281 - acc: 0.2475     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.7005 - acc: 0.2725     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.6348 - acc: 0.2625     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.6221 - acc: 0.2675     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.6198 - acc: 0.2725     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.5758 - acc: 0.2725     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.5561 - acc: 0.2675     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.5574 - acc: 0.2925     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.4514 - acc: 0.4050     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.4325 - acc: 0.3875     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.4102 - acc: 0.4025     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.3095 - acc: 0.4600     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.2197 - acc: 0.4900     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.1794 - acc: 0.5250     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 1.0852 - acc: 0.5625     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 1.0225 - acc: 0.5750     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 0.9269 - acc: 0.6500     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.8856 - acc: 0.6200     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.8312 - acc: 0.6375     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.7763 - acc: 0.6675     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.7444 - acc: 0.6850     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.6915 - acc: 0.7125     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.6261 - acc: 0.7700     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.5707 - acc: 0.7700     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.4880 - acc: 0.8350     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.4619 - acc: 0.8475     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.4256 - acc: 0.8650     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.3509 - acc: 0.9000     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.3282 - acc: 0.9150     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.2540 - acc: 0.9300     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.2603 - acc: 0.9300     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.2038 - acc: 0.9500     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.1862 - acc: 0.9500     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.1267 - acc: 0.9675     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.1284 - acc: 0.9675     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.1088 - acc: 0.9775     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.0958 - acc: 0.9775     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.1248 - acc: 0.9675     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.0831 - acc: 0.9775     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.0989 - acc: 0.9675     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1062 - acc: 0.9725     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.0938 - acc: 0.9725     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.0517 - acc: 0.9875     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.0812 - acc: 0.9725     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.0792 - acc: 0.9775     \n",
      "CPU times: user 23 s, sys: 2.23 s, total: 25.2 s\n",
      "Wall time: 30.1 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 6.568\n",
      "Test accuracy: 0.318\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.985\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 11s - loss: 15.4882 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 11.3880 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 6.1590 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.9674 - acc: 0.0840         \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.9182 - acc: 0.2260     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6717 - acc: 0.2540     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.6599 - acc: 0.2640     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6732 - acc: 0.2140     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.6392 - acc: 0.2640     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.5878 - acc: 0.2960     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.5565 - acc: 0.2760     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.5341 - acc: 0.3040     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.4922 - acc: 0.3480     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.4252 - acc: 0.3680     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.3759 - acc: 0.3940     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.3225 - acc: 0.3960     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.3069 - acc: 0.4280     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.2173 - acc: 0.4820     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.1551 - acc: 0.5020     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.1277 - acc: 0.5300     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 1.0682 - acc: 0.5460     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.9919 - acc: 0.6140     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.9453 - acc: 0.6120     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.8600 - acc: 0.6520     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.8041 - acc: 0.6960     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.7231 - acc: 0.7500     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.6838 - acc: 0.7500     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.6373 - acc: 0.7860     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.5168 - acc: 0.8400     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.4647 - acc: 0.8600     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.4379 - acc: 0.8600     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.3953 - acc: 0.8600     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.3023 - acc: 0.9080     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.2625 - acc: 0.9320     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.2502 - acc: 0.9300     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.1905 - acc: 0.9420     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.1555 - acc: 0.9620     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.1298 - acc: 0.9680     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.0983 - acc: 0.9800     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.1024 - acc: 0.9680     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0782 - acc: 0.9800     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0739 - acc: 0.9820     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.1120 - acc: 0.9700     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.1057 - acc: 0.9700     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.1122 - acc: 0.9620     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0747 - acc: 0.9780     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.1082 - acc: 0.9720     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0522 - acc: 0.9880     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0816 - acc: 0.9740     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0692 - acc: 0.9780     \n",
      "CPU times: user 24.8 s, sys: 2.73 s, total: 27.6 s\n",
      "Wall time: 34 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 6.519\n",
      "Test accuracy: 0.388\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.016\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    ## dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_reptiles[:num_train_images], Y_train_reptiles[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_reptiles, Y_test_reptiles, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without dreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 4s - loss: 14.5180 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 13.2704 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 10.5867 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.8858 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.3274 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 4.4993 - acc: 0.0000e+00\n",
      "CPU times: user 6.95 s, sys: 16.5 ms, total: 6.96 s\n",
      "Wall time: 6.87 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 11.199\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.129\n",
      "Test accuracy: 0.736\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 4s - loss: 15.8054 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 16.0474 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 15.7604 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 14.6853 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 12.2372 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 10.7935 - acc: 0.0000e+00\n",
      "CPU times: user 7.11 s, sys: 26 ms, total: 7.14 s\n",
      "Wall time: 7.04 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 9.850\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.523\n",
      "Test accuracy: 0.626\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 4s - loss: 15.7774 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.7097 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 15.4221 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 14.1642 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 11.9200 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 9.6311 - acc: 0.0000e+00\n",
      "CPU times: user 7.36 s, sys: 28.6 ms, total: 7.39 s\n",
      "Wall time: 7.3 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 8.090\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.572\n",
      "Test accuracy: 0.570\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 4s - loss: 15.9990 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.6871 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 15.1514 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 12.9001 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 10.8180 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 8.7406 - acc: 0.0000e+00\n",
      "CPU times: user 7.51 s, sys: 50.4 ms, total: 7.56 s\n",
      "Wall time: 7.47 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 7.235\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.245\n",
      "Test accuracy: 0.396\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 5s - loss: 15.7354 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.5223 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 14.1004 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 11.8059 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 9.2434 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 6.8152 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 5.4184 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 4.0894 - acc: 0.2000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 3.4364 - acc: 0.2000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.4301 - acc: 0.2500\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.9646 - acc: 0.3500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.6545 - acc: 0.4000\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.5957 - acc: 0.3500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.4672 - acc: 0.3000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.5696 - acc: 0.3000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.5603 - acc: 0.3500\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.6394 - acc: 0.3000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.3889 - acc: 0.4500\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.2610 - acc: 0.6000\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.1844 - acc: 0.5500\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 0.9866 - acc: 0.6000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 0.9953 - acc: 0.7000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 0.9927 - acc: 0.6000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 0.8891 - acc: 0.6000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 0.9092 - acc: 0.6500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.8932 - acc: 0.6500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.8481 - acc: 0.6500\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.6575 - acc: 0.8000\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.6051 - acc: 0.8000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.5635 - acc: 0.9000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.5442 - acc: 0.6500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.5714 - acc: 0.8000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.6138 - acc: 0.7000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.4809 - acc: 0.8000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.4641 - acc: 0.8500\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.4151 - acc: 0.9000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.3540 - acc: 0.8500\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.3914 - acc: 0.8000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.2994 - acc: 0.9500\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.4296 - acc: 0.8000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.2890 - acc: 0.8500\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.2326 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.2595 - acc: 0.9000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.2375 - acc: 0.9500\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.2322 - acc: 0.9000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.1434 - acc: 0.9500\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.1343 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.1246 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.0985 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.1013 - acc: 1.0000\n",
      "CPU times: user 9.29 s, sys: 188 ms, total: 9.48 s\n",
      "Wall time: 9.19 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 4.977\n",
      "Test accuracy: 0.276\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.304\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 5s - loss: 15.8498 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 15.0327 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 12.8776 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 10.0287 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 7.2156 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 5.2843 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 4.0020 - acc: 0.1900\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 3.1674 - acc: 0.2400\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 2.4105 - acc: 0.2400\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.8900 - acc: 0.3000\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.6243 - acc: 0.2700\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.5845 - acc: 0.3000\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.7448 - acc: 0.2600\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.7483 - acc: 0.2500\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.7175 - acc: 0.3100\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.5660 - acc: 0.3500\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.5394 - acc: 0.3000\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.3584 - acc: 0.4000\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.3206 - acc: 0.4200\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.3419 - acc: 0.3400\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.3778 - acc: 0.3800\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.3402 - acc: 0.4400\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.2575 - acc: 0.4500\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.2277 - acc: 0.4500\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.1893 - acc: 0.4100\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.1374 - acc: 0.4400\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.1538 - acc: 0.3700\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.0558 - acc: 0.4400\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 1.0939 - acc: 0.4300\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 0.9611 - acc: 0.5200\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.9710 - acc: 0.4700\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.9735 - acc: 0.4700\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.9050 - acc: 0.5000\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.9256 - acc: 0.5100\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.8950 - acc: 0.5400\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.8390 - acc: 0.5900\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.8513 - acc: 0.5500\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.8074 - acc: 0.6000\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.7860 - acc: 0.6000\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.7798 - acc: 0.5900\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.7631 - acc: 0.6200\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.7626 - acc: 0.6400\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.6526 - acc: 0.8100\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.6435 - acc: 0.7100\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.5919 - acc: 0.7600\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.5404 - acc: 0.7800\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.5718 - acc: 0.7300\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.5022 - acc: 0.7600\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.5001 - acc: 0.7900\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.4475 - acc: 0.7400\n",
      "CPU times: user 12.1 s, sys: 606 ms, total: 12.7 s\n",
      "Wall time: 13.5 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 3.209\n",
      "Test accuracy: 0.330\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.394\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 5s - loss: 15.7816 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.5004 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 11.8951 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 9.1339 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.5942 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.5670 - acc: 0.0050\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.3376 - acc: 0.2200\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.6940 - acc: 0.2400\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 2.1447 - acc: 0.3300\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.7732 - acc: 0.4100\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6474 - acc: 0.3650\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.6376 - acc: 0.3900\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.6793 - acc: 0.3000\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.5170 - acc: 0.3200\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.4892 - acc: 0.3250\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.4450 - acc: 0.3100\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.3688 - acc: 0.3250\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.3091 - acc: 0.4250\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.2418 - acc: 0.4350\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.3064 - acc: 0.4500\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.1832 - acc: 0.4400\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.2370 - acc: 0.4350\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.1469 - acc: 0.4400\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.1446 - acc: 0.4500\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.1202 - acc: 0.4400\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.0461 - acc: 0.5100\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.0474 - acc: 0.5000\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.0564 - acc: 0.4850\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.0052 - acc: 0.5500\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 0.9898 - acc: 0.5200\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 0.9170 - acc: 0.5700\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 0.8956 - acc: 0.5300\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 0.8706 - acc: 0.5850\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 0.8764 - acc: 0.5650\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 0.8291 - acc: 0.6050\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 0.7751 - acc: 0.6450\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 0.7930 - acc: 0.6400\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 0.7458 - acc: 0.6850\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 0.7104 - acc: 0.7000\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 0.6912 - acc: 0.7000\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 0.6592 - acc: 0.7700\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 0.6263 - acc: 0.7700\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 0.6176 - acc: 0.7600\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.5883 - acc: 0.8000\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.5531 - acc: 0.7850\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.5089 - acc: 0.8650\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.4699 - acc: 0.8850\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.4557 - acc: 0.8800\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.4316 - acc: 0.8950\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.3896 - acc: 0.9050\n",
      "CPU times: user 12.6 s, sys: 1.05 s, total: 13.7 s\n",
      "Wall time: 16.1 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 2.806\n",
      "Test accuracy: 0.364\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.353\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 5s - loss: 15.6501 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 12.0297 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 6.7975 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.4023 - acc: 0.0967     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 2.1620 - acc: 0.2500     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.6469 - acc: 0.2933     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.7254 - acc: 0.2900     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.6842 - acc: 0.2800     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.4921 - acc: 0.3100     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.3729 - acc: 0.4100     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.3565 - acc: 0.3967     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.3020 - acc: 0.4267     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.2249 - acc: 0.4533     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.2287 - acc: 0.4267     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.2227 - acc: 0.3833     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.1520 - acc: 0.4867     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.1825 - acc: 0.4167     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.1203 - acc: 0.5000     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.0920 - acc: 0.5100     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.0596 - acc: 0.4933     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.0647 - acc: 0.5100     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 0.9934 - acc: 0.5133     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.0110 - acc: 0.4833     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 0.9571 - acc: 0.5467     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 0.9243 - acc: 0.5433     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 0.8878 - acc: 0.5967     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 0.8299 - acc: 0.6300     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.8178 - acc: 0.6533     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.7295 - acc: 0.7033     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.7727 - acc: 0.6467     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.7445 - acc: 0.6367     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.6839 - acc: 0.6867     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.6412 - acc: 0.7400     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.5987 - acc: 0.7667     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.6169 - acc: 0.7367     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.5685 - acc: 0.7600     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.5253 - acc: 0.7867     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.4981 - acc: 0.8367     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.4533 - acc: 0.8400     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.4165 - acc: 0.8667     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.4404 - acc: 0.8467     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.4027 - acc: 0.8433     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.4233 - acc: 0.8400     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.2944 - acc: 0.9033     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.2918 - acc: 0.9167     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.3185 - acc: 0.8900     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.2644 - acc: 0.8933     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.1971 - acc: 0.9500     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.2220 - acc: 0.9333     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.2245 - acc: 0.9200     \n",
      "CPU times: user 15.9 s, sys: 1.66 s, total: 17.6 s\n",
      "Wall time: 21 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 2.670\n",
      "Test accuracy: 0.420\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.434\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 6s - loss: 15.3481 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 11.1065 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 5.9971 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 3.1414 - acc: 0.1975     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 2.0661 - acc: 0.2800     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.6004 - acc: 0.3250     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.6955 - acc: 0.2600     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.6498 - acc: 0.2700     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.4005 - acc: 0.3625     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.3766 - acc: 0.4150     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.4269 - acc: 0.3975     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.2873 - acc: 0.4075     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.2113 - acc: 0.4125     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.2542 - acc: 0.3725     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.2158 - acc: 0.3975     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.1198 - acc: 0.4425     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.0833 - acc: 0.4700     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.0729 - acc: 0.4825     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.0107 - acc: 0.4550     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 0.9561 - acc: 0.5050     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 0.9411 - acc: 0.4950     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 0.8753 - acc: 0.5750     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.8672 - acc: 0.5975     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.8097 - acc: 0.5925     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.7653 - acc: 0.6350     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.7353 - acc: 0.6400     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.7421 - acc: 0.6200     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.7049 - acc: 0.6250     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.6564 - acc: 0.6975     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.6088 - acc: 0.7050     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.5726 - acc: 0.7250     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.5497 - acc: 0.7525     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.4973 - acc: 0.7650     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.4703 - acc: 0.8050     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.4156 - acc: 0.8475     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.3865 - acc: 0.8575     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.3253 - acc: 0.8925     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.3087 - acc: 0.9025     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.3152 - acc: 0.8725     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.2775 - acc: 0.9100     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.2588 - acc: 0.9000     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.1785 - acc: 0.9475     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.1413 - acc: 0.9550     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.1109 - acc: 0.9650     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.0878 - acc: 0.9800     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1215 - acc: 0.9550     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.0842 - acc: 0.9775     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.1021 - acc: 0.9675     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.0880 - acc: 0.9675     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.1189 - acc: 0.9650     \n",
      "CPU times: user 17.8 s, sys: 2.17 s, total: 19.9 s\n",
      "Wall time: 24.8 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 3.475\n",
      "Test accuracy: 0.466\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.823\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 6s - loss: 15.2580 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.9534 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.8419 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.9842 - acc: 0.1660     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.8414 - acc: 0.2640     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6091 - acc: 0.2960     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.6425 - acc: 0.2700     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.5093 - acc: 0.2900     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.4026 - acc: 0.3640     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.4083 - acc: 0.3740     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.2809 - acc: 0.3920     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.2564 - acc: 0.4040     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.2329 - acc: 0.3680     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.1896 - acc: 0.4420     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.1370 - acc: 0.4320     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.0950 - acc: 0.4760     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.0774 - acc: 0.4600     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.0035 - acc: 0.5000     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.0252 - acc: 0.4860     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 0.9151 - acc: 0.5660     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.9212 - acc: 0.5520     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.8863 - acc: 0.5800     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.8577 - acc: 0.5860     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.8096 - acc: 0.5760     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.7646 - acc: 0.6400     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.7444 - acc: 0.6220     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.6786 - acc: 0.6560     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.6665 - acc: 0.6740     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.6381 - acc: 0.6960     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.5728 - acc: 0.7540     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.5600 - acc: 0.7440     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.5505 - acc: 0.7400     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.4931 - acc: 0.8080     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.4484 - acc: 0.8160     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.4103 - acc: 0.8600     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.3984 - acc: 0.8480     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.3505 - acc: 0.8720     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.3029 - acc: 0.8980     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.2728 - acc: 0.8980     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.2444 - acc: 0.9140     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.2117 - acc: 0.9380     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.1924 - acc: 0.9440     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.1697 - acc: 0.9500     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.1452 - acc: 0.9520     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0878 - acc: 0.9880     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.1086 - acc: 0.9580     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0886 - acc: 0.9720     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0857 - acc: 0.9760     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0819 - acc: 0.9820     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0835 - acc: 0.9780     \n",
      "CPU times: user 19.6 s, sys: 2.82 s, total: 22.4 s\n",
      "Wall time: 28.8 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 4.589\n",
      "Test accuracy: 0.448\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.061\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000, 2000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### Without dreaming\n",
    "    # dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_aquatic_mammals[:num_train_images], Y_train_aquatic_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s - loss: 0.3574 - acc: 0.9110 - val_loss: 0.3994 - val_acc: 0.9040\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4146 - acc: 0.8970 - val_loss: 0.4011 - val_acc: 0.9040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3586 - acc: 0.9080 - val_loss: 0.4025 - val_acc: 0.9040\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3517 - acc: 0.9160 - val_loss: 0.4041 - val_acc: 0.9040\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3949 - acc: 0.8930 - val_loss: 0.4055 - val_acc: 0.9040\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3864 - acc: 0.9020 - val_loss: 0.4073 - val_acc: 0.9040\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3812 - acc: 0.9090 - val_loss: 0.4089 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3741 - acc: 0.8900 - val_loss: 0.4106 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3522 - acc: 0.9020 - val_loss: 0.4119 - val_acc: 0.8880\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3554 - acc: 0.9150 - val_loss: 0.4135 - val_acc: 0.8880\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3996 - acc: 0.8970 - val_loss: 0.4152 - val_acc: 0.8880\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4166 - acc: 0.8870 - val_loss: 0.4165 - val_acc: 0.8880\n",
      "CPU times: user 9.45 s, sys: 1.25 s, total: 10.7 s\n",
      "Wall time: 14.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.514\n",
      "Test accuracy: 0.612\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 1s - loss: 13.0920 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 6.0322 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 2.6770 - acc: 0.0725     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 1.8506 - acc: 0.2625     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.6729 - acc: 0.2350     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.5915 - acc: 0.2700     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.4994 - acc: 0.3500     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.4627 - acc: 0.3950     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.3819 - acc: 0.4225     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.3218 - acc: 0.4400     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.2545 - acc: 0.4925     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.1086 - acc: 0.5550     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.0008 - acc: 0.5825     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 0.8700 - acc: 0.6375     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 0.7711 - acc: 0.7025     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 0.6730 - acc: 0.7400     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 0.5801 - acc: 0.7725     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 0.5132 - acc: 0.8425     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 0.4075 - acc: 0.8675     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 0.2716 - acc: 0.9225     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 0.2280 - acc: 0.9475     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 0.1416 - acc: 0.9675     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.1022 - acc: 0.9725     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.0793 - acc: 0.9875     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.0548 - acc: 0.9875     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.0614 - acc: 0.9850     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.0273 - acc: 0.9950     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.0375 - acc: 0.9875     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.0321 - acc: 0.9875     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.0267 - acc: 0.9925     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.0318 - acc: 0.9925     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.0588 - acc: 0.9825     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.0285 - acc: 0.9925     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.0244 - acc: 0.9900     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.0437 - acc: 0.9850     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.0440 - acc: 0.9875     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.0320 - acc: 0.9900     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.0412 - acc: 0.9850     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.0292 - acc: 0.9900     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.0290 - acc: 0.9875     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.0361 - acc: 0.9925     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.0729 - acc: 0.9700     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.0905 - acc: 0.9750     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.0539 - acc: 0.9850     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.1095 - acc: 0.9675     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.0561 - acc: 0.9775     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.0890 - acc: 0.9700     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.0732 - acc: 0.9775     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.1417 - acc: 0.9575     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.0308 - acc: 0.9825     \n",
      "CPU times: user 13.3 s, sys: 1.86 s, total: 15.2 s\n",
      "Wall time: 20 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.289\n",
      "Test accuracy: 0.648\n",
      "400 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 15.900\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.3776 - acc: 0.9010 - val_loss: 0.3992 - val_acc: 0.9040\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3700 - acc: 0.8960 - val_loss: 0.4007 - val_acc: 0.9040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3759 - acc: 0.9040 - val_loss: 0.4023 - val_acc: 0.9040\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3905 - acc: 0.9030 - val_loss: 0.4038 - val_acc: 0.9040\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3559 - acc: 0.8960 - val_loss: 0.4057 - val_acc: 0.9040\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3929 - acc: 0.8990 - val_loss: 0.4072 - val_acc: 0.9040\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3936 - acc: 0.8980 - val_loss: 0.4088 - val_acc: 0.8960\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3703 - acc: 0.9060 - val_loss: 0.4104 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3867 - acc: 0.9050 - val_loss: 0.4121 - val_acc: 0.8920\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3678 - acc: 0.8970 - val_loss: 0.4137 - val_acc: 0.8880\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3841 - acc: 0.9050 - val_loss: 0.4154 - val_acc: 0.8880\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3719 - acc: 0.9040 - val_loss: 0.4168 - val_acc: 0.8920\n",
      "CPU times: user 8.74 s, sys: 1.17 s, total: 9.91 s\n",
      "Wall time: 13.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.514\n",
      "Test accuracy: 0.614\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 1s - loss: 12.5843 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 5.6964 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 2.5187 - acc: 0.1440     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 1.8331 - acc: 0.2200     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.6110 - acc: 0.2580     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6179 - acc: 0.2760     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.5424 - acc: 0.2820     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.4759 - acc: 0.3540     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.4484 - acc: 0.3760     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.3738 - acc: 0.4220     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.3024 - acc: 0.4440     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.2483 - acc: 0.4560     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.1408 - acc: 0.5540     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.0765 - acc: 0.5540     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 0.9279 - acc: 0.6380     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 0.8229 - acc: 0.7000     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 0.7446 - acc: 0.7280     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 0.5761 - acc: 0.8020     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 0.4422 - acc: 0.8740     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 0.3397 - acc: 0.9160     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.2676 - acc: 0.9180     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.2166 - acc: 0.9380     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.1499 - acc: 0.9540     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.1179 - acc: 0.9640     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.1027 - acc: 0.9640     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.0714 - acc: 0.9760     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.0580 - acc: 0.9880     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.0332 - acc: 0.9940     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.0438 - acc: 0.9860     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.0332 - acc: 0.9920     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.0316 - acc: 0.9900     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.0277 - acc: 0.9880     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.0379 - acc: 0.9860     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.0277 - acc: 0.9920     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.0152 - acc: 0.9920     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.0582 - acc: 0.9840     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.0615 - acc: 0.9800     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.0208 - acc: 0.9940     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.0227 - acc: 0.9920     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.0720 - acc: 0.9820     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0652 - acc: 0.9880     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0455 - acc: 0.9880     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.0389 - acc: 0.9900     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.0390 - acc: 0.9860     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0267 - acc: 0.9900     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0195 - acc: 0.9940     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0315 - acc: 0.9920     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0274 - acc: 0.9900     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0500 - acc: 0.9860     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0244 - acc: 0.9940     \n",
      "CPU times: user 15.1 s, sys: 2.49 s, total: 17.6 s\n",
      "Wall time: 24 s\n",
      "500 shot leaning, test on B task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 2.908\n",
      "Test accuracy: 0.630\n",
      "500 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 16.057\n",
      "Test accuracy: 0.000\n",
      "1000 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1000 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s - loss: 0.3676 - acc: 0.9030 - val_loss: 0.3989 - val_acc: 0.9040\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3568 - acc: 0.9100 - val_loss: 0.4003 - val_acc: 0.9040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3711 - acc: 0.9030 - val_loss: 0.4017 - val_acc: 0.9040\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4092 - acc: 0.8930 - val_loss: 0.4031 - val_acc: 0.9040\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3917 - acc: 0.8980 - val_loss: 0.4049 - val_acc: 0.9040\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3867 - acc: 0.9010 - val_loss: 0.4063 - val_acc: 0.9040\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4233 - acc: 0.8940 - val_loss: 0.4081 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3922 - acc: 0.9010 - val_loss: 0.4095 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4216 - acc: 0.8930 - val_loss: 0.4111 - val_acc: 0.8920\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3793 - acc: 0.8980 - val_loss: 0.4127 - val_acc: 0.8880\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4063 - acc: 0.8960 - val_loss: 0.4142 - val_acc: 0.8880\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3853 - acc: 0.8980 - val_loss: 0.4156 - val_acc: 0.8880\n",
      "CPU times: user 8.98 s, sys: 1.15 s, total: 10.1 s\n",
      "Wall time: 13.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.512\n",
      "Test accuracy: 0.614\n",
      "1000 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s - loss: 9.1303 - acc: 0.0000e+00      \n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.1880 - acc: 0.1850     \n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.6256 - acc: 0.2770     \n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.5120 - acc: 0.3330     \n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.4025 - acc: 0.3890     \n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.2309 - acc: 0.4990     \n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.0536 - acc: 0.5840     \n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.9517 - acc: 0.6100     \n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.7911 - acc: 0.6960     \n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.6425 - acc: 0.7510     \n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4911 - acc: 0.8280     \n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4079 - acc: 0.8570     \n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3247 - acc: 0.8890     \n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2551 - acc: 0.9120     \n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1946 - acc: 0.9360     \n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1376 - acc: 0.9560     \n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0856 - acc: 0.9780     \n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1056 - acc: 0.9700     \n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0653 - acc: 0.9790     \n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0648 - acc: 0.9760     \n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0968 - acc: 0.9620     \n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0948 - acc: 0.9720     \n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0752 - acc: 0.9710     \n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1138 - acc: 0.9660     \n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0695 - acc: 0.9740     \n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0691 - acc: 0.9780     \n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0576 - acc: 0.9830     \n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0345 - acc: 0.9890     \n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0243 - acc: 0.9940     \n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0303 - acc: 0.9930     \n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0222 - acc: 0.9940     \n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0226 - acc: 0.9960     \n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0115 - acc: 0.9950     \n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0116 - acc: 0.9970     \n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0143 - acc: 0.9970     \n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0220 - acc: 0.9940     \n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0289 - acc: 0.9920     \n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0538 - acc: 0.9870     \n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0492 - acc: 0.9880     \n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0638 - acc: 0.9820     \n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0379 - acc: 0.9860     \n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0169 - acc: 0.9950     \n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0230 - acc: 0.9940     \n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0249 - acc: 0.9910     \n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0150 - acc: 0.9970     \n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0207 - acc: 0.9930     \n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0384 - acc: 0.9920     \n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0171 - acc: 0.9950     \n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0154 - acc: 0.9960     \n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0186 - acc: 0.9930     \n",
      "CPU times: user 27.1 s, sys: 4.65 s, total: 31.8 s\n",
      "Wall time: 44.5 s\n",
      "1000 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 2.452\n",
      "Test accuracy: 0.664\n",
      "1000 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 15.618\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [400, 500, 1000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dreaming with 50 -> 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 13s - loss: 0.4051 - acc: 0.8910 - val_loss: 0.3997 - val_acc: 0.9040\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3962 - acc: 0.9000 - val_loss: 0.4012 - val_acc: 0.9040\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4092 - acc: 0.8910 - val_loss: 0.4028 - val_acc: 0.9040\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3751 - acc: 0.9110 - val_loss: 0.4044 - val_acc: 0.9040\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3599 - acc: 0.9060 - val_loss: 0.4060 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3878 - acc: 0.8900 - val_loss: 0.4077 - val_acc: 0.9040\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3889 - acc: 0.9030 - val_loss: 0.4095 - val_acc: 0.8960\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3941 - acc: 0.9010 - val_loss: 0.4109 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3927 - acc: 0.8900 - val_loss: 0.4125 - val_acc: 0.8920\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3761 - acc: 0.9070 - val_loss: 0.4137 - val_acc: 0.8880\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4068 - acc: 0.8990 - val_loss: 0.4149 - val_acc: 0.8880\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3841 - acc: 0.8980 - val_loss: 0.4163 - val_acc: 0.8880\n",
      "CPU times: user 20.1 s, sys: 1.42 s, total: 21.5 s\n",
      "Wall time: 25.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.513\n",
      "Test accuracy: 0.612\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 11s - loss: 13.7711 - acc: 0.0000e+00    \n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 0s - loss: 6.9145 - acc: 0.0000e+00     \n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 0s - loss: 2.9178 - acc: 0.0467     \n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 0s - loss: 1.9280 - acc: 0.2333     \n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 0s - loss: 1.6901 - acc: 0.3000     \n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 0s - loss: 1.6230 - acc: 0.2767     \n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 0s - loss: 1.6121 - acc: 0.2600     \n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 0s - loss: 1.5595 - acc: 0.3000     \n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 0s - loss: 1.5605 - acc: 0.3000     \n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 0s - loss: 1.4450 - acc: 0.3800     \n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 0s - loss: 1.3987 - acc: 0.3867     \n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 0s - loss: 1.3173 - acc: 0.4233     \n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 0s - loss: 1.2763 - acc: 0.4233     \n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 0s - loss: 1.1916 - acc: 0.4933     \n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 0s - loss: 1.1012 - acc: 0.5367     \n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 0s - loss: 1.0029 - acc: 0.6100     \n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 0s - loss: 0.9011 - acc: 0.6567     \n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 0s - loss: 0.8510 - acc: 0.6767     \n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 0s - loss: 0.7196 - acc: 0.7300     \n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 0s - loss: 0.6180 - acc: 0.7900     \n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 0s - loss: 0.5790 - acc: 0.8067     \n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 0s - loss: 0.4860 - acc: 0.8233     \n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 0s - loss: 0.4093 - acc: 0.8767     \n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 0s - loss: 0.3202 - acc: 0.8967     \n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 0s - loss: 0.3375 - acc: 0.8967     \n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 0s - loss: 0.2795 - acc: 0.9033     \n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 0s - loss: 0.2360 - acc: 0.9200     \n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 0s - loss: 0.2894 - acc: 0.9200     \n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 0s - loss: 0.2906 - acc: 0.9067     \n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 0s - loss: 0.2058 - acc: 0.9300     \n",
      "CPU times: user 18.7 s, sys: 1.13 s, total: 19.9 s\n",
      "Wall time: 21.8 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 2.866\n",
      "Test accuracy: 0.420\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.021\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 13s - loss: 0.3837 - acc: 0.8980 - val_loss: 0.3992 - val_acc: 0.9040\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3898 - acc: 0.8960 - val_loss: 0.4007 - val_acc: 0.9040\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3763 - acc: 0.9020 - val_loss: 0.4022 - val_acc: 0.9040\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3729 - acc: 0.8970 - val_loss: 0.4038 - val_acc: 0.9040\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3766 - acc: 0.9070 - val_loss: 0.4054 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4169 - acc: 0.9020 - val_loss: 0.4070 - val_acc: 0.9040\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3695 - acc: 0.9050 - val_loss: 0.4088 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3876 - acc: 0.8980 - val_loss: 0.4105 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4148 - acc: 0.8870 - val_loss: 0.4118 - val_acc: 0.8920\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4032 - acc: 0.8920 - val_loss: 0.4131 - val_acc: 0.8920\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3791 - acc: 0.9070 - val_loss: 0.4145 - val_acc: 0.8880\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3988 - acc: 0.8910 - val_loss: 0.4163 - val_acc: 0.8880\n",
      "CPU times: user 20.5 s, sys: 1.45 s, total: 22 s\n",
      "Wall time: 25.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.513\n",
      "Test accuracy: 0.614\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "400/400 [==============================] - 12s - loss: 12.9463 - acc: 0.0000e+00    \n",
      "Epoch 2/30\n",
      "400/400 [==============================] - 0s - loss: 5.9784 - acc: 0.0000e+00     \n",
      "Epoch 3/30\n",
      "400/400 [==============================] - 0s - loss: 2.6290 - acc: 0.0850     \n",
      "Epoch 4/30\n",
      "400/400 [==============================] - 0s - loss: 1.8427 - acc: 0.2625     \n",
      "Epoch 5/30\n",
      "400/400 [==============================] - 0s - loss: 1.6592 - acc: 0.2600     \n",
      "Epoch 6/30\n",
      "400/400 [==============================] - 0s - loss: 1.6027 - acc: 0.2675     \n",
      "Epoch 7/30\n",
      "400/400 [==============================] - 0s - loss: 1.5400 - acc: 0.3350     \n",
      "Epoch 8/30\n",
      "400/400 [==============================] - 0s - loss: 1.4658 - acc: 0.3950     \n",
      "Epoch 9/30\n",
      "400/400 [==============================] - 0s - loss: 1.4119 - acc: 0.4050     \n",
      "Epoch 10/30\n",
      "400/400 [==============================] - 0s - loss: 1.3629 - acc: 0.4300     \n",
      "Epoch 11/30\n",
      "400/400 [==============================] - 0s - loss: 1.2340 - acc: 0.4975     \n",
      "Epoch 12/30\n",
      "400/400 [==============================] - 0s - loss: 1.1522 - acc: 0.5450     \n",
      "Epoch 13/30\n",
      "400/400 [==============================] - 0s - loss: 1.0139 - acc: 0.6475     \n",
      "Epoch 14/30\n",
      "400/400 [==============================] - 0s - loss: 0.9437 - acc: 0.6625     \n",
      "Epoch 15/30\n",
      "400/400 [==============================] - 0s - loss: 0.8229 - acc: 0.7175     \n",
      "Epoch 16/30\n",
      "400/400 [==============================] - 0s - loss: 0.6625 - acc: 0.7825     \n",
      "Epoch 17/30\n",
      "400/400 [==============================] - 0s - loss: 0.5377 - acc: 0.8400     \n",
      "Epoch 18/30\n",
      "400/400 [==============================] - 0s - loss: 0.4232 - acc: 0.8675     \n",
      "Epoch 19/30\n",
      "400/400 [==============================] - 0s - loss: 0.3237 - acc: 0.9225     \n",
      "Epoch 20/30\n",
      "400/400 [==============================] - 0s - loss: 0.2478 - acc: 0.9225     \n",
      "Epoch 21/30\n",
      "400/400 [==============================] - 0s - loss: 0.1801 - acc: 0.9550     \n",
      "Epoch 22/30\n",
      "400/400 [==============================] - 0s - loss: 0.1119 - acc: 0.9700     \n",
      "Epoch 23/30\n",
      "400/400 [==============================] - 0s - loss: 0.0896 - acc: 0.9750     \n",
      "Epoch 24/30\n",
      "400/400 [==============================] - 0s - loss: 0.0811 - acc: 0.9825     \n",
      "Epoch 25/30\n",
      "400/400 [==============================] - 0s - loss: 0.0705 - acc: 0.9850     \n",
      "Epoch 26/30\n",
      "400/400 [==============================] - 0s - loss: 0.0725 - acc: 0.9800     \n",
      "Epoch 27/30\n",
      "400/400 [==============================] - 0s - loss: 0.0534 - acc: 0.9800     \n",
      "Epoch 28/30\n",
      "400/400 [==============================] - 0s - loss: 0.0213 - acc: 0.9950     \n",
      "Epoch 29/30\n",
      "400/400 [==============================] - 0s - loss: 0.0424 - acc: 0.9825     \n",
      "Epoch 30/30\n",
      "400/400 [==============================] - 0s - loss: 0.0653 - acc: 0.9875     \n",
      "CPU times: user 20.1 s, sys: 1.43 s, total: 21.5 s\n",
      "Wall time: 24.4 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 5.827\n",
      "Test accuracy: 0.358\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.111\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 14s - loss: 0.3908 - acc: 0.8930 - val_loss: 0.3994 - val_acc: 0.9040\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3998 - acc: 0.8990 - val_loss: 0.4009 - val_acc: 0.9040\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4007 - acc: 0.8970 - val_loss: 0.4026 - val_acc: 0.9040\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4024 - acc: 0.8930 - val_loss: 0.4043 - val_acc: 0.9040\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3700 - acc: 0.9110 - val_loss: 0.4057 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3858 - acc: 0.8950 - val_loss: 0.4073 - val_acc: 0.9040\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3806 - acc: 0.8990 - val_loss: 0.4089 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3602 - acc: 0.9060 - val_loss: 0.4105 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3739 - acc: 0.8990 - val_loss: 0.4122 - val_acc: 0.8920\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3971 - acc: 0.8900 - val_loss: 0.4139 - val_acc: 0.8880\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3952 - acc: 0.8980 - val_loss: 0.4156 - val_acc: 0.8880\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3898 - acc: 0.8960 - val_loss: 0.4169 - val_acc: 0.8880\n",
      "CPU times: user 20.9 s, sys: 1.4 s, total: 22.3 s\n",
      "Wall time: 25.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.514\n",
      "Test accuracy: 0.608\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 12s - loss: 12.6998 - acc: 0.0000e+00     \n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 0s - loss: 5.7523 - acc: 0.0000e+00     \n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 0s - loss: 2.5896 - acc: 0.1120     \n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 0s - loss: 1.8052 - acc: 0.2480     \n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 0s - loss: 1.6225 - acc: 0.2860     \n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 0s - loss: 1.5539 - acc: 0.2940     \n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 0s - loss: 1.5522 - acc: 0.2760     \n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 0s - loss: 1.4649 - acc: 0.3660     \n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 0s - loss: 1.4308 - acc: 0.3340     \n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 0s - loss: 1.3238 - acc: 0.3920     \n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 0s - loss: 1.2221 - acc: 0.4940     \n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 0s - loss: 1.1800 - acc: 0.5080     \n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 0s - loss: 1.0326 - acc: 0.5900     \n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 0s - loss: 0.8762 - acc: 0.6700     \n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 0s - loss: 0.7640 - acc: 0.7020     \n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 0s - loss: 0.6479 - acc: 0.7640     \n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 0s - loss: 0.5230 - acc: 0.8300     \n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 0s - loss: 0.4724 - acc: 0.8400     \n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 0s - loss: 0.3882 - acc: 0.8800     \n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 0s - loss: 0.3082 - acc: 0.9100     \n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 0s - loss: 0.2598 - acc: 0.9280     \n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 0s - loss: 0.2120 - acc: 0.9400     \n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 0s - loss: 0.1892 - acc: 0.9460     \n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 0s - loss: 0.1374 - acc: 0.9700     \n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 0s - loss: 0.0812 - acc: 0.9880     \n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 0s - loss: 0.0842 - acc: 0.9860     \n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 0s - loss: 0.0924 - acc: 0.9760     \n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 0s - loss: 0.0958 - acc: 0.9740     \n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 0s - loss: 0.0419 - acc: 0.9920     \n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 0s - loss: 0.0899 - acc: 0.9780     \n",
      "CPU times: user 21.3 s, sys: 1.76 s, total: 23.1 s\n",
      "Wall time: 26.9 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 4.846\n",
      "Test accuracy: 0.462\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.996\n",
      "Test accuracy: 0.000\n",
      "1000 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1000 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 14s - loss: 0.3885 - acc: 0.8960 - val_loss: 0.3994 - val_acc: 0.9040\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3810 - acc: 0.9050 - val_loss: 0.4010 - val_acc: 0.9040\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3632 - acc: 0.9030 - val_loss: 0.4027 - val_acc: 0.9040\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4019 - acc: 0.8880 - val_loss: 0.4046 - val_acc: 0.9040\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3988 - acc: 0.9000 - val_loss: 0.4061 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3855 - acc: 0.9060 - val_loss: 0.4079 - val_acc: 0.9040\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4089 - acc: 0.8930 - val_loss: 0.4091 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3966 - acc: 0.9070 - val_loss: 0.4110 - val_acc: 0.8920\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3933 - acc: 0.8920 - val_loss: 0.4125 - val_acc: 0.8880\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4111 - acc: 0.8940 - val_loss: 0.4141 - val_acc: 0.8880\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3822 - acc: 0.8960 - val_loss: 0.4154 - val_acc: 0.8880\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3665 - acc: 0.9070 - val_loss: 0.4171 - val_acc: 0.8880\n",
      "CPU times: user 21.3 s, sys: 1.54 s, total: 22.8 s\n",
      "Wall time: 26.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.514\n",
      "Test accuracy: 0.612\n",
      "1000 shot leaning training, on second day task\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 13s - loss: 9.1721 - acc: 0.0000e+00     \n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 2.1892 - acc: 0.2100     \n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.6049 - acc: 0.2890     \n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.4839 - acc: 0.3670     \n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.3495 - acc: 0.3940     \n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.1979 - acc: 0.4870     \n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.0195 - acc: 0.5810     \n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.8748 - acc: 0.6280     \n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.7242 - acc: 0.7160     \n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.5861 - acc: 0.7980     \n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4297 - acc: 0.8520     \n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3552 - acc: 0.8810     \n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3110 - acc: 0.8900     \n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3093 - acc: 0.9060     \n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.2429 - acc: 0.9160     \n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1815 - acc: 0.9430     \n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1706 - acc: 0.9410     \n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1809 - acc: 0.9400     \n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1183 - acc: 0.9670     \n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0788 - acc: 0.9750     \n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0573 - acc: 0.9830     \n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0622 - acc: 0.9810     \n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0550 - acc: 0.9870     \n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0589 - acc: 0.9870     \n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0777 - acc: 0.9750     \n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0475 - acc: 0.9810     \n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0340 - acc: 0.9910     \n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0192 - acc: 0.9920     \n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0293 - acc: 0.9910     \n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0300 - acc: 0.9880     \n",
      "CPU times: user 28.4 s, sys: 3.4 s, total: 31.8 s\n",
      "Wall time: 39.6 s\n",
      "1000 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 3.465\n",
      "Test accuracy: 0.576\n",
      "1000 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.906\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [300, 400, 500, 1000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 30\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium sized mammals -> people with dream and without dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "without dreaming\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s - loss: 15.3929 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 13.5580 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 9.7336 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.7200 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.1197 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 3.5429 - acc: 0.0000e+00\n",
      "CPU times: user 3.39 s, sys: 61.4 ms, total: 3.45 s\n",
      "Wall time: 3.55 s\n",
      "1 shot leaning, test on B task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 11.441\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 1.863\n",
      "Test accuracy: 0.630\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "without dreaming\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 0s - loss: 15.8439 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 15.4800 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 15.3610 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 13.2100 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 10.9546 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 8.4961 - acc: 0.0000e+00\n",
      "CPU times: user 3.65 s, sys: 53.4 ms, total: 3.7 s\n",
      "Wall time: 3.62 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 8.998\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 2.408\n",
      "Test accuracy: 0.272\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "without dreaming\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 1s - loss: 15.9571 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.6684 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 14.1016 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 12.2892 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 9.4142 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 7.0051 - acc: 0.0000e+00\n",
      "CPU times: user 3.73 s, sys: 69.4 ms, total: 3.8 s\n",
      "Wall time: 3.73 s\n",
      "10 shot leaning, test on B task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 7.748\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 1.616\n",
      "Test accuracy: 0.634\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "without dreaming\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 1s - loss: 16.0081 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.0553 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 15.0119 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 12.9106 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 9.6248 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 7.1969 - acc: 0.0000e+00\n",
      "CPU times: user 3.98 s, sys: 78.3 ms, total: 4.05 s\n",
      "Wall time: 3.99 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 6.690\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.500\n",
      "Test accuracy: 0.546\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "without dreaming\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s - loss: 15.7185 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.1366 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 13.6067 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 10.9615 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 8.2620 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 6.0803 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 4.1662 - acc: 0.1500\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 3.2238 - acc: 0.4000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 2.7883 - acc: 0.4000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.4450 - acc: 0.4000\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 2.2616 - acc: 0.4500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 2.1670 - acc: 0.4500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 2.0110 - acc: 0.4500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.8673 - acc: 0.5000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.5328 - acc: 0.4500\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.3911 - acc: 0.6000\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.3288 - acc: 0.5000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.5213 - acc: 0.4000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.4575 - acc: 0.4500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.2659 - acc: 0.5000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.2272 - acc: 0.5500\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.1448 - acc: 0.5000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.0921 - acc: 0.6000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 1.0056 - acc: 0.7000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.0006 - acc: 0.6500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 1.0086 - acc: 0.6500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.8962 - acc: 0.6500\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.9071 - acc: 0.6500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.9631 - acc: 0.6500\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.8141 - acc: 0.6500\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.8284 - acc: 0.7000\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.7719 - acc: 0.6500\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.8898 - acc: 0.6000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.7466 - acc: 0.7000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.6604 - acc: 0.6500\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.6663 - acc: 0.7000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.7319 - acc: 0.6000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.6170 - acc: 0.8000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.5860 - acc: 0.8500\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.5741 - acc: 0.7500\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.6239 - acc: 0.7500\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.5312 - acc: 0.8000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.5754 - acc: 0.8000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.5608 - acc: 0.8000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.4440 - acc: 0.8500\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.4144 - acc: 0.8500\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.3889 - acc: 0.8500\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.4302 - acc: 0.8500\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.3374 - acc: 0.9000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.2713 - acc: 0.9500\n",
      "CPU times: user 5.73 s, sys: 177 ms, total: 5.91 s\n",
      "Wall time: 5.65 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 5.501\n",
      "Test accuracy: 0.190\n",
      "20 shot leaning, test on A task\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 13.525\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "without dreaming\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s - loss: 15.8774 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 15.0928 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 12.8513 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 9.7899 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 7.1057 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 4.8932 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.4202 - acc: 0.1100\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.6978 - acc: 0.2700\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 2.2810 - acc: 0.2700\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.8630 - acc: 0.2400\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.7418 - acc: 0.2300\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.8096 - acc: 0.2300\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.8118 - acc: 0.2700\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.8330 - acc: 0.1700\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.7192 - acc: 0.2000\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.8062 - acc: 0.1500\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.7129 - acc: 0.2200\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.6151 - acc: 0.2300\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.6679 - acc: 0.2600\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.6880 - acc: 0.2700\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.6599 - acc: 0.2900\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.6932 - acc: 0.3300\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.6435 - acc: 0.2500\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.6901 - acc: 0.1700\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.5532 - acc: 0.2800\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.5704 - acc: 0.2900\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.6032 - acc: 0.2800\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.5898 - acc: 0.2800\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 1.5596 - acc: 0.2900\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 1.5444 - acc: 0.2800\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 1.5082 - acc: 0.3400\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 1.4914 - acc: 0.3400\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 1.4757 - acc: 0.3900\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 1.4422 - acc: 0.4100\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 1.4522 - acc: 0.3900\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 1.4170 - acc: 0.4200\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 1.3784 - acc: 0.4300\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 1.3402 - acc: 0.4900\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 1.3598 - acc: 0.4600\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 1.2603 - acc: 0.4600\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 1.2185 - acc: 0.4600\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 1.1705 - acc: 0.5100\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 1.1458 - acc: 0.5200\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 1.0592 - acc: 0.5700\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 1.0163 - acc: 0.5900\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.9720 - acc: 0.6100\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.8868 - acc: 0.6300\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.8993 - acc: 0.6500\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.8428 - acc: 0.5900\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.8061 - acc: 0.6600\n",
      "CPU times: user 7.41 s, sys: 605 ms, total: 8.02 s\n",
      "Wall time: 8.88 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 2.793\n",
      "Test accuracy: 0.226\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.580\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "without dreaming\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 2s - loss: 15.7936 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.6694 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 12.0117 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 9.1136 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.6374 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.6478 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.2200 - acc: 0.0200\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.3503 - acc: 0.1850\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 1.9710 - acc: 0.2250\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.7864 - acc: 0.2200\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.7297 - acc: 0.1950\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.7127 - acc: 0.2300\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.7774 - acc: 0.2250\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.7647 - acc: 0.1950\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.6949 - acc: 0.2200\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.7300 - acc: 0.1500\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.6911 - acc: 0.2150\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.6676 - acc: 0.2500\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.6757 - acc: 0.2200\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.7293 - acc: 0.2300\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.7096 - acc: 0.2050\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.6572 - acc: 0.2200\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.6521 - acc: 0.2600\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.6867 - acc: 0.2300\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.6939 - acc: 0.2300\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.6855 - acc: 0.2350\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.6118 - acc: 0.2600\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.6648 - acc: 0.2150\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.6591 - acc: 0.1950\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.6162 - acc: 0.2400\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 1.6502 - acc: 0.2850\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 1.5829 - acc: 0.2850\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 1.6613 - acc: 0.2600\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 1.6294 - acc: 0.2450\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 1.5913 - acc: 0.2400\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 1.6385 - acc: 0.2300\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 1.6259 - acc: 0.2200\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 1.6065 - acc: 0.2650\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 1.5764 - acc: 0.2650\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 1.5566 - acc: 0.3150\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 1.5268 - acc: 0.2850\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 1.4840 - acc: 0.3500\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 1.4428 - acc: 0.3850\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 1.4688 - acc: 0.3800\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 1.4292 - acc: 0.4200\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 1.4204 - acc: 0.4350\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 1.3934 - acc: 0.4550\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 1.3427 - acc: 0.4550\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 1.3201 - acc: 0.4800\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 1.2175 - acc: 0.5300\n",
      "CPU times: user 8.99 s, sys: 1.16 s, total: 10.1 s\n",
      "Wall time: 12.5 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 1.796\n",
      "Test accuracy: 0.256\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.639\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "without dreaming\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 2s - loss: 15.6670 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 12.1676 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 7.0730 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.5746 - acc: 0.0200     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 2.2516 - acc: 0.2333     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.7611 - acc: 0.2333     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.7601 - acc: 0.2067     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.7641 - acc: 0.2067     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.6606 - acc: 0.1967     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.6945 - acc: 0.2600     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.7066 - acc: 0.2333     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.6233 - acc: 0.2533     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.6708 - acc: 0.2200     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.6393 - acc: 0.2400     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.6207 - acc: 0.2067     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.5492 - acc: 0.2900     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.5781 - acc: 0.3233     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.5543 - acc: 0.2967     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.5454 - acc: 0.3267     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.5181 - acc: 0.2900     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.4969 - acc: 0.3133     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.4688 - acc: 0.3467     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.4111 - acc: 0.3467     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 1.3374 - acc: 0.4233     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 1.3088 - acc: 0.3933     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 1.2472 - acc: 0.4700     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 1.1955 - acc: 0.5300     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 1.0730 - acc: 0.5600     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 1.0513 - acc: 0.5400     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 1.0194 - acc: 0.5267     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.9201 - acc: 0.5733     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.8696 - acc: 0.6033     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.8234 - acc: 0.6600     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.7344 - acc: 0.7000     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.8147 - acc: 0.6367     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.6802 - acc: 0.6733     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.6548 - acc: 0.7233     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.6282 - acc: 0.7700     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.5718 - acc: 0.7833     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.5771 - acc: 0.7600     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.5098 - acc: 0.7733     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.5108 - acc: 0.7933     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.5501 - acc: 0.7600     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.4088 - acc: 0.8300     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.3329 - acc: 0.8833     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.3812 - acc: 0.8633     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.3266 - acc: 0.8767     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.2380 - acc: 0.9300     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.2694 - acc: 0.9033     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.3370 - acc: 0.8600     \n",
      "CPU times: user 12.5 s, sys: 1.72 s, total: 14.2 s\n",
      "Wall time: 17.7 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 4.463\n",
      "Test accuracy: 0.270\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.583\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "without dreaming\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 2s - loss: 15.4281 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 11.3609 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 6.0366 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 2.9336 - acc: 0.0975     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.9707 - acc: 0.2625     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.7099 - acc: 0.2100     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.7370 - acc: 0.2050     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.7199 - acc: 0.1825     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.7105 - acc: 0.1925     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.7066 - acc: 0.2250     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.6731 - acc: 0.1850     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.6999 - acc: 0.1875     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.6688 - acc: 0.2200     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.6636 - acc: 0.2275     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.6720 - acc: 0.2350     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.6270 - acc: 0.2550     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.6027 - acc: 0.2400     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.5607 - acc: 0.2975     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.5708 - acc: 0.2925     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 1.5632 - acc: 0.3250     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 1.5485 - acc: 0.3300     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 1.4851 - acc: 0.3850     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 1.4183 - acc: 0.3925     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 1.3942 - acc: 0.3975     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 1.3292 - acc: 0.4375     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 1.2243 - acc: 0.4900     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 1.1524 - acc: 0.5400     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 1.0884 - acc: 0.5500     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.9754 - acc: 0.6075     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.9011 - acc: 0.6525     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.8102 - acc: 0.6675     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.7083 - acc: 0.7175     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.6137 - acc: 0.7550     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.5187 - acc: 0.8025     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.4938 - acc: 0.8200     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.4531 - acc: 0.8475     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.3858 - acc: 0.8750     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.3607 - acc: 0.8650     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.3767 - acc: 0.8500     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.3418 - acc: 0.8775     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.3239 - acc: 0.9000     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.2533 - acc: 0.9050     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.2783 - acc: 0.9200     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.2238 - acc: 0.9250     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.1936 - acc: 0.9375     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1802 - acc: 0.9375     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.1475 - acc: 0.9425     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.1470 - acc: 0.9475     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.0977 - acc: 0.9775     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.1385 - acc: 0.9625     \n",
      "CPU times: user 14.7 s, sys: 2.1 s, total: 16.8 s\n",
      "Wall time: 21.6 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 5.104\n",
      "Test accuracy: 0.332\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.847\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "without dreaming\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 3s - loss: 15.2784 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.9026 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.8442 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.8536 - acc: 0.1120     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.9243 - acc: 0.2480     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6949 - acc: 0.2340     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.7632 - acc: 0.1840     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.7148 - acc: 0.2000     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.6971 - acc: 0.2400     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.6960 - acc: 0.2200     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.6880 - acc: 0.1920     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.6283 - acc: 0.2440     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.6504 - acc: 0.2100     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.6180 - acc: 0.2500     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.5705 - acc: 0.2820     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.5486 - acc: 0.3020     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.5688 - acc: 0.2780     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.5236 - acc: 0.3480     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.4640 - acc: 0.3580     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.4410 - acc: 0.3680     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 1.3780 - acc: 0.4140     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 1.3520 - acc: 0.4520     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 1.2851 - acc: 0.4500     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 1.2072 - acc: 0.5020     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 1.1394 - acc: 0.4980     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 1.0730 - acc: 0.5120     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 1.0014 - acc: 0.5480     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.9023 - acc: 0.6080     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.9041 - acc: 0.6080     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.8354 - acc: 0.6080     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.7870 - acc: 0.6460     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.7132 - acc: 0.6840     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.7435 - acc: 0.6800     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.6125 - acc: 0.7280     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.6230 - acc: 0.7100     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.5175 - acc: 0.7900     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.5132 - acc: 0.7900     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.4472 - acc: 0.8220     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.4119 - acc: 0.8240     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.3418 - acc: 0.8720     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.3508 - acc: 0.8820     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.2937 - acc: 0.9240     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.2558 - acc: 0.9260     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.2285 - acc: 0.9120     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.1830 - acc: 0.9400     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.1966 - acc: 0.9320     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.1737 - acc: 0.9500     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.1450 - acc: 0.9540     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.1132 - acc: 0.9700     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0678 - acc: 0.9780     \n",
      "CPU times: user 16.3 s, sys: 2.66 s, total: 18.9 s\n",
      "Wall time: 25.3 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 1s     \n",
      "\n",
      "Test loss: 5.356\n",
      "Test accuracy: 0.326\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.861\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"without dreaming\")\n",
    "    ## dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_people[:num_train_images], Y_train_people[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 4s - loss: 15.8619 - acc: 0.0000e+00 - val_loss: 15.8249 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8652 - acc: 0.0000e+00 - val_loss: 15.8214 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.7935 - acc: 0.0000e+00 - val_loss: 15.8172 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8349 - acc: 0.0000e+00 - val_loss: 15.8140 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8121 - acc: 0.0000e+00 - val_loss: 15.8113 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8475 - acc: 0.0000e+00 - val_loss: 15.8095 - val_acc: 0.0000e+00\n",
      "CPU times: user 7.69 s, sys: 898 ms, total: 8.59 s\n",
      "Wall time: 10.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.309\n",
      "Test accuracy: 0.786\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 2s - loss: 14.0007 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 12.4977 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 9.0636 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 7.5845 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 5.9511 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 2.8878 - acc: 0.0000e+00\n",
      "CPU times: user 5.58 s, sys: 45.4 ms, total: 5.63 s\n",
      "Wall time: 5.54 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 12.225\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.882\n",
      "Test accuracy: 0.648\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 3s - loss: 15.8676 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8659 - acc: 0.0000e+00 - val_loss: 15.8206 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8893 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8466 - acc: 0.0000e+00 - val_loss: 15.8143 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8488 - acc: 0.0000e+00 - val_loss: 15.8121 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8251 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "CPU times: user 7.48 s, sys: 666 ms, total: 8.15 s\n",
      "Wall time: 9.96 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.309\n",
      "Test accuracy: 0.786\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 2s - loss: 15.6238 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 15.9448 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 14.7617 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 12.2173 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 10.4758 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 8.3393 - acc: 0.0000e+00\n",
      "CPU times: user 5.3 s, sys: 49.9 ms, total: 5.35 s\n",
      "Wall time: 5.27 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 9.543\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.987\n",
      "Test accuracy: 0.520\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 4s - loss: 15.8453 - acc: 0.0000e+00 - val_loss: 15.8243 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8300 - acc: 0.0000e+00 - val_loss: 15.8206 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8405 - acc: 0.0000e+00 - val_loss: 15.8172 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8517 - acc: 0.0000e+00 - val_loss: 15.8139 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8368 - acc: 0.0000e+00 - val_loss: 15.8115 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8202 - acc: 0.0000e+00 - val_loss: 15.8094 - val_acc: 0.0000e+00\n",
      "CPU times: user 8.49 s, sys: 680 ms, total: 9.17 s\n",
      "Wall time: 11 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.308\n",
      "Test accuracy: 0.786\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 3s - loss: 15.8986 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.7994 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 14.4536 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 11.8757 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 9.8822 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 7.4286 - acc: 0.0000e+00\n",
      "CPU times: user 5.59 s, sys: 49.6 ms, total: 5.64 s\n",
      "Wall time: 5.56 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 8.226\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.734\n",
      "Test accuracy: 0.600\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 4s - loss: 15.8775 - acc: 0.0000e+00 - val_loss: 15.8250 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8507 - acc: 0.0000e+00 - val_loss: 15.8217 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8387 - acc: 0.0000e+00 - val_loss: 15.8174 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8243 - acc: 0.0000e+00 - val_loss: 15.8137 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8286 - acc: 0.0000e+00 - val_loss: 15.8108 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8466 - acc: 0.0000e+00 - val_loss: 15.8087 - val_acc: 0.0000e+00\n",
      "CPU times: user 8.11 s, sys: 697 ms, total: 8.8 s\n",
      "Wall time: 10.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.305\n",
      "Test accuracy: 0.786\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 3s - loss: 15.7444 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.5924 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 14.1574 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 11.9404 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 8.9877 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 7.0170 - acc: 0.0000e+00\n",
      "CPU times: user 5.88 s, sys: 63.8 ms, total: 5.95 s\n",
      "Wall time: 5.87 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 2s     \n",
      "\n",
      "Test loss: 6.363\n",
      "Test accuracy: 0.062\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.822\n",
      "Test accuracy: 0.364\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s - loss: 15.8827 - acc: 0.0000e+00 - val_loss: 15.8249 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8740 - acc: 0.0000e+00 - val_loss: 15.8220 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8359 - acc: 0.0000e+00 - val_loss: 15.8181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8353 - acc: 0.0000e+00 - val_loss: 15.8147 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8706 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8274 - acc: 0.0000e+00 - val_loss: 15.8108 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8472 - acc: 0.0000e+00 - val_loss: 15.8098 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8374 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8180 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8155 - acc: 0.0000e+00 - val_loss: 15.8058 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8300 - acc: 0.0000e+00 - val_loss: 15.8043 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8191 - acc: 0.0000e+00 - val_loss: 15.8018 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8279 - acc: 0.0000e+00 - val_loss: 15.7984 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8212 - acc: 0.0000e+00 - val_loss: 15.7954 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7889 - acc: 0.0000e+00 - val_loss: 15.7924 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7641 - acc: 0.0000e+00 - val_loss: 15.7905 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7942 - acc: 0.0000e+00 - val_loss: 15.7884 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7914 - acc: 0.0000e+00 - val_loss: 15.7862 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7720 - acc: 0.0000e+00 - val_loss: 15.7844 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7841 - acc: 0.0000e+00 - val_loss: 15.7830 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7721 - acc: 0.0000e+00 - val_loss: 15.7810 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7768 - acc: 0.0000e+00 - val_loss: 15.7776 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7709 - acc: 0.0000e+00 - val_loss: 15.7737 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7546 - acc: 0.0000e+00 - val_loss: 15.7686 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7797 - acc: 0.0000e+00 - val_loss: 15.7634 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7496 - acc: 0.0000e+00 - val_loss: 15.7581 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7574 - acc: 0.0000e+00 - val_loss: 15.7520 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7211 - acc: 0.0000e+00 - val_loss: 15.7467 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7174 - acc: 0.0000e+00 - val_loss: 15.7418 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7362 - acc: 0.0000e+00 - val_loss: 15.7365 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6879 - acc: 0.0000e+00 - val_loss: 15.7306 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6959 - acc: 0.0000e+00 - val_loss: 15.7251 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7026 - acc: 0.0000e+00 - val_loss: 15.7188 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7065 - acc: 0.0000e+00 - val_loss: 15.7128 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6815 - acc: 0.0000e+00 - val_loss: 15.7068 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6515 - acc: 0.0000e+00 - val_loss: 15.7012 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6921 - acc: 0.0000e+00 - val_loss: 15.6951 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6684 - acc: 0.0000e+00 - val_loss: 15.6886 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6672 - acc: 0.0000e+00 - val_loss: 15.6822 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6733 - acc: 0.0000e+00 - val_loss: 15.6763 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6569 - acc: 0.0000e+00 - val_loss: 15.6708 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6259 - acc: 0.0000e+00 - val_loss: 15.6650 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6277 - acc: 0.0000e+00 - val_loss: 15.6585 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6470 - acc: 0.0000e+00 - val_loss: 15.6522 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5738 - acc: 0.0000e+00 - val_loss: 15.6456 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5909 - acc: 0.0000e+00 - val_loss: 15.6388 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5616 - acc: 0.0000e+00 - val_loss: 15.6323 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5992 - acc: 0.0000e+00 - val_loss: 15.6254 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5853 - acc: 0.0000e+00 - val_loss: 15.6186 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5921 - acc: 0.0000e+00 - val_loss: 15.6116 - val_acc: 0.0000e+00\n",
      "CPU times: user 29.3 s, sys: 5.39 s, total: 34.7 s\n",
      "Wall time: 50.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.227\n",
      "Test accuracy: 0.802\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 3s - loss: 15.7394 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.1113 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 13.5733 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 10.4804 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 7.8556 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 5.6665 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 3.8910 - acc: 0.2500\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 3.1273 - acc: 0.4000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 2.8155 - acc: 0.4000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.3787 - acc: 0.4500\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 2.2600 - acc: 0.5500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 2.1494 - acc: 0.4000\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 2.0956 - acc: 0.4500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.7343 - acc: 0.4000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.6091 - acc: 0.4000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.3100 - acc: 0.5000\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.2966 - acc: 0.4000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.2619 - acc: 0.5500\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.4721 - acc: 0.4000\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.4376 - acc: 0.5500\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.2999 - acc: 0.5000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.1959 - acc: 0.4500\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.0191 - acc: 0.5500\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 0.9419 - acc: 0.5500\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.0029 - acc: 0.6000\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.9813 - acc: 0.6500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 1.0336 - acc: 0.6500\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.8336 - acc: 0.6500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.7802 - acc: 0.7500\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.7420 - acc: 0.7000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.7014 - acc: 0.7000\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.7159 - acc: 0.7500\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.6937 - acc: 0.8000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.7204 - acc: 0.8000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.7156 - acc: 0.8500\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.6178 - acc: 0.8000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.6026 - acc: 0.7500\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.7413 - acc: 0.7000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.5357 - acc: 0.8500\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.5518 - acc: 0.8500\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.4980 - acc: 0.8000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.5488 - acc: 0.8000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.5031 - acc: 0.8500\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.5019 - acc: 0.8000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.4979 - acc: 0.8000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.4202 - acc: 0.8500\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.3360 - acc: 0.8500\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.2777 - acc: 0.9000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.3347 - acc: 0.9000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.2920 - acc: 0.9000\n",
      "CPU times: user 7.86 s, sys: 184 ms, total: 8.05 s\n",
      "Wall time: 7.76 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 6.404\n",
      "Test accuracy: 0.202\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.581\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8290 - acc: 0.0000e+00 - val_loss: 15.8242 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8373 - acc: 0.0000e+00 - val_loss: 15.8206 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8701 - acc: 0.0000e+00 - val_loss: 15.8170 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8371 - acc: 0.0000e+00 - val_loss: 15.8142 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8247 - acc: 0.0000e+00 - val_loss: 15.8120 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8447 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8166 - acc: 0.0000e+00 - val_loss: 15.8091 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8125 - acc: 0.0000e+00 - val_loss: 15.8084 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8402 - acc: 0.0000e+00 - val_loss: 15.8067 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8028 - acc: 0.0000e+00 - val_loss: 15.8056 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7869 - acc: 0.0000e+00 - val_loss: 15.8037 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7970 - acc: 0.0000e+00 - val_loss: 15.8006 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7927 - acc: 0.0000e+00 - val_loss: 15.7967 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8110 - acc: 0.0000e+00 - val_loss: 15.7935 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7713 - acc: 0.0000e+00 - val_loss: 15.7907 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7866 - acc: 0.0000e+00 - val_loss: 15.7884 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7678 - acc: 0.0000e+00 - val_loss: 15.7866 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7892 - acc: 0.0000e+00 - val_loss: 15.7851 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7978 - acc: 0.0000e+00 - val_loss: 15.7838 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7956 - acc: 0.0000e+00 - val_loss: 15.7810 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8029 - acc: 0.0000e+00 - val_loss: 15.7786 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7858 - acc: 0.0000e+00 - val_loss: 15.7748 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7647 - acc: 0.0000e+00 - val_loss: 15.7700 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7259 - acc: 0.0000e+00 - val_loss: 15.7654 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7498 - acc: 0.0000e+00 - val_loss: 15.7609 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7212 - acc: 0.0000e+00 - val_loss: 15.7556 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7455 - acc: 0.0000e+00 - val_loss: 15.7498 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7487 - acc: 0.0000e+00 - val_loss: 15.7443 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7565 - acc: 0.0000e+00 - val_loss: 15.7392 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7109 - acc: 0.0000e+00 - val_loss: 15.7338 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7090 - acc: 0.0000e+00 - val_loss: 15.7285 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7259 - acc: 0.0000e+00 - val_loss: 15.7223 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7027 - acc: 0.0000e+00 - val_loss: 15.7165 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6958 - acc: 0.0000e+00 - val_loss: 15.7105 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6895 - acc: 0.0000e+00 - val_loss: 15.7043 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6556 - acc: 0.0000e+00 - val_loss: 15.6984 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6889 - acc: 0.0000e+00 - val_loss: 15.6928 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6353 - acc: 0.0000e+00 - val_loss: 15.6862 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6302 - acc: 0.0000e+00 - val_loss: 15.6794 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6382 - acc: 0.0000e+00 - val_loss: 15.6738 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6295 - acc: 0.0000e+00 - val_loss: 15.6675 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6182 - acc: 0.0000e+00 - val_loss: 15.6609 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6049 - acc: 0.0000e+00 - val_loss: 15.6543 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6243 - acc: 0.0000e+00 - val_loss: 15.6480 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6410 - acc: 0.0000e+00 - val_loss: 15.6420 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5961 - acc: 0.0000e+00 - val_loss: 15.6354 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6045 - acc: 0.0000e+00 - val_loss: 15.6291 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5748 - acc: 0.0000e+00 - val_loss: 15.6217 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5630 - acc: 0.0000e+00 - val_loss: 15.6146 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5546 - acc: 0.0000e+00 - val_loss: 15.6079 - val_acc: 0.0000e+00\n",
      "CPU times: user 29.2 s, sys: 5.46 s, total: 34.6 s\n",
      "Wall time: 50.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.224\n",
      "Test accuracy: 0.802\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 4s - loss: 15.7981 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 14.7648 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 12.3739 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 9.3671 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 6.8215 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 4.5786 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.1300 - acc: 0.0100\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.3669 - acc: 0.2200\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 1.9832 - acc: 0.2800\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.8288 - acc: 0.2900\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.7404 - acc: 0.1800\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.6748 - acc: 0.1900\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.7058 - acc: 0.2200\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.7332 - acc: 0.1800\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.6945 - acc: 0.2300\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.6872 - acc: 0.2100\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.6491 - acc: 0.2300\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.6775 - acc: 0.2400\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.6244 - acc: 0.3300\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.6512 - acc: 0.2200\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.7112 - acc: 0.2200\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.6964 - acc: 0.2500\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.7275 - acc: 0.1300\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.6109 - acc: 0.2300\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.5569 - acc: 0.2500\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.6399 - acc: 0.2300\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.5684 - acc: 0.2700\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.7255 - acc: 0.1900\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 1.5222 - acc: 0.3300\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 1.5138 - acc: 0.3100\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 1.5640 - acc: 0.2900\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 1.5210 - acc: 0.2900\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 1.4792 - acc: 0.3400\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 1.4417 - acc: 0.3800\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 1.4741 - acc: 0.3500\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 1.3674 - acc: 0.3900\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 1.3472 - acc: 0.4100\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 1.2840 - acc: 0.4600\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 1.2499 - acc: 0.4400\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 1.1862 - acc: 0.4600\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 1.1110 - acc: 0.5500\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.9990 - acc: 0.5900\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 1.0448 - acc: 0.5600\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.9011 - acc: 0.6100\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.8372 - acc: 0.6400\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.7956 - acc: 0.6500\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.7735 - acc: 0.6400\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.7411 - acc: 0.6700\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.6917 - acc: 0.7500\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.6001 - acc: 0.8200\n",
      "CPU times: user 9.44 s, sys: 621 ms, total: 10.1 s\n",
      "Wall time: 10.9 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 2.769\n",
      "Test accuracy: 0.250\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.934\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8407 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8614 - acc: 0.0000e+00 - val_loss: 15.8211 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8648 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8543 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8470 - acc: 0.0000e+00 - val_loss: 15.8116 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8346 - acc: 0.0000e+00 - val_loss: 15.8098 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8174 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8321 - acc: 0.0000e+00 - val_loss: 15.8079 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8338 - acc: 0.0000e+00 - val_loss: 15.8061 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8263 - acc: 0.0000e+00 - val_loss: 15.8041 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8119 - acc: 0.0000e+00 - val_loss: 15.8024 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8029 - acc: 0.0000e+00 - val_loss: 15.7992 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8013 - acc: 0.0000e+00 - val_loss: 15.7960 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7847 - acc: 0.0000e+00 - val_loss: 15.7930 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7827 - acc: 0.0000e+00 - val_loss: 15.7898 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8040 - acc: 0.0000e+00 - val_loss: 15.7877 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7846 - acc: 0.0000e+00 - val_loss: 15.7857 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7587 - acc: 0.0000e+00 - val_loss: 15.7834 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7553 - acc: 0.0000e+00 - val_loss: 15.7810 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7463 - acc: 0.0000e+00 - val_loss: 15.7790 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7627 - acc: 0.0000e+00 - val_loss: 15.7761 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7859 - acc: 0.0000e+00 - val_loss: 15.7732 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7599 - acc: 0.0000e+00 - val_loss: 15.7692 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7273 - acc: 0.0000e+00 - val_loss: 15.7646 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7645 - acc: 0.0000e+00 - val_loss: 15.7600 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7276 - acc: 0.0000e+00 - val_loss: 15.7545 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7343 - acc: 0.0000e+00 - val_loss: 15.7488 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7249 - acc: 0.0000e+00 - val_loss: 15.7432 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7239 - acc: 0.0000e+00 - val_loss: 15.7374 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7053 - acc: 0.0000e+00 - val_loss: 15.7311 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6770 - acc: 0.0000e+00 - val_loss: 15.7254 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7074 - acc: 0.0000e+00 - val_loss: 15.7198 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6787 - acc: 0.0000e+00 - val_loss: 15.7145 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6815 - acc: 0.0000e+00 - val_loss: 15.7089 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6569 - acc: 0.0000e+00 - val_loss: 15.7025 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6703 - acc: 0.0000e+00 - val_loss: 15.6972 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6554 - acc: 0.0000e+00 - val_loss: 15.6908 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6545 - acc: 0.0000e+00 - val_loss: 15.6850 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6521 - acc: 0.0000e+00 - val_loss: 15.6792 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6588 - acc: 0.0000e+00 - val_loss: 15.6729 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6673 - acc: 0.0000e+00 - val_loss: 15.6669 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6513 - acc: 0.0000e+00 - val_loss: 15.6610 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6446 - acc: 0.0000e+00 - val_loss: 15.6550 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6019 - acc: 0.0000e+00 - val_loss: 15.6485 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6119 - acc: 0.0000e+00 - val_loss: 15.6416 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5846 - acc: 0.0000e+00 - val_loss: 15.6342 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6029 - acc: 0.0000e+00 - val_loss: 15.6274 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5779 - acc: 0.0000e+00 - val_loss: 15.6204 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5521 - acc: 0.0000e+00 - val_loss: 15.6125 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5471 - acc: 0.0000e+00 - val_loss: 15.6050 - val_acc: 0.0000e+00\n",
      "CPU times: user 29.7 s, sys: 5.48 s, total: 35.2 s\n",
      "Wall time: 50.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.226\n",
      "Test accuracy: 0.804\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 4s - loss: 15.7122 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.4505 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 11.4743 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 8.6141 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.2323 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.2993 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 2.9613 - acc: 0.0600\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.2656 - acc: 0.2300\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 1.8950 - acc: 0.2400\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.7909 - acc: 0.2400\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6731 - acc: 0.2050\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.7143 - acc: 0.1750\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.7653 - acc: 0.2350\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.7554 - acc: 0.2000\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.6957 - acc: 0.1850\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.6411 - acc: 0.2450\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.6662 - acc: 0.2050\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.7041 - acc: 0.2200\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.7127 - acc: 0.1800\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.6801 - acc: 0.2100\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.6589 - acc: 0.2350\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.6647 - acc: 0.2400\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.7066 - acc: 0.2150\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.6693 - acc: 0.2350\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.6248 - acc: 0.2500\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.6151 - acc: 0.2200\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.6238 - acc: 0.2550\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.6220 - acc: 0.2350\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.6145 - acc: 0.2800\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.5830 - acc: 0.2800\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 1.5515 - acc: 0.3350\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 1.5737 - acc: 0.2250\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 1.5705 - acc: 0.3050\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 1.5677 - acc: 0.2650\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 1.5443 - acc: 0.3000\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 1.4272 - acc: 0.3600\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 1.4957 - acc: 0.3050\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 1.3960 - acc: 0.4200\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 1.3701 - acc: 0.4150\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 1.3417 - acc: 0.3850\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 1.2892 - acc: 0.4600\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 1.2478 - acc: 0.4700\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 1.1727 - acc: 0.4550\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 1.1683 - acc: 0.4300\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 1.0492 - acc: 0.5550\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 1.0463 - acc: 0.5650\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.9745 - acc: 0.5100\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.9250 - acc: 0.5650\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.8556 - acc: 0.6200\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.7919 - acc: 0.6350\n",
      "CPU times: user 11.4 s, sys: 1.16 s, total: 12.6 s\n",
      "Wall time: 15 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 3.218\n",
      "Test accuracy: 0.200\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.916\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8618 - acc: 0.0000e+00 - val_loss: 15.8250 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8590 - acc: 0.0000e+00 - val_loss: 15.8220 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8753 - acc: 0.0000e+00 - val_loss: 15.8188 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8366 - acc: 0.0000e+00 - val_loss: 15.8146 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8369 - acc: 0.0000e+00 - val_loss: 15.8126 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8019 - acc: 0.0000e+00 - val_loss: 15.8105 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8239 - acc: 0.0000e+00 - val_loss: 15.8091 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8399 - acc: 0.0000e+00 - val_loss: 15.8080 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8474 - acc: 0.0000e+00 - val_loss: 15.8070 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8200 - acc: 0.0000e+00 - val_loss: 15.8054 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8447 - acc: 0.0000e+00 - val_loss: 15.8040 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8043 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8181 - acc: 0.0000e+00 - val_loss: 15.7981 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7796 - acc: 0.0000e+00 - val_loss: 15.7949 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7986 - acc: 0.0000e+00 - val_loss: 15.7926 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7759 - acc: 0.0000e+00 - val_loss: 15.7904 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7885 - acc: 0.0000e+00 - val_loss: 15.7892 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7923 - acc: 0.0000e+00 - val_loss: 15.7871 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7630 - acc: 0.0000e+00 - val_loss: 15.7856 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7102 - acc: 0.0000e+00 - val_loss: 15.7838 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7737 - acc: 0.0000e+00 - val_loss: 15.7806 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7762 - acc: 0.0000e+00 - val_loss: 15.7768 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7580 - acc: 0.0000e+00 - val_loss: 15.7714 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7353 - acc: 0.0000e+00 - val_loss: 15.7666 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7750 - acc: 0.0000e+00 - val_loss: 15.7608 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7159 - acc: 0.0000e+00 - val_loss: 15.7550 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7486 - acc: 0.0000e+00 - val_loss: 15.7494 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7134 - acc: 0.0000e+00 - val_loss: 15.7436 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7137 - acc: 0.0000e+00 - val_loss: 15.7390 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7105 - acc: 0.0000e+00 - val_loss: 15.7333 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7431 - acc: 0.0000e+00 - val_loss: 15.7276 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6777 - acc: 0.0000e+00 - val_loss: 15.7226 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6837 - acc: 0.0000e+00 - val_loss: 15.7170 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6902 - acc: 0.0000e+00 - val_loss: 15.7109 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6860 - acc: 0.0000e+00 - val_loss: 15.7046 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6916 - acc: 0.0000e+00 - val_loss: 15.6987 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6480 - acc: 0.0000e+00 - val_loss: 15.6926 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6604 - acc: 0.0000e+00 - val_loss: 15.6865 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6450 - acc: 0.0000e+00 - val_loss: 15.6801 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6489 - acc: 0.0000e+00 - val_loss: 15.6741 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6080 - acc: 0.0000e+00 - val_loss: 15.6679 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6188 - acc: 0.0000e+00 - val_loss: 15.6614 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6152 - acc: 0.0000e+00 - val_loss: 15.6550 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6473 - acc: 0.0000e+00 - val_loss: 15.6485 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6037 - acc: 0.0000e+00 - val_loss: 15.6425 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5910 - acc: 0.0000e+00 - val_loss: 15.6361 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5945 - acc: 0.0000e+00 - val_loss: 15.6293 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5966 - acc: 0.0000e+00 - val_loss: 15.6223 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5889 - acc: 0.0000e+00 - val_loss: 15.6149 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5560 - acc: 0.0000e+00 - val_loss: 15.6076 - val_acc: 0.0000e+00\n",
      "CPU times: user 30.9 s, sys: 5.63 s, total: 36.5 s\n",
      "Wall time: 52.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.804\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 4s - loss: 15.5006 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 11.5831 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 6.3901 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.1000 - acc: 0.0400     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 1.9868 - acc: 0.2300     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.7793 - acc: 0.1967     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.7381 - acc: 0.2100     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.6555 - acc: 0.2533     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.7145 - acc: 0.1800     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.7210 - acc: 0.2333     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.6984 - acc: 0.2233     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.6802 - acc: 0.2433     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.6799 - acc: 0.2100     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.6876 - acc: 0.2433     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.6194 - acc: 0.2333     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.6758 - acc: 0.2133     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.6390 - acc: 0.2233     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.6126 - acc: 0.2333     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.6376 - acc: 0.2133     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.6238 - acc: 0.2567     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.6590 - acc: 0.2567     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.5869 - acc: 0.3200     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.5681 - acc: 0.3300     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 1.5478 - acc: 0.3167     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 1.5320 - acc: 0.3333     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 1.4663 - acc: 0.3367     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 1.4371 - acc: 0.3533     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 1.3509 - acc: 0.4267     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 1.3767 - acc: 0.3900     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 1.2504 - acc: 0.4833     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 1.2119 - acc: 0.5000     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 1.1277 - acc: 0.5100     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 1.0458 - acc: 0.5433     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.9785 - acc: 0.5900     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.9052 - acc: 0.6167     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.8288 - acc: 0.6633     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.7735 - acc: 0.6567     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.6999 - acc: 0.7100     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.6827 - acc: 0.7167     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.6371 - acc: 0.7500     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.6395 - acc: 0.7533     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.5860 - acc: 0.7633     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.5283 - acc: 0.8067     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.5035 - acc: 0.8200     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.4862 - acc: 0.8233     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.4559 - acc: 0.8433     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.4230 - acc: 0.8533     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.3493 - acc: 0.8900     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.3576 - acc: 0.8567     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.3769 - acc: 0.8667     \n",
      "CPU times: user 14.9 s, sys: 1.8 s, total: 16.7 s\n",
      "Wall time: 20.1 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 3s     \n",
      "\n",
      "Test loss: 4.376\n",
      "Test accuracy: 0.262\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.015\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 5s - loss: 15.8503 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8644 - acc: 0.0000e+00 - val_loss: 15.8214 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8545 - acc: 0.0000e+00 - val_loss: 15.8178 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8418 - acc: 0.0000e+00 - val_loss: 15.8144 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8562 - acc: 0.0000e+00 - val_loss: 15.8124 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8072 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8438 - acc: 0.0000e+00 - val_loss: 15.8093 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8418 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8110 - acc: 0.0000e+00 - val_loss: 15.8075 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8498 - acc: 0.0000e+00 - val_loss: 15.8060 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8139 - acc: 0.0000e+00 - val_loss: 15.8042 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8405 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7909 - acc: 0.0000e+00 - val_loss: 15.7975 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8203 - acc: 0.0000e+00 - val_loss: 15.7947 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7847 - acc: 0.0000e+00 - val_loss: 15.7916 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8028 - acc: 0.0000e+00 - val_loss: 15.7899 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7819 - acc: 0.0000e+00 - val_loss: 15.7880 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7684 - acc: 0.0000e+00 - val_loss: 15.7858 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7786 - acc: 0.0000e+00 - val_loss: 15.7842 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7651 - acc: 0.0000e+00 - val_loss: 15.7821 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7550 - acc: 0.0000e+00 - val_loss: 15.7783 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7789 - acc: 0.0000e+00 - val_loss: 15.7740 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7793 - acc: 0.0000e+00 - val_loss: 15.7698 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7378 - acc: 0.0000e+00 - val_loss: 15.7656 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7985 - acc: 0.0000e+00 - val_loss: 15.7606 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7553 - acc: 0.0000e+00 - val_loss: 15.7552 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7448 - acc: 0.0000e+00 - val_loss: 15.7500 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7351 - acc: 0.0000e+00 - val_loss: 15.7450 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7521 - acc: 0.0000e+00 - val_loss: 15.7397 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7232 - acc: 0.0000e+00 - val_loss: 15.7352 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7269 - acc: 0.0000e+00 - val_loss: 15.7304 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7116 - acc: 0.0000e+00 - val_loss: 15.7252 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7372 - acc: 0.0000e+00 - val_loss: 15.7192 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6972 - acc: 0.0000e+00 - val_loss: 15.7135 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6922 - acc: 0.0000e+00 - val_loss: 15.7073 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6816 - acc: 0.0000e+00 - val_loss: 15.7015 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6721 - acc: 0.0000e+00 - val_loss: 15.6958 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6591 - acc: 0.0000e+00 - val_loss: 15.6896 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6912 - acc: 0.0000e+00 - val_loss: 15.6830 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6355 - acc: 0.0000e+00 - val_loss: 15.6762 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6523 - acc: 0.0000e+00 - val_loss: 15.6700 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6574 - acc: 0.0000e+00 - val_loss: 15.6644 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6335 - acc: 0.0000e+00 - val_loss: 15.6584 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6277 - acc: 0.0000e+00 - val_loss: 15.6526 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6124 - acc: 0.0000e+00 - val_loss: 15.6460 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5822 - acc: 0.0000e+00 - val_loss: 15.6399 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5869 - acc: 0.0000e+00 - val_loss: 15.6329 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5741 - acc: 0.0000e+00 - val_loss: 15.6258 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6052 - acc: 0.0000e+00 - val_loss: 15.6183 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5790 - acc: 0.0000e+00 - val_loss: 15.6110 - val_acc: 0.0000e+00\n",
      "CPU times: user 30.1 s, sys: 5.67 s, total: 35.8 s\n",
      "Wall time: 51.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.226\n",
      "Test accuracy: 0.802\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 5s - loss: 15.1223 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 10.3497 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 5.6256 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 2.8063 - acc: 0.1100     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.9130 - acc: 0.2375     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.7086 - acc: 0.1875     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.7391 - acc: 0.1900     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.7111 - acc: 0.2100     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.7093 - acc: 0.2000     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.7153 - acc: 0.1925     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.6460 - acc: 0.2750     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.6654 - acc: 0.2225     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.6485 - acc: 0.2400     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.6563 - acc: 0.2550     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.6713 - acc: 0.2225     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.6554 - acc: 0.2525     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.6332 - acc: 0.2500     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.5664 - acc: 0.2900     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.6309 - acc: 0.2475     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 1.5934 - acc: 0.2425     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 1.5357 - acc: 0.3025     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 1.5239 - acc: 0.3425     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 1.4729 - acc: 0.3725     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 1.4380 - acc: 0.4075     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 1.3683 - acc: 0.4050     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 1.3000 - acc: 0.4600     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 1.2826 - acc: 0.4425     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 1.2180 - acc: 0.4700     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 1.1349 - acc: 0.5225     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 1.0455 - acc: 0.5525     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.9562 - acc: 0.6275     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.8983 - acc: 0.6225     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.8290 - acc: 0.6500     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.7291 - acc: 0.7200     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.6795 - acc: 0.7425     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.6993 - acc: 0.7125     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.5982 - acc: 0.7925     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.5659 - acc: 0.7975     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.4919 - acc: 0.8275     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.4747 - acc: 0.8075     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.4111 - acc: 0.8475     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.3686 - acc: 0.8800     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.3197 - acc: 0.9000     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.3040 - acc: 0.8925     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.2578 - acc: 0.9175     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.2437 - acc: 0.9275     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.2158 - acc: 0.9175     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.1390 - acc: 0.9700     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.1547 - acc: 0.9550     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.1065 - acc: 0.9825     \n",
      "CPU times: user 16.9 s, sys: 2.29 s, total: 19.2 s\n",
      "Wall time: 24.2 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 4.614\n",
      "Test accuracy: 0.314\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.392\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 6s - loss: 15.8499 - acc: 0.0000e+00 - val_loss: 15.8242 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8659 - acc: 0.0000e+00 - val_loss: 15.8210 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8586 - acc: 0.0000e+00 - val_loss: 15.8171 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8356 - acc: 0.0000e+00 - val_loss: 15.8135 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8731 - acc: 0.0000e+00 - val_loss: 15.8113 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8263 - acc: 0.0000e+00 - val_loss: 15.8093 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8014 - acc: 0.0000e+00 - val_loss: 15.8084 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8139 - acc: 0.0000e+00 - val_loss: 15.8071 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8498 - acc: 0.0000e+00 - val_loss: 15.8061 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8138 - acc: 0.0000e+00 - val_loss: 15.8043 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8160 - acc: 0.0000e+00 - val_loss: 15.8019 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8251 - acc: 0.0000e+00 - val_loss: 15.7995 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7878 - acc: 0.0000e+00 - val_loss: 15.7964 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7980 - acc: 0.0000e+00 - val_loss: 15.7933 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8086 - acc: 0.0000e+00 - val_loss: 15.7909 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8204 - acc: 0.0000e+00 - val_loss: 15.7894 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7952 - acc: 0.0000e+00 - val_loss: 15.7877 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7945 - acc: 0.0000e+00 - val_loss: 15.7855 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7687 - acc: 0.0000e+00 - val_loss: 15.7834 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7679 - acc: 0.0000e+00 - val_loss: 15.7811 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7696 - acc: 0.0000e+00 - val_loss: 15.7771 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7766 - acc: 0.0000e+00 - val_loss: 15.7731 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7772 - acc: 0.0000e+00 - val_loss: 15.7686 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7764 - acc: 0.0000e+00 - val_loss: 15.7643 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7479 - acc: 0.0000e+00 - val_loss: 15.7587 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7400 - acc: 0.0000e+00 - val_loss: 15.7524 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7254 - acc: 0.0000e+00 - val_loss: 15.7471 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7493 - acc: 0.0000e+00 - val_loss: 15.7425 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7051 - acc: 0.0000e+00 - val_loss: 15.7374 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7140 - acc: 0.0000e+00 - val_loss: 15.7325 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7573 - acc: 0.0000e+00 - val_loss: 15.7276 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7162 - acc: 0.0000e+00 - val_loss: 15.7230 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7021 - acc: 0.0000e+00 - val_loss: 15.7176 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6540 - acc: 0.0000e+00 - val_loss: 15.7119 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6924 - acc: 0.0000e+00 - val_loss: 15.7062 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6783 - acc: 0.0000e+00 - val_loss: 15.6998 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6542 - acc: 0.0000e+00 - val_loss: 15.6931 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6488 - acc: 0.0000e+00 - val_loss: 15.6871 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6449 - acc: 0.0000e+00 - val_loss: 15.6805 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6792 - acc: 0.0000e+00 - val_loss: 15.6748 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6377 - acc: 0.0000e+00 - val_loss: 15.6689 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6180 - acc: 0.0000e+00 - val_loss: 15.6630 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6380 - acc: 0.0000e+00 - val_loss: 15.6568 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6270 - acc: 0.0000e+00 - val_loss: 15.6504 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6270 - acc: 0.0000e+00 - val_loss: 15.6441 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5523 - acc: 0.0000e+00 - val_loss: 15.6368 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5798 - acc: 0.0000e+00 - val_loss: 15.6298 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5441 - acc: 0.0000e+00 - val_loss: 15.6223 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5697 - acc: 0.0000e+00 - val_loss: 15.6153 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5651 - acc: 0.0000e+00 - val_loss: 15.6082 - val_acc: 0.0000e+00\n",
      "CPU times: user 30.5 s, sys: 5.54 s, total: 36 s\n",
      "Wall time: 51.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.804\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 5s - loss: 15.1413 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.4957 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.4568 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.7547 - acc: 0.1240     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.9642 - acc: 0.2180     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6960 - acc: 0.2380     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.7658 - acc: 0.1860     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6915 - acc: 0.2100     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.6994 - acc: 0.1940     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.7302 - acc: 0.2000     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.7139 - acc: 0.1980     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.6519 - acc: 0.2540     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.6652 - acc: 0.2320     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.6150 - acc: 0.2340     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.6269 - acc: 0.2520     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.6244 - acc: 0.2800     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.5613 - acc: 0.2980     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.5378 - acc: 0.2980     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.5185 - acc: 0.3120     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.5036 - acc: 0.3120     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 1.4855 - acc: 0.3380     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 1.4069 - acc: 0.3640     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 1.3514 - acc: 0.4120     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 1.3482 - acc: 0.4020     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 1.2888 - acc: 0.4020     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 1.2352 - acc: 0.4500     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 1.1924 - acc: 0.4560     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 1.0895 - acc: 0.5240     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 1.0857 - acc: 0.5340     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.9791 - acc: 0.5660     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.9212 - acc: 0.5820     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.8704 - acc: 0.6260     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.7930 - acc: 0.6660     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.7087 - acc: 0.7080     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.6854 - acc: 0.7120     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.6282 - acc: 0.7360     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.5892 - acc: 0.7320     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.5327 - acc: 0.7680     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.4741 - acc: 0.7920     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.4482 - acc: 0.7900     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.4041 - acc: 0.8240     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.3852 - acc: 0.8360     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.3403 - acc: 0.8500     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.3394 - acc: 0.8480     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.3032 - acc: 0.8640     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.3018 - acc: 0.8640     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.2581 - acc: 0.9060     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.2577 - acc: 0.8820     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.2048 - acc: 0.9240     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.2093 - acc: 0.9220     \n",
      "CPU times: user 18.8 s, sys: 2.85 s, total: 21.7 s\n",
      "Wall time: 28.2 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 4.840\n",
      "Test accuracy: 0.324\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.968\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    print(\"dreaming\")\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_people[:num_train_images], Y_train_people[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium sized -> small without dream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "without dreaming\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 5s - loss: 15.4190 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 11.7872 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 9.6544 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 8.1147 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 4.2993 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 3.2070 - acc: 0.0000e+00\n",
      "CPU times: user 9.09 s, sys: 42.4 ms, total: 9.13 s\n",
      "Wall time: 9.02 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 11.477\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.203\n",
      "Test accuracy: 0.590\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "without dreaming\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 5s - loss: 15.9245 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 15.6466 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 14.5591 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 13.2735 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 11.1496 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 9.3089 - acc: 0.0000e+00\n",
      "CPU times: user 7.97 s, sys: 39.9 ms, total: 8.01 s\n",
      "Wall time: 7.9 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 8.463\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.253\n",
      "Test accuracy: 0.468\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "without dreaming\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 5s - loss: 15.8720 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 15.8469 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 15.2070 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 14.3013 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 12.1260 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 10.4361 - acc: 0.0000e+00\n",
      "CPU times: user 8.16 s, sys: 50.1 ms, total: 8.21 s\n",
      "Wall time: 8.11 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 8.883\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.310\n",
      "Test accuracy: 0.408\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "without dreaming\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 5s - loss: 16.0261 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 15.6013 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 15.0879 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 13.0505 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 11.0882 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 8.8540 - acc: 0.0000e+00\n",
      "CPU times: user 8.38 s, sys: 65.5 ms, total: 8.44 s\n",
      "Wall time: 8.35 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 6.702\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.086\n",
      "Test accuracy: 0.642\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "without dreaming\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 5s - loss: 15.9212 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 15.5867 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 15.0414 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 12.8728 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 10.8309 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 8.0918 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 6.1349 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 4.2315 - acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 3.1106 - acc: 0.1000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 2.3961 - acc: 0.2000\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.9332 - acc: 0.2500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.6686 - acc: 0.3000\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.5023 - acc: 0.4000\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.3808 - acc: 0.5500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.4518 - acc: 0.4000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.3936 - acc: 0.3500\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 1.4885 - acc: 0.5000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 1.4191 - acc: 0.4500\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 1.2692 - acc: 0.5000\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 1.3806 - acc: 0.3000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 1.0816 - acc: 0.7000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 1.0664 - acc: 0.6000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 1.3032 - acc: 0.3500\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 1.0128 - acc: 0.5000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 1.2157 - acc: 0.3500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.9306 - acc: 0.5000\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.9670 - acc: 0.7000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.9479 - acc: 0.5500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.9776 - acc: 0.7000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.7489 - acc: 0.6500\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.8549 - acc: 0.6500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.8023 - acc: 0.7000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.8183 - acc: 0.7000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.6678 - acc: 0.7500\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.7147 - acc: 0.7000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.6943 - acc: 0.7500\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.6267 - acc: 0.7500\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.6399 - acc: 0.8500\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.5287 - acc: 0.9000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.4186 - acc: 0.8000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.5218 - acc: 0.8000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.4099 - acc: 0.8500\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.3216 - acc: 0.8500\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.3416 - acc: 0.9000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.3507 - acc: 0.8500\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.3014 - acc: 0.9500\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.2112 - acc: 0.9500\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.3078 - acc: 0.9000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.2063 - acc: 0.9500\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.2118 - acc: 0.9500\n",
      "CPU times: user 10.2 s, sys: 199 ms, total: 10.4 s\n",
      "Wall time: 10 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 5.454\n",
      "Test accuracy: 0.210\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.378\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "without dreaming\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 6s - loss: 15.9393 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 15.0625 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 12.7970 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 10.0967 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 7.4462 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 5.3076 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 3.6405 - acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 2.6148 - acc: 0.2200\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 2.0895 - acc: 0.2400\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.8398 - acc: 0.1600\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.7040 - acc: 0.2600\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.6789 - acc: 0.2700\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.7186 - acc: 0.2500\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.7246 - acc: 0.2300\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.6509 - acc: 0.2500\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.6595 - acc: 0.2500\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.6217 - acc: 0.2500\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.6610 - acc: 0.2600\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.6821 - acc: 0.2400\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.6429 - acc: 0.2400\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.5843 - acc: 0.3000\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.5445 - acc: 0.2700\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.6068 - acc: 0.2900\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.6106 - acc: 0.2100\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.5660 - acc: 0.2700\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.5781 - acc: 0.3000\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.4927 - acc: 0.3500\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.5272 - acc: 0.2900\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 1.5291 - acc: 0.2900\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 1.4912 - acc: 0.3500\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 1.4200 - acc: 0.3600\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 1.4773 - acc: 0.3500\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 1.4170 - acc: 0.4100\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 1.3840 - acc: 0.4600\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 1.2736 - acc: 0.4800\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 1.2628 - acc: 0.5000\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 1.2880 - acc: 0.4500\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 1.1477 - acc: 0.5900\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 1.1013 - acc: 0.5800\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 1.0674 - acc: 0.5600\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 1.0084 - acc: 0.6100\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.8909 - acc: 0.6600\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.8482 - acc: 0.7300\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.7848 - acc: 0.7200\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.7661 - acc: 0.7000\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.6889 - acc: 0.8000\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.5969 - acc: 0.8200\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.5710 - acc: 0.8700\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.4980 - acc: 0.8300\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.4704 - acc: 0.8200\n",
      "CPU times: user 11.7 s, sys: 573 ms, total: 12.3 s\n",
      "Wall time: 13.1 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 2.781\n",
      "Test accuracy: 0.318\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 12.920\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "without dreaming\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 6s - loss: 15.9016 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 14.8466 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 12.3836 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 9.5141 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 6.8056 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 4.7243 - acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 3.2256 - acc: 0.0050\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 2.3136 - acc: 0.2450\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 1.9993 - acc: 0.2600\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.8091 - acc: 0.2050\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6349 - acc: 0.3000\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.6727 - acc: 0.2050\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.6954 - acc: 0.2050\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.6943 - acc: 0.2400\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.6699 - acc: 0.2050\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.6447 - acc: 0.2150\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.5671 - acc: 0.2950\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.6725 - acc: 0.2200\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.6459 - acc: 0.2900\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.5983 - acc: 0.2850\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.5982 - acc: 0.2550\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.6137 - acc: 0.2700\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.5598 - acc: 0.2950\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.5337 - acc: 0.2850\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 1.5736 - acc: 0.3000\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 1.5171 - acc: 0.2700\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 1.4535 - acc: 0.4150\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 1.4556 - acc: 0.3400\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 1.4120 - acc: 0.4050\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 1.3793 - acc: 0.3950\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 1.4123 - acc: 0.3750\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 1.3047 - acc: 0.4500\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 1.3371 - acc: 0.3700\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 1.2964 - acc: 0.4450\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 1.2366 - acc: 0.4550\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 1.2004 - acc: 0.4550\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 1.1951 - acc: 0.4900\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 1.1703 - acc: 0.5000\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 1.1365 - acc: 0.5300\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 1.0616 - acc: 0.5400\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 1.0812 - acc: 0.5450\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 1.0125 - acc: 0.6100\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 1.0130 - acc: 0.5800\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.9657 - acc: 0.5650\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.9265 - acc: 0.6150\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.8463 - acc: 0.6400\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.8167 - acc: 0.6650\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.8007 - acc: 0.6250\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.7747 - acc: 0.6650\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.7096 - acc: 0.7050\n",
      "CPU times: user 13.5 s, sys: 1.09 s, total: 14.6 s\n",
      "Wall time: 16.9 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 4.677\n",
      "Test accuracy: 0.272\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.089\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "without dreaming\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 6s - loss: 15.7549 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 12.1472 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 6.8454 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 3.3915 - acc: 0.0367     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 2.0705 - acc: 0.2067     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.7208 - acc: 0.2700     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.7142 - acc: 0.2067     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.7611 - acc: 0.2300     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.6648 - acc: 0.2233     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.5553 - acc: 0.3000     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.6445 - acc: 0.2800     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.5149 - acc: 0.3700     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.4632 - acc: 0.3767     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.4464 - acc: 0.3567     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.3891 - acc: 0.3967     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.3554 - acc: 0.4067     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.3404 - acc: 0.3700     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.3213 - acc: 0.3900     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.2699 - acc: 0.4600     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 1.2243 - acc: 0.4633     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 1.1896 - acc: 0.4333     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 1.1522 - acc: 0.4300     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 1.0768 - acc: 0.4967     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 1.0580 - acc: 0.5600     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 1.0905 - acc: 0.5033     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 0.9881 - acc: 0.5967     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 1.0016 - acc: 0.5500     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.9427 - acc: 0.5467     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.9006 - acc: 0.5900     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.8255 - acc: 0.6567     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.8140 - acc: 0.6300     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.7747 - acc: 0.6633     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.6849 - acc: 0.6900     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.7131 - acc: 0.6900     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.6528 - acc: 0.6900     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.6258 - acc: 0.6933     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.6312 - acc: 0.7300     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.6098 - acc: 0.7267     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.5572 - acc: 0.7333     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.5252 - acc: 0.7733     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.5535 - acc: 0.7433     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.5486 - acc: 0.7700     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.4844 - acc: 0.8300     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.4668 - acc: 0.8033     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.4351 - acc: 0.8633     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.3553 - acc: 0.8733     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.3554 - acc: 0.8500     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.3466 - acc: 0.8767     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.2410 - acc: 0.9400     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.2231 - acc: 0.9200     \n",
      "CPU times: user 18.4 s, sys: 1.74 s, total: 20.1 s\n",
      "Wall time: 23.5 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 4.990\n",
      "Test accuracy: 0.338\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.838\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "without dreaming\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 7s - loss: 15.4361 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 11.2461 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 5.9591 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 2.8760 - acc: 0.0775     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.8825 - acc: 0.2500     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.7239 - acc: 0.2425     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.7208 - acc: 0.1925     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.6486 - acc: 0.2550     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.6310 - acc: 0.2700     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.6349 - acc: 0.2900     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.5293 - acc: 0.3300     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.5047 - acc: 0.3125     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.4569 - acc: 0.3350     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.4042 - acc: 0.3650     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 1.3696 - acc: 0.3825     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 1.3799 - acc: 0.3600     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 1.3391 - acc: 0.3950     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 1.2922 - acc: 0.3725     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 1.2599 - acc: 0.4150     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 1.2364 - acc: 0.4150     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 1.1788 - acc: 0.4800     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 1.1650 - acc: 0.4650     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 1.1273 - acc: 0.5025     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 1.1114 - acc: 0.5100     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 1.0426 - acc: 0.5550     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.9894 - acc: 0.5775     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.9780 - acc: 0.5925     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.9073 - acc: 0.6350     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.8056 - acc: 0.6700     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.7676 - acc: 0.6800     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.6921 - acc: 0.7175     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.6559 - acc: 0.7125     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.5665 - acc: 0.7425     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.5333 - acc: 0.7425     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.4457 - acc: 0.7975     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.4456 - acc: 0.7850     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.3992 - acc: 0.8225     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.3506 - acc: 0.8450     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.3099 - acc: 0.8625     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.2737 - acc: 0.8850     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.2505 - acc: 0.9125     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.1906 - acc: 0.9325     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.1808 - acc: 0.9575     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.1767 - acc: 0.9625     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.1637 - acc: 0.9525     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1394 - acc: 0.9575     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.1524 - acc: 0.9575     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.1007 - acc: 0.9800     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.1118 - acc: 0.9625     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.0986 - acc: 0.9675     \n",
      "CPU times: user 18.9 s, sys: 2.17 s, total: 21 s\n",
      "Wall time: 25.8 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 6.669\n",
      "Test accuracy: 0.332\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.947\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "without dreaming\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 7s - loss: 15.1856 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 10.8082 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 5.6094 - acc: 0.0000e+00     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 2.8219 - acc: 0.1320     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.9479 - acc: 0.2500     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6477 - acc: 0.2640     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.6839 - acc: 0.2420     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6307 - acc: 0.2580     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.5712 - acc: 0.3100     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.5433 - acc: 0.3560     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.4593 - acc: 0.3980     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.4196 - acc: 0.3300     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.4014 - acc: 0.3720     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.3666 - acc: 0.3720     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.3186 - acc: 0.3800     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.3197 - acc: 0.3940     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.2910 - acc: 0.4120     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.2475 - acc: 0.4060     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 1.2392 - acc: 0.3920     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 1.1973 - acc: 0.4200     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 1.1664 - acc: 0.4440     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 1.1257 - acc: 0.4880     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 1.1241 - acc: 0.4820     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 1.0504 - acc: 0.5440     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 1.0001 - acc: 0.5320     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.9377 - acc: 0.5780     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.8952 - acc: 0.6120     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.8198 - acc: 0.6520     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.7863 - acc: 0.6460     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.7279 - acc: 0.6660     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.6439 - acc: 0.7320     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.6336 - acc: 0.6840     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.5456 - acc: 0.7280     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.5201 - acc: 0.7400     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.4604 - acc: 0.7400     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.4644 - acc: 0.7540     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.4179 - acc: 0.7840     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.3924 - acc: 0.7880     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.3962 - acc: 0.7800     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.3727 - acc: 0.7800     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.3470 - acc: 0.7940     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.3394 - acc: 0.7960     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.3361 - acc: 0.7940     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.3253 - acc: 0.8240     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.3035 - acc: 0.8320     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.2971 - acc: 0.8640     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.2754 - acc: 0.8700     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.2712 - acc: 0.8700     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.2511 - acc: 0.8940     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.2324 - acc: 0.9000     \n",
      "CPU times: user 20.6 s, sys: 2.75 s, total: 23.4 s\n",
      "Wall time: 29.8 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 4.744\n",
      "Test accuracy: 0.384\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.155\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"without dreaming\")\n",
    "    ## dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # target domain: second day task\n",
    "    %time his = model_whole.fit(X_train_small_mammals[:num_train_images], Y_train_small_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "    # target domain: second day task\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 8s - loss: 15.8551 - acc: 0.0000e+00 - val_loss: 15.8248 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8470 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8501 - acc: 0.0000e+00 - val_loss: 15.8172 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8393 - acc: 0.0000e+00 - val_loss: 15.8134 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8289 - acc: 0.0000e+00 - val_loss: 15.8115 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8358 - acc: 0.0000e+00 - val_loss: 15.8097 - val_acc: 0.0000e+00\n",
      "CPU times: user 12.6 s, sys: 762 ms, total: 13.4 s\n",
      "Wall time: 15.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.310\n",
      "Test accuracy: 0.784\n",
      "1 shot leaning training, on second day task\n",
      "CPU times: user 9.81 s, sys: 71.4 ms, total: 9.88 s\n",
      "Wall time: 9.77 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 10.897\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.069\n",
      "Test accuracy: 0.732\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 9s - loss: 15.8650 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8545 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8576 - acc: 0.0000e+00 - val_loss: 15.8185 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8786 - acc: 0.0000e+00 - val_loss: 15.8155 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8196 - acc: 0.0000e+00 - val_loss: 15.8130 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8506 - acc: 0.0000e+00 - val_loss: 15.8112 - val_acc: 0.0000e+00\n",
      "CPU times: user 12.9 s, sys: 728 ms, total: 13.6 s\n",
      "Wall time: 15.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.310\n",
      "Test accuracy: 0.784\n",
      "5 shot leaning training, on second day task\n",
      "CPU times: user 10.2 s, sys: 65.8 ms, total: 10.3 s\n",
      "Wall time: 10.2 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 10.537\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.657\n",
      "Test accuracy: 0.488\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 9s - loss: 15.8441 - acc: 0.0000e+00 - val_loss: 15.8247 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8543 - acc: 0.0000e+00 - val_loss: 15.8215 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8400 - acc: 0.0000e+00 - val_loss: 15.8178 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8458 - acc: 0.0000e+00 - val_loss: 15.8143 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8058 - acc: 0.0000e+00 - val_loss: 15.8115 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8515 - acc: 0.0000e+00 - val_loss: 15.8094 - val_acc: 0.0000e+00\n",
      "CPU times: user 13.3 s, sys: 724 ms, total: 14 s\n",
      "Wall time: 15.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.305\n",
      "Test accuracy: 0.786\n",
      "10 shot leaning training, on second day task\n",
      "CPU times: user 10.5 s, sys: 74.2 ms, total: 10.5 s\n",
      "Wall time: 10.4 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 7.085\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.154\n",
      "Test accuracy: 0.392\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 9s - loss: 15.8739 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8669 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8749 - acc: 0.0000e+00 - val_loss: 15.8179 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8218 - acc: 0.0000e+00 - val_loss: 15.8145 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8627 - acc: 0.0000e+00 - val_loss: 15.8114 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8299 - acc: 0.0000e+00 - val_loss: 15.8095 - val_acc: 0.0000e+00\n",
      "CPU times: user 13.7 s, sys: 738 ms, total: 14.4 s\n",
      "Wall time: 16.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.307\n",
      "Test accuracy: 0.786\n",
      "15 shot leaning training, on second day task\n",
      "CPU times: user 12.7 s, sys: 89.7 ms, total: 12.8 s\n",
      "Wall time: 12.6 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 5.945\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.986\n",
      "Test accuracy: 0.430\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8539 - acc: 0.0000e+00 - val_loss: 15.8251 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8589 - acc: 0.0000e+00 - val_loss: 15.8219 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8384 - acc: 0.0000e+00 - val_loss: 15.8183 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8372 - acc: 0.0000e+00 - val_loss: 15.8146 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8126 - acc: 0.0000e+00 - val_loss: 15.8120 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8208 - acc: 0.0000e+00 - val_loss: 15.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8379 - acc: 0.0000e+00 - val_loss: 15.8096 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8067 - acc: 0.0000e+00 - val_loss: 15.8085 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8245 - acc: 0.0000e+00 - val_loss: 15.8068 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8347 - acc: 0.0000e+00 - val_loss: 15.8049 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8375 - acc: 0.0000e+00 - val_loss: 15.8046 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8187 - acc: 0.0000e+00 - val_loss: 15.8025 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8204 - acc: 0.0000e+00 - val_loss: 15.7996 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7914 - acc: 0.0000e+00 - val_loss: 15.7967 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7959 - acc: 0.0000e+00 - val_loss: 15.7938 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8052 - acc: 0.0000e+00 - val_loss: 15.7918 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7928 - acc: 0.0000e+00 - val_loss: 15.7898 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7798 - acc: 0.0000e+00 - val_loss: 15.7871 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7609 - acc: 0.0000e+00 - val_loss: 15.7847 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7946 - acc: 0.0000e+00 - val_loss: 15.7831 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7512 - acc: 0.0000e+00 - val_loss: 15.7812 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7857 - acc: 0.0000e+00 - val_loss: 15.7781 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7389 - acc: 0.0000e+00 - val_loss: 15.7744 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7504 - acc: 0.0000e+00 - val_loss: 15.7696 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7621 - acc: 0.0000e+00 - val_loss: 15.7656 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7200 - acc: 0.0000e+00 - val_loss: 15.7591 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7361 - acc: 0.0000e+00 - val_loss: 15.7541 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7191 - acc: 0.0000e+00 - val_loss: 15.7483 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7314 - acc: 0.0000e+00 - val_loss: 15.7429 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7202 - acc: 0.0000e+00 - val_loss: 15.7373 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7169 - acc: 0.0000e+00 - val_loss: 15.7315 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7090 - acc: 0.0000e+00 - val_loss: 15.7256 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6753 - acc: 0.0000e+00 - val_loss: 15.7196 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6629 - acc: 0.0000e+00 - val_loss: 15.7131 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6741 - acc: 0.0000e+00 - val_loss: 15.7070 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6965 - acc: 0.0000e+00 - val_loss: 15.7015 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6889 - acc: 0.0000e+00 - val_loss: 15.6954 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6339 - acc: 0.0000e+00 - val_loss: 15.6886 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6545 - acc: 0.0000e+00 - val_loss: 15.6823 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6661 - acc: 0.0000e+00 - val_loss: 15.6760 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6138 - acc: 0.0000e+00 - val_loss: 15.6696 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6098 - acc: 0.0000e+00 - val_loss: 15.6634 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6078 - acc: 0.0000e+00 - val_loss: 15.6569 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6377 - acc: 0.0000e+00 - val_loss: 15.6505 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5805 - acc: 0.0000e+00 - val_loss: 15.6432 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6063 - acc: 0.0000e+00 - val_loss: 15.6366 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5758 - acc: 0.0000e+00 - val_loss: 15.6295 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5732 - acc: 0.0000e+00 - val_loss: 15.6228 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5216 - acc: 0.0000e+00 - val_loss: 15.6156 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5497 - acc: 0.0000e+00 - val_loss: 15.6080 - val_acc: 0.0000e+00\n",
      "CPU times: user 34.1 s, sys: 5.52 s, total: 39.6 s\n",
      "Wall time: 55.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.802\n",
      "20 shot leaning training, on second day task\n",
      "CPU times: user 12.7 s, sys: 211 ms, total: 12.9 s\n",
      "Wall time: 12.6 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.726\n",
      "Test accuracy: 0.262\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.623\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8663 - acc: 0.0000e+00 - val_loss: 15.8248 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8628 - acc: 0.0000e+00 - val_loss: 15.8218 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8782 - acc: 0.0000e+00 - val_loss: 15.8187 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8515 - acc: 0.0000e+00 - val_loss: 15.8150 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8775 - acc: 0.0000e+00 - val_loss: 15.8126 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8275 - acc: 0.0000e+00 - val_loss: 15.8105 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8305 - acc: 0.0000e+00 - val_loss: 15.8096 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8472 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7797 - acc: 0.0000e+00 - val_loss: 15.8068 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8167 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8308 - acc: 0.0000e+00 - val_loss: 15.8038 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8475 - acc: 0.0000e+00 - val_loss: 15.8015 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7988 - acc: 0.0000e+00 - val_loss: 15.7979 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8098 - acc: 0.0000e+00 - val_loss: 15.7941 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8147 - acc: 0.0000e+00 - val_loss: 15.7913 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8035 - acc: 0.0000e+00 - val_loss: 15.7889 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7819 - acc: 0.0000e+00 - val_loss: 15.7864 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7699 - acc: 0.0000e+00 - val_loss: 15.7844 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8093 - acc: 0.0000e+00 - val_loss: 15.7832 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7912 - acc: 0.0000e+00 - val_loss: 15.7808 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7685 - acc: 0.0000e+00 - val_loss: 15.7775 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7389 - acc: 0.0000e+00 - val_loss: 15.7729 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7554 - acc: 0.0000e+00 - val_loss: 15.7685 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7488 - acc: 0.0000e+00 - val_loss: 15.7644 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7495 - acc: 0.0000e+00 - val_loss: 15.7591 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7463 - acc: 0.0000e+00 - val_loss: 15.7533 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7598 - acc: 0.0000e+00 - val_loss: 15.7485 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7102 - acc: 0.0000e+00 - val_loss: 15.7434 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6905 - acc: 0.0000e+00 - val_loss: 15.7382 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7273 - acc: 0.0000e+00 - val_loss: 15.7327 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7001 - acc: 0.0000e+00 - val_loss: 15.7276 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7159 - acc: 0.0000e+00 - val_loss: 15.7222 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7097 - acc: 0.0000e+00 - val_loss: 15.7165 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6828 - acc: 0.0000e+00 - val_loss: 15.7108 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6537 - acc: 0.0000e+00 - val_loss: 15.7047 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6776 - acc: 0.0000e+00 - val_loss: 15.6989 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6313 - acc: 0.0000e+00 - val_loss: 15.6929 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6410 - acc: 0.0000e+00 - val_loss: 15.6867 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6404 - acc: 0.0000e+00 - val_loss: 15.6804 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6434 - acc: 0.0000e+00 - val_loss: 15.6744 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6431 - acc: 0.0000e+00 - val_loss: 15.6686 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6458 - acc: 0.0000e+00 - val_loss: 15.6619 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6617 - acc: 0.0000e+00 - val_loss: 15.6559 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6297 - acc: 0.0000e+00 - val_loss: 15.6498 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6057 - acc: 0.0000e+00 - val_loss: 15.6434 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5810 - acc: 0.0000e+00 - val_loss: 15.6367 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5793 - acc: 0.0000e+00 - val_loss: 15.6304 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6175 - acc: 0.0000e+00 - val_loss: 15.6245 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5596 - acc: 0.0000e+00 - val_loss: 15.6178 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5680 - acc: 0.0000e+00 - val_loss: 15.6103 - val_acc: 0.0000e+00\n",
      "CPU times: user 34.5 s, sys: 5.7 s, total: 40.2 s\n",
      "Wall time: 56 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.227\n",
      "Test accuracy: 0.802\n",
      "100 shot leaning training, on second day task\n",
      "CPU times: user 14.2 s, sys: 642 ms, total: 14.9 s\n",
      "Wall time: 15.8 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 4.212\n",
      "Test accuracy: 0.204\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.152\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 10s - loss: 15.8516 - acc: 0.0000e+00 - val_loss: 15.8247 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8543 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8612 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8070 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8401 - acc: 0.0000e+00 - val_loss: 15.8113 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8191 - acc: 0.0000e+00 - val_loss: 15.8093 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8312 - acc: 0.0000e+00 - val_loss: 15.8085 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8259 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8344 - acc: 0.0000e+00 - val_loss: 15.8063 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8506 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8332 - acc: 0.0000e+00 - val_loss: 15.8040 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8100 - acc: 0.0000e+00 - val_loss: 15.8015 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8313 - acc: 0.0000e+00 - val_loss: 15.7987 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8077 - acc: 0.0000e+00 - val_loss: 15.7954 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8128 - acc: 0.0000e+00 - val_loss: 15.7925 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7923 - acc: 0.0000e+00 - val_loss: 15.7902 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7988 - acc: 0.0000e+00 - val_loss: 15.7883 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7453 - acc: 0.0000e+00 - val_loss: 15.7857 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7714 - acc: 0.0000e+00 - val_loss: 15.7838 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7731 - acc: 0.0000e+00 - val_loss: 15.7826 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7659 - acc: 0.0000e+00 - val_loss: 15.7803 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7895 - acc: 0.0000e+00 - val_loss: 15.7765 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7602 - acc: 0.0000e+00 - val_loss: 15.7718 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7204 - acc: 0.0000e+00 - val_loss: 15.7663 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7347 - acc: 0.0000e+00 - val_loss: 15.7608 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7563 - acc: 0.0000e+00 - val_loss: 15.7554 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6871 - acc: 0.0000e+00 - val_loss: 15.7491 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7305 - acc: 0.0000e+00 - val_loss: 15.7440 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7335 - acc: 0.0000e+00 - val_loss: 15.7394 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7615 - acc: 0.0000e+00 - val_loss: 15.7346 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7125 - acc: 0.0000e+00 - val_loss: 15.7283 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7024 - acc: 0.0000e+00 - val_loss: 15.7225 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7078 - acc: 0.0000e+00 - val_loss: 15.7178 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7074 - acc: 0.0000e+00 - val_loss: 15.7122 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7156 - acc: 0.0000e+00 - val_loss: 15.7057 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6770 - acc: 0.0000e+00 - val_loss: 15.6997 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6368 - acc: 0.0000e+00 - val_loss: 15.6936 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6818 - acc: 0.0000e+00 - val_loss: 15.6876 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6727 - acc: 0.0000e+00 - val_loss: 15.6820 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6604 - acc: 0.0000e+00 - val_loss: 15.6759 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6730 - acc: 0.0000e+00 - val_loss: 15.6705 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6478 - acc: 0.0000e+00 - val_loss: 15.6647 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6552 - acc: 0.0000e+00 - val_loss: 15.6588 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5976 - acc: 0.0000e+00 - val_loss: 15.6522 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6002 - acc: 0.0000e+00 - val_loss: 15.6457 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5931 - acc: 0.0000e+00 - val_loss: 15.6392 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5945 - acc: 0.0000e+00 - val_loss: 15.6317 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5535 - acc: 0.0000e+00 - val_loss: 15.6248 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5357 - acc: 0.0000e+00 - val_loss: 15.6184 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5461 - acc: 0.0000e+00 - val_loss: 15.6104 - val_acc: 0.0000e+00\n",
      "CPU times: user 35.1 s, sys: 5.57 s, total: 40.7 s\n",
      "Wall time: 56.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.802\n",
      "200 shot leaning training, on second day task\n",
      "CPU times: user 16.3 s, sys: 1.17 s, total: 17.5 s\n",
      "Wall time: 19.9 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.907\n",
      "Test accuracy: 0.334\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.156\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s - loss: 15.8356 - acc: 0.0000e+00 - val_loss: 15.8243 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8225 - acc: 0.0000e+00 - val_loss: 15.8208 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8298 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8436 - acc: 0.0000e+00 - val_loss: 15.8145 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8759 - acc: 0.0000e+00 - val_loss: 15.8125 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8546 - acc: 0.0000e+00 - val_loss: 15.8107 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8359 - acc: 0.0000e+00 - val_loss: 15.8096 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8357 - acc: 0.0000e+00 - val_loss: 15.8085 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8199 - acc: 0.0000e+00 - val_loss: 15.8069 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8038 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7947 - acc: 0.0000e+00 - val_loss: 15.8046 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8171 - acc: 0.0000e+00 - val_loss: 15.8020 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7864 - acc: 0.0000e+00 - val_loss: 15.7989 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7642 - acc: 0.0000e+00 - val_loss: 15.7957 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7805 - acc: 0.0000e+00 - val_loss: 15.7930 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8071 - acc: 0.0000e+00 - val_loss: 15.7914 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8001 - acc: 0.0000e+00 - val_loss: 15.7896 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7809 - acc: 0.0000e+00 - val_loss: 15.7878 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8047 - acc: 0.0000e+00 - val_loss: 15.7856 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7691 - acc: 0.0000e+00 - val_loss: 15.7840 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7393 - acc: 0.0000e+00 - val_loss: 15.7815 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7648 - acc: 0.0000e+00 - val_loss: 15.7780 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7896 - acc: 0.0000e+00 - val_loss: 15.7736 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7627 - acc: 0.0000e+00 - val_loss: 15.7699 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7185 - acc: 0.0000e+00 - val_loss: 15.7657 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7536 - acc: 0.0000e+00 - val_loss: 15.7599 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7138 - acc: 0.0000e+00 - val_loss: 15.7538 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7436 - acc: 0.0000e+00 - val_loss: 15.7482 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7102 - acc: 0.0000e+00 - val_loss: 15.7432 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7134 - acc: 0.0000e+00 - val_loss: 15.7373 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7023 - acc: 0.0000e+00 - val_loss: 15.7318 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7205 - acc: 0.0000e+00 - val_loss: 15.7266 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6926 - acc: 0.0000e+00 - val_loss: 15.7203 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7073 - acc: 0.0000e+00 - val_loss: 15.7139 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6804 - acc: 0.0000e+00 - val_loss: 15.7079 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6810 - acc: 0.0000e+00 - val_loss: 15.7021 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6289 - acc: 0.0000e+00 - val_loss: 15.6955 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6480 - acc: 0.0000e+00 - val_loss: 15.6895 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6684 - acc: 0.0000e+00 - val_loss: 15.6832 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6641 - acc: 0.0000e+00 - val_loss: 15.6771 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6365 - acc: 0.0000e+00 - val_loss: 15.6711 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6344 - acc: 0.0000e+00 - val_loss: 15.6652 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6396 - acc: 0.0000e+00 - val_loss: 15.6592 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6302 - acc: 0.0000e+00 - val_loss: 15.6522 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5989 - acc: 0.0000e+00 - val_loss: 15.6452 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5947 - acc: 0.0000e+00 - val_loss: 15.6390 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6051 - acc: 0.0000e+00 - val_loss: 15.6326 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6180 - acc: 0.0000e+00 - val_loss: 15.6252 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5725 - acc: 0.0000e+00 - val_loss: 15.6177 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5145 - acc: 0.0000e+00 - val_loss: 15.6102 - val_acc: 0.0000e+00\n",
      "CPU times: user 35.6 s, sys: 5.54 s, total: 41.2 s\n",
      "Wall time: 56.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.232\n",
      "Test accuracy: 0.802\n",
      "300 shot leaning training, on second day task\n",
      "CPU times: user 19.9 s, sys: 1.7 s, total: 21.6 s\n",
      "Wall time: 25.1 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 3.018\n",
      "Test accuracy: 0.320\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.164\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s - loss: 15.8556 - acc: 0.0000e+00 - val_loss: 15.8250 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8386 - acc: 0.0000e+00 - val_loss: 15.8221 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8500 - acc: 0.0000e+00 - val_loss: 15.8191 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8404 - acc: 0.0000e+00 - val_loss: 15.8150 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8588 - acc: 0.0000e+00 - val_loss: 15.8131 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8450 - acc: 0.0000e+00 - val_loss: 15.8115 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8043 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8371 - acc: 0.0000e+00 - val_loss: 15.8091 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8262 - acc: 0.0000e+00 - val_loss: 15.8078 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7773 - acc: 0.0000e+00 - val_loss: 15.8062 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8054 - acc: 0.0000e+00 - val_loss: 15.8044 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7960 - acc: 0.0000e+00 - val_loss: 15.8019 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7828 - acc: 0.0000e+00 - val_loss: 15.7977 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8121 - acc: 0.0000e+00 - val_loss: 15.7943 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7997 - acc: 0.0000e+00 - val_loss: 15.7912 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7730 - acc: 0.0000e+00 - val_loss: 15.7888 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7904 - acc: 0.0000e+00 - val_loss: 15.7869 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7649 - acc: 0.0000e+00 - val_loss: 15.7842 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7736 - acc: 0.0000e+00 - val_loss: 15.7819 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7583 - acc: 0.0000e+00 - val_loss: 15.7808 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7452 - acc: 0.0000e+00 - val_loss: 15.7783 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7582 - acc: 0.0000e+00 - val_loss: 15.7745 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7589 - acc: 0.0000e+00 - val_loss: 15.7707 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7544 - acc: 0.0000e+00 - val_loss: 15.7676 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7132 - acc: 0.0000e+00 - val_loss: 15.7627 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7449 - acc: 0.0000e+00 - val_loss: 15.7570 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7332 - acc: 0.0000e+00 - val_loss: 15.7514 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7123 - acc: 0.0000e+00 - val_loss: 15.7456 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7338 - acc: 0.0000e+00 - val_loss: 15.7403 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7060 - acc: 0.0000e+00 - val_loss: 15.7349 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7257 - acc: 0.0000e+00 - val_loss: 15.7296 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7150 - acc: 0.0000e+00 - val_loss: 15.7241 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7251 - acc: 0.0000e+00 - val_loss: 15.7186 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6912 - acc: 0.0000e+00 - val_loss: 15.7130 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6736 - acc: 0.0000e+00 - val_loss: 15.7068 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6925 - acc: 0.0000e+00 - val_loss: 15.7008 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6547 - acc: 0.0000e+00 - val_loss: 15.6942 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6706 - acc: 0.0000e+00 - val_loss: 15.6882 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6296 - acc: 0.0000e+00 - val_loss: 15.6820 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6164 - acc: 0.0000e+00 - val_loss: 15.6751 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6653 - acc: 0.0000e+00 - val_loss: 15.6683 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5988 - acc: 0.0000e+00 - val_loss: 15.6619 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6179 - acc: 0.0000e+00 - val_loss: 15.6551 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6185 - acc: 0.0000e+00 - val_loss: 15.6488 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6287 - acc: 0.0000e+00 - val_loss: 15.6424 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6154 - acc: 0.0000e+00 - val_loss: 15.6356 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6071 - acc: 0.0000e+00 - val_loss: 15.6290 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5714 - acc: 0.0000e+00 - val_loss: 15.6223 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5569 - acc: 0.0000e+00 - val_loss: 15.6152 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5444 - acc: 0.0000e+00 - val_loss: 15.6081 - val_acc: 0.0000e+00\n",
      "CPU times: user 36.1 s, sys: 5.74 s, total: 41.8 s\n",
      "Wall time: 57.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.802\n",
      "400 shot leaning training, on second day task\n",
      "CPU times: user 21.7 s, sys: 2.44 s, total: 24.1 s\n",
      "Wall time: 29.1 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 5.670\n",
      "Test accuracy: 0.358\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.949\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s - loss: 15.8665 - acc: 0.0000e+00 - val_loss: 15.8248 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8550 - acc: 0.0000e+00 - val_loss: 15.8214 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8571 - acc: 0.0000e+00 - val_loss: 15.8180 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8421 - acc: 0.0000e+00 - val_loss: 15.8148 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8322 - acc: 0.0000e+00 - val_loss: 15.8120 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8358 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8442 - acc: 0.0000e+00 - val_loss: 15.8093 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8226 - acc: 0.0000e+00 - val_loss: 15.8083 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8164 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8410 - acc: 0.0000e+00 - val_loss: 15.8058 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8133 - acc: 0.0000e+00 - val_loss: 15.8043 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8165 - acc: 0.0000e+00 - val_loss: 15.8015 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7980 - acc: 0.0000e+00 - val_loss: 15.7985 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7832 - acc: 0.0000e+00 - val_loss: 15.7952 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8044 - acc: 0.0000e+00 - val_loss: 15.7927 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7981 - acc: 0.0000e+00 - val_loss: 15.7911 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7786 - acc: 0.0000e+00 - val_loss: 15.7896 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7896 - acc: 0.0000e+00 - val_loss: 15.7870 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8102 - acc: 0.0000e+00 - val_loss: 15.7857 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7632 - acc: 0.0000e+00 - val_loss: 15.7840 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7726 - acc: 0.0000e+00 - val_loss: 15.7815 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7764 - acc: 0.0000e+00 - val_loss: 15.7770 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7823 - acc: 0.0000e+00 - val_loss: 15.7728 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7517 - acc: 0.0000e+00 - val_loss: 15.7680 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7503 - acc: 0.0000e+00 - val_loss: 15.7626 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7767 - acc: 0.0000e+00 - val_loss: 15.7571 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7480 - acc: 0.0000e+00 - val_loss: 15.7519 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7353 - acc: 0.0000e+00 - val_loss: 15.7462 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7217 - acc: 0.0000e+00 - val_loss: 15.7411 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7179 - acc: 0.0000e+00 - val_loss: 15.7365 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7361 - acc: 0.0000e+00 - val_loss: 15.7321 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7133 - acc: 0.0000e+00 - val_loss: 15.7266 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7119 - acc: 0.0000e+00 - val_loss: 15.7201 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6926 - acc: 0.0000e+00 - val_loss: 15.7139 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7126 - acc: 0.0000e+00 - val_loss: 15.7083 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7336 - acc: 0.0000e+00 - val_loss: 15.7027 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6738 - acc: 0.0000e+00 - val_loss: 15.6966 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6706 - acc: 0.0000e+00 - val_loss: 15.6902 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6584 - acc: 0.0000e+00 - val_loss: 15.6844 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6253 - acc: 0.0000e+00 - val_loss: 15.6780 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6589 - acc: 0.0000e+00 - val_loss: 15.6720 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6119 - acc: 0.0000e+00 - val_loss: 15.6658 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6130 - acc: 0.0000e+00 - val_loss: 15.6595 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6364 - acc: 0.0000e+00 - val_loss: 15.6531 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6144 - acc: 0.0000e+00 - val_loss: 15.6464 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5988 - acc: 0.0000e+00 - val_loss: 15.6402 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5966 - acc: 0.0000e+00 - val_loss: 15.6335 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5705 - acc: 0.0000e+00 - val_loss: 15.6266 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5852 - acc: 0.0000e+00 - val_loss: 15.6195 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5586 - acc: 0.0000e+00 - val_loss: 15.6116 - val_acc: 0.0000e+00\n",
      "CPU times: user 36.2 s, sys: 5.89 s, total: 42.1 s\n",
      "Wall time: 57.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.222\n",
      "Test accuracy: 0.802\n",
      "500 shot leaning training, on second day task\n",
      "CPU times: user 23.8 s, sys: 2.75 s, total: 26.6 s\n",
      "Wall time: 33.1 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 6.615\n",
      "Test accuracy: 0.292\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.026\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"***dreaming\")\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # target domain: second day task\n",
    "    %time his = model_whole.fit(X_train_small_mammals[:num_train_images], Y_train_small_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              verbose=0, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "    # target domain: second day task\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"***dreaming\")\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # target domain: second day task\n",
    "    %time his = model_whole.fit(X_train_small_mammals[:num_train_images], Y_train_small_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              verbose=0, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "    # target domain: second day task\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X_train_carnivores large carnivores\t:bear, leopard, lion, tiger, wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 12s - loss: 15.8390 - acc: 0.0000e+00 - val_loss: 15.8245 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8598 - acc: 0.0000e+00 - val_loss: 15.8212 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8433 - acc: 0.0000e+00 - val_loss: 15.8176 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8550 - acc: 0.0000e+00 - val_loss: 15.8138 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8447 - acc: 0.0000e+00 - val_loss: 15.8105 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8158 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "CPU times: user 16 s, sys: 1.01 s, total: 17 s\n",
      "Wall time: 18.7 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.305\n",
      "Test accuracy: 0.786\n",
      "1 shot leaning training, on second day task\n",
      "CPU times: user 12.9 s, sys: 192 ms, total: 13.1 s\n",
      "Wall time: 12.9 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 13.040\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.665\n",
      "Test accuracy: 0.706\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 12s - loss: 15.8496 - acc: 0.0000e+00 - val_loss: 15.8248 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8481 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8481 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8454 - acc: 0.0000e+00 - val_loss: 15.8142 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8449 - acc: 0.0000e+00 - val_loss: 15.8119 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8245 - acc: 0.0000e+00 - val_loss: 15.8099 - val_acc: 0.0000e+00\n",
      "CPU times: user 16.2 s, sys: 982 ms, total: 17.2 s\n",
      "Wall time: 19 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.307\n",
      "Test accuracy: 0.786\n",
      "5 shot leaning training, on second day task\n",
      "CPU times: user 13.2 s, sys: 266 ms, total: 13.5 s\n",
      "Wall time: 13.3 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 8.552\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.064\n",
      "Test accuracy: 0.258\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 13s - loss: 15.8346 - acc: 0.0000e+00 - val_loss: 15.8244 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8482 - acc: 0.0000e+00 - val_loss: 15.8208 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8568 - acc: 0.0000e+00 - val_loss: 15.8169 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8722 - acc: 0.0000e+00 - val_loss: 15.8138 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8722 - acc: 0.0000e+00 - val_loss: 15.8118 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8571 - acc: 0.0000e+00 - val_loss: 15.8100 - val_acc: 0.0000e+00\n",
      "CPU times: user 16.7 s, sys: 954 ms, total: 17.7 s\n",
      "Wall time: 19.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.306\n",
      "Test accuracy: 0.786\n",
      "10 shot leaning training, on second day task\n",
      "CPU times: user 13.5 s, sys: 225 ms, total: 13.7 s\n",
      "Wall time: 13.6 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 7.836\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.383\n",
      "Test accuracy: 0.366\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 13s - loss: 15.8623 - acc: 0.0000e+00 - val_loss: 15.8249 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8609 - acc: 0.0000e+00 - val_loss: 15.8216 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8446 - acc: 0.0000e+00 - val_loss: 15.8182 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8563 - acc: 0.0000e+00 - val_loss: 15.8149 - val_acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8372 - acc: 0.0000e+00 - val_loss: 15.8119 - val_acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 0s - loss: 15.8226 - acc: 0.0000e+00 - val_loss: 15.8103 - val_acc: 0.0000e+00\n",
      "CPU times: user 17 s, sys: 872 ms, total: 17.9 s\n",
      "Wall time: 19.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.310\n",
      "Test accuracy: 0.784\n",
      "15 shot leaning training, on second day task\n",
      "CPU times: user 13.8 s, sys: 136 ms, total: 13.9 s\n",
      "Wall time: 13.8 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 6.094\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.008\n",
      "Test accuracy: 0.322\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 13s - loss: 15.8275 - acc: 0.0000e+00 - val_loss: 15.8243 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8608 - acc: 0.0000e+00 - val_loss: 15.8207 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8373 - acc: 0.0000e+00 - val_loss: 15.8172 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8160 - acc: 0.0000e+00 - val_loss: 15.8137 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8232 - acc: 0.0000e+00 - val_loss: 15.8116 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8664 - acc: 0.0000e+00 - val_loss: 15.8102 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8263 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8318 - acc: 0.0000e+00 - val_loss: 15.8088 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8164 - acc: 0.0000e+00 - val_loss: 15.8071 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7924 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7900 - acc: 0.0000e+00 - val_loss: 15.8039 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8051 - acc: 0.0000e+00 - val_loss: 15.8011 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7990 - acc: 0.0000e+00 - val_loss: 15.7971 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7873 - acc: 0.0000e+00 - val_loss: 15.7938 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8153 - acc: 0.0000e+00 - val_loss: 15.7909 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7795 - acc: 0.0000e+00 - val_loss: 15.7886 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7946 - acc: 0.0000e+00 - val_loss: 15.7870 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8085 - acc: 0.0000e+00 - val_loss: 15.7848 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7591 - acc: 0.0000e+00 - val_loss: 15.7825 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7885 - acc: 0.0000e+00 - val_loss: 15.7805 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7652 - acc: 0.0000e+00 - val_loss: 15.7788 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7566 - acc: 0.0000e+00 - val_loss: 15.7751 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7379 - acc: 0.0000e+00 - val_loss: 15.7717 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7496 - acc: 0.0000e+00 - val_loss: 15.7667 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7727 - acc: 0.0000e+00 - val_loss: 15.7616 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7594 - acc: 0.0000e+00 - val_loss: 15.7572 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7351 - acc: 0.0000e+00 - val_loss: 15.7513 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7117 - acc: 0.0000e+00 - val_loss: 15.7452 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7191 - acc: 0.0000e+00 - val_loss: 15.7404 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6899 - acc: 0.0000e+00 - val_loss: 15.7344 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7198 - acc: 0.0000e+00 - val_loss: 15.7293 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6911 - acc: 0.0000e+00 - val_loss: 15.7229 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6953 - acc: 0.0000e+00 - val_loss: 15.7164 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6781 - acc: 0.0000e+00 - val_loss: 15.7106 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6987 - acc: 0.0000e+00 - val_loss: 15.7050 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6577 - acc: 0.0000e+00 - val_loss: 15.6991 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6561 - acc: 0.0000e+00 - val_loss: 15.6927 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6314 - acc: 0.0000e+00 - val_loss: 15.6857 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6560 - acc: 0.0000e+00 - val_loss: 15.6794 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6318 - acc: 0.0000e+00 - val_loss: 15.6727 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6522 - acc: 0.0000e+00 - val_loss: 15.6665 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6608 - acc: 0.0000e+00 - val_loss: 15.6607 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6119 - acc: 0.0000e+00 - val_loss: 15.6545 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6306 - acc: 0.0000e+00 - val_loss: 15.6475 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5949 - acc: 0.0000e+00 - val_loss: 15.6408 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5957 - acc: 0.0000e+00 - val_loss: 15.6340 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5989 - acc: 0.0000e+00 - val_loss: 15.6266 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5673 - acc: 0.0000e+00 - val_loss: 15.6185 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5763 - acc: 0.0000e+00 - val_loss: 15.6104 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5242 - acc: 0.0000e+00 - val_loss: 15.6025 - val_acc: 0.0000e+00\n",
      "CPU times: user 37.9 s, sys: 5.74 s, total: 43.7 s\n",
      "Wall time: 59.1 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.802\n",
      "20 shot leaning training, on second day task\n",
      "CPU times: user 15.9 s, sys: 308 ms, total: 16.2 s\n",
      "Wall time: 15.9 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 3.429\n",
      "Test accuracy: 0.284\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.335\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 13s - loss: 15.8566 - acc: 0.0000e+00 - val_loss: 15.8247 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8537 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8468 - acc: 0.0000e+00 - val_loss: 15.8178 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8591 - acc: 0.0000e+00 - val_loss: 15.8141 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7993 - acc: 0.0000e+00 - val_loss: 15.8112 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8427 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8077 - acc: 0.0000e+00 - val_loss: 15.8077 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7870 - acc: 0.0000e+00 - val_loss: 15.8068 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8523 - acc: 0.0000e+00 - val_loss: 15.8052 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8215 - acc: 0.0000e+00 - val_loss: 15.8042 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8142 - acc: 0.0000e+00 - val_loss: 15.8025 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8153 - acc: 0.0000e+00 - val_loss: 15.7995 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8030 - acc: 0.0000e+00 - val_loss: 15.7960 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8208 - acc: 0.0000e+00 - val_loss: 15.7934 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7877 - acc: 0.0000e+00 - val_loss: 15.7907 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7978 - acc: 0.0000e+00 - val_loss: 15.7891 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8040 - acc: 0.0000e+00 - val_loss: 15.7870 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7928 - acc: 0.0000e+00 - val_loss: 15.7852 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7728 - acc: 0.0000e+00 - val_loss: 15.7832 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7823 - acc: 0.0000e+00 - val_loss: 15.7818 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7448 - acc: 0.0000e+00 - val_loss: 15.7782 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7501 - acc: 0.0000e+00 - val_loss: 15.7745 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7289 - acc: 0.0000e+00 - val_loss: 15.7694 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7902 - acc: 0.0000e+00 - val_loss: 15.7656 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7592 - acc: 0.0000e+00 - val_loss: 15.7610 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7568 - acc: 0.0000e+00 - val_loss: 15.7553 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7417 - acc: 0.0000e+00 - val_loss: 15.7497 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7351 - acc: 0.0000e+00 - val_loss: 15.7443 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7602 - acc: 0.0000e+00 - val_loss: 15.7390 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7408 - acc: 0.0000e+00 - val_loss: 15.7336 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7096 - acc: 0.0000e+00 - val_loss: 15.7287 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6947 - acc: 0.0000e+00 - val_loss: 15.7226 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6737 - acc: 0.0000e+00 - val_loss: 15.7163 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7083 - acc: 0.0000e+00 - val_loss: 15.7113 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6380 - acc: 0.0000e+00 - val_loss: 15.7043 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6469 - acc: 0.0000e+00 - val_loss: 15.6981 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6828 - acc: 0.0000e+00 - val_loss: 15.6919 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6670 - acc: 0.0000e+00 - val_loss: 15.6855 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6434 - acc: 0.0000e+00 - val_loss: 15.6792 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6405 - acc: 0.0000e+00 - val_loss: 15.6728 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6735 - acc: 0.0000e+00 - val_loss: 15.6672 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5970 - acc: 0.0000e+00 - val_loss: 15.6608 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5962 - acc: 0.0000e+00 - val_loss: 15.6550 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5594 - acc: 0.0000e+00 - val_loss: 15.6481 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6137 - acc: 0.0000e+00 - val_loss: 15.6420 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6338 - acc: 0.0000e+00 - val_loss: 15.6352 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6005 - acc: 0.0000e+00 - val_loss: 15.6289 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6082 - acc: 0.0000e+00 - val_loss: 15.6214 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5870 - acc: 0.0000e+00 - val_loss: 15.6145 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5520 - acc: 0.0000e+00 - val_loss: 15.6067 - val_acc: 0.0000e+00\n",
      "CPU times: user 38.2 s, sys: 5.84 s, total: 44 s\n",
      "Wall time: 59.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.231\n",
      "Test accuracy: 0.804\n",
      "100 shot leaning training, on second day task\n",
      "CPU times: user 17.5 s, sys: 854 ms, total: 18.4 s\n",
      "Wall time: 19.2 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 3.238\n",
      "Test accuracy: 0.386\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.633\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 14s - loss: 15.8467 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8252 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8292 - acc: 0.0000e+00 - val_loss: 15.8170 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8570 - acc: 0.0000e+00 - val_loss: 15.8135 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8356 - acc: 0.0000e+00 - val_loss: 15.8114 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8718 - acc: 0.0000e+00 - val_loss: 15.8099 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8361 - acc: 0.0000e+00 - val_loss: 15.8092 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8166 - acc: 0.0000e+00 - val_loss: 15.8082 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8122 - acc: 0.0000e+00 - val_loss: 15.8070 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8074 - acc: 0.0000e+00 - val_loss: 15.8059 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8114 - acc: 0.0000e+00 - val_loss: 15.8038 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8294 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7882 - acc: 0.0000e+00 - val_loss: 15.7983 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8055 - acc: 0.0000e+00 - val_loss: 15.7950 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7929 - acc: 0.0000e+00 - val_loss: 15.7921 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8207 - acc: 0.0000e+00 - val_loss: 15.7912 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7873 - acc: 0.0000e+00 - val_loss: 15.7893 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8217 - acc: 0.0000e+00 - val_loss: 15.7871 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7685 - acc: 0.0000e+00 - val_loss: 15.7861 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7454 - acc: 0.0000e+00 - val_loss: 15.7834 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7748 - acc: 0.0000e+00 - val_loss: 15.7797 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7334 - acc: 0.0000e+00 - val_loss: 15.7762 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7577 - acc: 0.0000e+00 - val_loss: 15.7716 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7544 - acc: 0.0000e+00 - val_loss: 15.7670 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7512 - acc: 0.0000e+00 - val_loss: 15.7614 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7696 - acc: 0.0000e+00 - val_loss: 15.7557 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7328 - acc: 0.0000e+00 - val_loss: 15.7502 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7456 - acc: 0.0000e+00 - val_loss: 15.7455 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7243 - acc: 0.0000e+00 - val_loss: 15.7400 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7043 - acc: 0.0000e+00 - val_loss: 15.7347 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7272 - acc: 0.0000e+00 - val_loss: 15.7291 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6896 - acc: 0.0000e+00 - val_loss: 15.7240 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7003 - acc: 0.0000e+00 - val_loss: 15.7181 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7045 - acc: 0.0000e+00 - val_loss: 15.7123 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6434 - acc: 0.0000e+00 - val_loss: 15.7056 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6675 - acc: 0.0000e+00 - val_loss: 15.6987 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6701 - acc: 0.0000e+00 - val_loss: 15.6928 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6693 - acc: 0.0000e+00 - val_loss: 15.6868 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6438 - acc: 0.0000e+00 - val_loss: 15.6804 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6167 - acc: 0.0000e+00 - val_loss: 15.6741 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6669 - acc: 0.0000e+00 - val_loss: 15.6682 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6454 - acc: 0.0000e+00 - val_loss: 15.6622 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6676 - acc: 0.0000e+00 - val_loss: 15.6573 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6075 - acc: 0.0000e+00 - val_loss: 15.6508 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6063 - acc: 0.0000e+00 - val_loss: 15.6449 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5898 - acc: 0.0000e+00 - val_loss: 15.6379 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5832 - acc: 0.0000e+00 - val_loss: 15.6314 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5554 - acc: 0.0000e+00 - val_loss: 15.6247 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5695 - acc: 0.0000e+00 - val_loss: 15.6170 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5816 - acc: 0.0000e+00 - val_loss: 15.6101 - val_acc: 0.0000e+00\n",
      "CPU times: user 38.9 s, sys: 5.87 s, total: 44.7 s\n",
      "Wall time: 1min\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.230\n",
      "Test accuracy: 0.802\n",
      "200 shot leaning training, on second day task\n",
      "CPU times: user 19.5 s, sys: 1.26 s, total: 20.8 s\n",
      "Wall time: 23.2 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 4.058\n",
      "Test accuracy: 0.370\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.914\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 14s - loss: 15.8350 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8408 - acc: 0.0000e+00 - val_loss: 15.8213 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8503 - acc: 0.0000e+00 - val_loss: 15.8176 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8531 - acc: 0.0000e+00 - val_loss: 15.8155 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8487 - acc: 0.0000e+00 - val_loss: 15.8132 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8340 - acc: 0.0000e+00 - val_loss: 15.8110 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8212 - acc: 0.0000e+00 - val_loss: 15.8096 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8138 - acc: 0.0000e+00 - val_loss: 15.8089 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8403 - acc: 0.0000e+00 - val_loss: 15.8074 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8181 - acc: 0.0000e+00 - val_loss: 15.8060 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7914 - acc: 0.0000e+00 - val_loss: 15.8039 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8199 - acc: 0.0000e+00 - val_loss: 15.8011 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8228 - acc: 0.0000e+00 - val_loss: 15.7974 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7947 - acc: 0.0000e+00 - val_loss: 15.7942 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7882 - acc: 0.0000e+00 - val_loss: 15.7913 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7923 - acc: 0.0000e+00 - val_loss: 15.7899 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8041 - acc: 0.0000e+00 - val_loss: 15.7882 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7544 - acc: 0.0000e+00 - val_loss: 15.7859 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7416 - acc: 0.0000e+00 - val_loss: 15.7839 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7889 - acc: 0.0000e+00 - val_loss: 15.7825 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7884 - acc: 0.0000e+00 - val_loss: 15.7805 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7482 - acc: 0.0000e+00 - val_loss: 15.7770 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7492 - acc: 0.0000e+00 - val_loss: 15.7721 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7548 - acc: 0.0000e+00 - val_loss: 15.7675 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7559 - acc: 0.0000e+00 - val_loss: 15.7627 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7178 - acc: 0.0000e+00 - val_loss: 15.7580 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7514 - acc: 0.0000e+00 - val_loss: 15.7517 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7202 - acc: 0.0000e+00 - val_loss: 15.7464 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7255 - acc: 0.0000e+00 - val_loss: 15.7414 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7180 - acc: 0.0000e+00 - val_loss: 15.7360 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7177 - acc: 0.0000e+00 - val_loss: 15.7303 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7321 - acc: 0.0000e+00 - val_loss: 15.7242 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6617 - acc: 0.0000e+00 - val_loss: 15.7188 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6868 - acc: 0.0000e+00 - val_loss: 15.7133 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6821 - acc: 0.0000e+00 - val_loss: 15.7069 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6734 - acc: 0.0000e+00 - val_loss: 15.7007 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6387 - acc: 0.0000e+00 - val_loss: 15.6941 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6233 - acc: 0.0000e+00 - val_loss: 15.6876 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6536 - acc: 0.0000e+00 - val_loss: 15.6816 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6676 - acc: 0.0000e+00 - val_loss: 15.6755 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6042 - acc: 0.0000e+00 - val_loss: 15.6689 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6447 - acc: 0.0000e+00 - val_loss: 15.6627 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5947 - acc: 0.0000e+00 - val_loss: 15.6555 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6273 - acc: 0.0000e+00 - val_loss: 15.6492 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6006 - acc: 0.0000e+00 - val_loss: 15.6425 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6515 - acc: 0.0000e+00 - val_loss: 15.6364 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5712 - acc: 0.0000e+00 - val_loss: 15.6295 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6088 - acc: 0.0000e+00 - val_loss: 15.6228 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5546 - acc: 0.0000e+00 - val_loss: 15.6161 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5106 - acc: 0.0000e+00 - val_loss: 15.6085 - val_acc: 0.0000e+00\n",
      "CPU times: user 39.2 s, sys: 5.69 s, total: 44.9 s\n",
      "Wall time: 1min\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.226\n",
      "Test accuracy: 0.802\n",
      "300 shot leaning training, on second day task\n",
      "CPU times: user 23.2 s, sys: 1.74 s, total: 24.9 s\n",
      "Wall time: 28.4 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 1.957\n",
      "Test accuracy: 0.642\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.500\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 14s - loss: 15.8490 - acc: 0.0000e+00 - val_loss: 15.8241 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8620 - acc: 0.0000e+00 - val_loss: 15.8209 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8482 - acc: 0.0000e+00 - val_loss: 15.8177 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8687 - acc: 0.0000e+00 - val_loss: 15.8143 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8423 - acc: 0.0000e+00 - val_loss: 15.8114 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8290 - acc: 0.0000e+00 - val_loss: 15.8098 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8170 - acc: 0.0000e+00 - val_loss: 15.8090 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8262 - acc: 0.0000e+00 - val_loss: 15.8076 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8242 - acc: 0.0000e+00 - val_loss: 15.8060 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7990 - acc: 0.0000e+00 - val_loss: 15.8048 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8347 - acc: 0.0000e+00 - val_loss: 15.8035 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7934 - acc: 0.0000e+00 - val_loss: 15.8002 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7944 - acc: 0.0000e+00 - val_loss: 15.7968 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8170 - acc: 0.0000e+00 - val_loss: 15.7938 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8141 - acc: 0.0000e+00 - val_loss: 15.7906 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7978 - acc: 0.0000e+00 - val_loss: 15.7890 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7980 - acc: 0.0000e+00 - val_loss: 15.7873 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7577 - acc: 0.0000e+00 - val_loss: 15.7849 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7830 - acc: 0.0000e+00 - val_loss: 15.7836 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7591 - acc: 0.0000e+00 - val_loss: 15.7820 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7721 - acc: 0.0000e+00 - val_loss: 15.7786 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7363 - acc: 0.0000e+00 - val_loss: 15.7756 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7339 - acc: 0.0000e+00 - val_loss: 15.7705 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7467 - acc: 0.0000e+00 - val_loss: 15.7661 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7528 - acc: 0.0000e+00 - val_loss: 15.7606 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7696 - acc: 0.0000e+00 - val_loss: 15.7555 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7594 - acc: 0.0000e+00 - val_loss: 15.7499 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7285 - acc: 0.0000e+00 - val_loss: 15.7449 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7034 - acc: 0.0000e+00 - val_loss: 15.7396 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7036 - acc: 0.0000e+00 - val_loss: 15.7335 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7170 - acc: 0.0000e+00 - val_loss: 15.7274 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7070 - acc: 0.0000e+00 - val_loss: 15.7220 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7461 - acc: 0.0000e+00 - val_loss: 15.7170 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7172 - acc: 0.0000e+00 - val_loss: 15.7109 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7088 - acc: 0.0000e+00 - val_loss: 15.7054 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6641 - acc: 0.0000e+00 - val_loss: 15.6993 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6676 - acc: 0.0000e+00 - val_loss: 15.6935 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6675 - acc: 0.0000e+00 - val_loss: 15.6872 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6540 - acc: 0.0000e+00 - val_loss: 15.6817 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6534 - acc: 0.0000e+00 - val_loss: 15.6759 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6395 - acc: 0.0000e+00 - val_loss: 15.6702 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6044 - acc: 0.0000e+00 - val_loss: 15.6646 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5725 - acc: 0.0000e+00 - val_loss: 15.6580 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6127 - acc: 0.0000e+00 - val_loss: 15.6515 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5857 - acc: 0.0000e+00 - val_loss: 15.6451 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5917 - acc: 0.0000e+00 - val_loss: 15.6385 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5693 - acc: 0.0000e+00 - val_loss: 15.6315 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5541 - acc: 0.0000e+00 - val_loss: 15.6247 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5658 - acc: 0.0000e+00 - val_loss: 15.6178 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5693 - acc: 0.0000e+00 - val_loss: 15.6102 - val_acc: 0.0000e+00\n",
      "CPU times: user 42.6 s, sys: 5.75 s, total: 48.3 s\n",
      "Wall time: 1min 3s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.228\n",
      "Test accuracy: 0.800\n",
      "400 shot leaning training, on second day task\n",
      "CPU times: user 25 s, sys: 2.4 s, total: 27.4 s\n",
      "Wall time: 32.4 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 2.603\n",
      "Test accuracy: 0.650\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.045\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "***dreaming\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 15s - loss: 15.8649 - acc: 0.0000e+00 - val_loss: 15.8246 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8537 - acc: 0.0000e+00 - val_loss: 15.8215 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8652 - acc: 0.0000e+00 - val_loss: 15.8185 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8268 - acc: 0.0000e+00 - val_loss: 15.8150 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8291 - acc: 0.0000e+00 - val_loss: 15.8130 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8092 - acc: 0.0000e+00 - val_loss: 15.8107 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8216 - acc: 0.0000e+00 - val_loss: 15.8097 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8507 - acc: 0.0000e+00 - val_loss: 15.8094 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8131 - acc: 0.0000e+00 - val_loss: 15.8075 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8233 - acc: 0.0000e+00 - val_loss: 15.8055 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8114 - acc: 0.0000e+00 - val_loss: 15.8038 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.8256 - acc: 0.0000e+00 - val_loss: 15.8014 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7873 - acc: 0.0000e+00 - val_loss: 15.7979 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7958 - acc: 0.0000e+00 - val_loss: 15.7952 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7819 - acc: 0.0000e+00 - val_loss: 15.7925 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7803 - acc: 0.0000e+00 - val_loss: 15.7906 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7713 - acc: 0.0000e+00 - val_loss: 15.7886 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7825 - acc: 0.0000e+00 - val_loss: 15.7861 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7698 - acc: 0.0000e+00 - val_loss: 15.7844 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7631 - acc: 0.0000e+00 - val_loss: 15.7829 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7824 - acc: 0.0000e+00 - val_loss: 15.7801 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7818 - acc: 0.0000e+00 - val_loss: 15.7767 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7497 - acc: 0.0000e+00 - val_loss: 15.7723 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7306 - acc: 0.0000e+00 - val_loss: 15.7671 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7240 - acc: 0.0000e+00 - val_loss: 15.7623 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7427 - acc: 0.0000e+00 - val_loss: 15.7556 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7698 - acc: 0.0000e+00 - val_loss: 15.7502 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7235 - acc: 0.0000e+00 - val_loss: 15.7454 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7293 - acc: 0.0000e+00 - val_loss: 15.7400 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6828 - acc: 0.0000e+00 - val_loss: 15.7340 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7000 - acc: 0.0000e+00 - val_loss: 15.7280 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6902 - acc: 0.0000e+00 - val_loss: 15.7225 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6776 - acc: 0.0000e+00 - val_loss: 15.7165 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.7273 - acc: 0.0000e+00 - val_loss: 15.7109 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6854 - acc: 0.0000e+00 - val_loss: 15.7056 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6982 - acc: 0.0000e+00 - val_loss: 15.6995 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6738 - acc: 0.0000e+00 - val_loss: 15.6925 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6243 - acc: 0.0000e+00 - val_loss: 15.6861 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6591 - acc: 0.0000e+00 - val_loss: 15.6799 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6544 - acc: 0.0000e+00 - val_loss: 15.6734 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6257 - acc: 0.0000e+00 - val_loss: 15.6675 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6086 - acc: 0.0000e+00 - val_loss: 15.6615 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6271 - acc: 0.0000e+00 - val_loss: 15.6556 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6133 - acc: 0.0000e+00 - val_loss: 15.6493 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5780 - acc: 0.0000e+00 - val_loss: 15.6423 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.6011 - acc: 0.0000e+00 - val_loss: 15.6352 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5753 - acc: 0.0000e+00 - val_loss: 15.6283 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5828 - acc: 0.0000e+00 - val_loss: 15.6216 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5914 - acc: 0.0000e+00 - val_loss: 15.6138 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 15.5613 - acc: 0.0000e+00 - val_loss: 15.6069 - val_acc: 0.0000e+00\n",
      "CPU times: user 40.2 s, sys: 5.87 s, total: 46.1 s\n",
      "Wall time: 1min 1s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.229\n",
      "Test accuracy: 0.804\n",
      "500 shot leaning training, on second day task\n",
      "CPU times: user 27.2 s, sys: 2.96 s, total: 30.2 s\n",
      "Wall time: 36.7 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 3.866\n",
      "Test accuracy: 0.546\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.079\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"***dreaming\")\n",
    "    dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # target domain: second day task\n",
    "    %time his = model_whole.fit(X_train_carnivores[:num_train_images], Y_train_carnivores[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              verbose=0, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "    # target domain: second day task\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_carnivores, Y_test_carnivores, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.375\n",
      "Test accuracy: 0.770\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "without dreaming\n",
      "1 shot leaning training, on second day task\n",
      "CPU times: user 16 s, sys: 244 ms, total: 16.2 s\n",
      "Wall time: 16.1 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 11.960\n",
      "Test accuracy: 0.000\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.210\n",
      "Test accuracy: 0.596\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "without dreaming\n",
      "5 shot leaning training, on second day task\n",
      "CPU times: user 16.5 s, sys: 259 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 8.566\n",
      "Test accuracy: 0.000\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.258\n",
      "Test accuracy: 0.392\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "without dreaming\n",
      "10 shot leaning training, on second day task\n",
      "CPU times: user 16.7 s, sys: 266 ms, total: 17 s\n",
      "Wall time: 17.3 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 8.027\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.931\n",
      "Test accuracy: 0.484\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "without dreaming\n",
      "15 shot leaning training, on second day task\n",
      "CPU times: user 16.8 s, sys: 204 ms, total: 17 s\n",
      "Wall time: 16.9 s\n",
      "15 shot leaning, test on B task\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('medium_sized_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"medium_sized_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### WITHOUT dreaming\n",
    "    print(\"without dreaming\")\n",
    "    # dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # target domain: second day task\n",
    "    %time his = model_whole.fit(X_train_carnivores[:num_train_images], Y_train_carnivores[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              verbose=0, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "    # target domain: second day task\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_carnivores, Y_test_carnivores, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No dreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 17s    \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 17s - loss: 12.5218 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 9.6412 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 7.2950 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 5.1484 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 1.7238 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 0.6016 - acc: 1.0000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20.4 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 15s    \n",
      "\n",
      "Test loss: 8.767\n",
      "Test accuracy: 0.200\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.468\n",
      "Test accuracy: 0.012\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 18s - loss: 14.1688 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 12.3312 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 9.8215 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 6.6359 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 4.1361 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 2.5581 - acc: 0.4000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20.6 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 15s    \n",
      "\n",
      "Test loss: 5.578\n",
      "Test accuracy: 0.200\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.177\n",
      "Test accuracy: 0.000\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 18s - loss: 13.2973 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 11.8821 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 8.0829 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 5.1099 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 2.9259 - acc: 0.4000\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 2.3724 - acc: 0.6000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20.7 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 15s    \n",
      "\n",
      "Test loss: 5.462\n",
      "Test accuracy: 0.200\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 4.303\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 18s - loss: 14.1789 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 11.6287 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 8.3540 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 5.8772 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 3.5815 - acc: 0.0667\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 2.5541 - acc: 0.4667\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 21.5 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 15s    \n",
      "\n",
      "Test loss: 3.755\n",
      "Test accuracy: 0.200\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.758\n",
      "Test accuracy: 0.000\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 19s - loss: 14.4493 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 12.2650 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 8.7605 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 5.6904 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 3.5301 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 2.4859 - acc: 0.3500\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 2.0670 - acc: 0.3500\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 1.8743 - acc: 0.4000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 1.6834 - acc: 0.3500\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 1.4333 - acc: 0.5500\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.3603 - acc: 0.4500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.3268 - acc: 0.4500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.1972 - acc: 0.5500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.1541 - acc: 0.5500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.0460 - acc: 0.6000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 0.9151 - acc: 0.7000\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 0.7774 - acc: 0.6500\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 0.6955 - acc: 0.7500\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 0.6717 - acc: 0.7500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 0.4999 - acc: 0.8500\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 0.4595 - acc: 0.8500\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 0.4458 - acc: 0.9000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 0.4835 - acc: 0.8500\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 0.4134 - acc: 0.9000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 0.2925 - acc: 0.9000\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.3156 - acc: 0.9000\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.2628 - acc: 0.9000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.3243 - acc: 0.8000\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.1791 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.1853 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.1757 - acc: 0.9500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.1781 - acc: 0.9500\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.1393 - acc: 0.9500\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.1428 - acc: 0.9500\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.1795 - acc: 0.9500\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.0723 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.0749 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.0569 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.0736 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.0240 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.0066 - acc: 1.0000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 23.8 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 15s    \n",
      "\n",
      "Test loss: 5.626\n",
      "Test accuracy: 0.240\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.553\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,8,8,256]\n\t [[Node: moments_794/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](add_13631, moments_794/StopGradient)]]\n\t [[Node: moments_798/sufficient_statistics/Gather/_54683 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3953_moments_798/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'moments_794/sufficient_statistics/SquaredDifference', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-39-05e43c9edd55>\", line 46, in <module>\n    model_whole = model_from_json(loaded_model_json)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 210, in model_from_json\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/utils/layer_utils.py\", line 38, in layer_from_config\n    return layer_class.from_config(config['config'], custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 2575, in from_config\n    process_layer(layer_data)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 2570, in process_layer\n    layer(input_tensors[0])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 569, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 632, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 164, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/layers/normalization.py\", line 117, in call\n    epsilon=self.epsilon)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1171, in normalize_batch_in_training\n    shift=None, name=None, keep_dims=False)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 617, in moments\n    y, axes, shift=shift, keep_dims=keep_dims, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 535, in sufficient_statistics\n    v_ss = math_ops.squared_difference(x, shift)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2754, in squared_difference\n    result = _op_def_lib.apply_op(\"SquaredDifference\", x=x, y=y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,8,8,256]\n\t [[Node: moments_794/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](add_13631, moments_794/StopGradient)]]\n\t [[Node: moments_798/sufficient_statistics/Gather/_54683 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3953_moments_798/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,8,8,256]\n\t [[Node: moments_794/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](add_13631, moments_794/StopGradient)]]\n\t [[Node: moments_798/sufficient_statistics/Gather/_54683 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3953_moments_798/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-05e43c9edd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel_whole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,8,8,256]\n\t [[Node: moments_794/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](add_13631, moments_794/StopGradient)]]\n\t [[Node: moments_798/sufficient_statistics/Gather/_54683 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3953_moments_798/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'moments_794/sufficient_statistics/SquaredDifference', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-39-05e43c9edd55>\", line 46, in <module>\n    model_whole = model_from_json(loaded_model_json)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 210, in model_from_json\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/utils/layer_utils.py\", line 38, in layer_from_config\n    return layer_class.from_config(config['config'], custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 2575, in from_config\n    process_layer(layer_data)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 2570, in process_layer\n    layer(input_tensors[0])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 569, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 632, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 164, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/layers/normalization.py\", line 117, in call\n    epsilon=self.epsilon)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1171, in normalize_batch_in_training\n    shift=None, name=None, keep_dims=False)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 617, in moments\n    y, axes, shift=shift, keep_dims=keep_dims, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 535, in sufficient_statistics\n    v_ss = math_ops.squared_difference(x, shift)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2754, in squared_difference\n    result = _op_def_lib.apply_op(\"SquaredDifference\", x=x, y=y, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,8,8,256]\n\t [[Node: moments_794/sufficient_statistics/SquaredDifference = SquaredDifference[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](add_13631, moments_794/StopGradient)]]\n\t [[Node: moments_798/sufficient_statistics/Gather/_54683 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3953_moments_798/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### No dreaming\n",
    "    # dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 18s    \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,16,16,128]\n\t [[Node: gradients_112/zeros_32 = Fill[T=DT_FLOAT, _class=[\"loc:@Relu_974\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_112/Shape_33, gradients_112/zeros_32/Const)]]\n\t [[Node: moments_817/sufficient_statistics/Gather/_55515 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1416_moments_817/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'gradients_112/zeros_32', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-40-70a2164a6641>\", line 58, in <module>\n    get_ipython().magic('time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2158, in magic\n    return self.run_line_magic(magic_name, magic_arg_s)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2079, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1180, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 1, in <module>\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 1115, in fit\n    self._make_train_function()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 713, in _make_train_function\n    self.total_loss)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 375, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 61, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1628, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 474, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1307, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1355, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in fill\n    result = _op_def_lib.apply_op(\"Fill\", dims=dims, value=value, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,16,16,128]\n\t [[Node: gradients_112/zeros_32 = Fill[T=DT_FLOAT, _class=[\"loc:@Relu_974\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_112/Shape_33, gradients_112/zeros_32/Const)]]\n\t [[Node: moments_817/sufficient_statistics/Gather/_55515 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1416_moments_817/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,16,16,128]\n\t [[Node: gradients_112/zeros_32 = Fill[T=DT_FLOAT, _class=[\"loc:@Relu_974\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_112/Shape_33, gradients_112/zeros_32/Const)]]\n\t [[Node: moments_817/sufficient_statistics/Gather/_55515 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1416_moments_817/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-70a2164a6641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel_whole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,16,16,128]\n\t [[Node: gradients_112/zeros_32 = Fill[T=DT_FLOAT, _class=[\"loc:@Relu_974\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_112/Shape_33, gradients_112/zeros_32/Const)]]\n\t [[Node: moments_817/sufficient_statistics/Gather/_55515 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1416_moments_817/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'gradients_112/zeros_32', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-40-70a2164a6641>\", line 58, in <module>\n    get_ipython().magic('time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images],               batch_size=batch_size,               nb_epoch=nb_epoch,               shuffle=True)')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2158, in magic\n    return self.run_line_magic(magic_name, magic_arg_s)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2079, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-59>\", line 2, in time\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\", line 188, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\", line 1180, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 1, in <module>\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 1115, in fit\n    self._make_train_function()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\", line 713, in _make_train_function\n    self.total_loss)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 375, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\", line 61, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1628, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 474, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1307, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1355, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in fill\n    result = _op_def_lib.apply_op(\"Fill\", dims=dims, value=value, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,16,16,128]\n\t [[Node: gradients_112/zeros_32 = Fill[T=DT_FLOAT, _class=[\"loc:@Relu_974\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients_112/Shape_33, gradients_112/zeros_32/Const)]]\n\t [[Node: moments_817/sufficient_statistics/Gather/_55515 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1416_moments_817/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### No dreaming\n",
    "    # dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 11s - loss: 13.8667 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s - loss: 9.7906 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s - loss: 7.2969 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s - loss: 3.8749 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s - loss: 2.0922 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s - loss: 0.3734 - acc: 1.0000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.1 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 9.033\n",
      "Test accuracy: 0.188\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.464\n",
      "Test accuracy: 0.066\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 11s - loss: 14.5085 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s - loss: 11.9143 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s - loss: 9.3662 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s - loss: 5.8850 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s - loss: 3.8874 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s - loss: 2.3160 - acc: 0.6000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.3 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 5.569\n",
      "Test accuracy: 0.196\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.268\n",
      "Test accuracy: 0.004\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "10/10 [==============================] - 11s - loss: 13.9607 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "10/10 [==============================] - 0s - loss: 10.8887 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "10/10 [==============================] - 0s - loss: 7.6675 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "10/10 [==============================] - 0s - loss: 5.1149 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "10/10 [==============================] - 0s - loss: 2.9921 - acc: 0.5000\n",
      "Epoch 6/6\n",
      "10/10 [==============================] - 0s - loss: 2.2564 - acc: 0.6000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.5 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 5.176\n",
      "Test accuracy: 0.200\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 4.112\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "15/15 [==============================] - 12s - loss: 13.7649 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 0s - loss: 11.3028 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 0s - loss: 8.5824 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 0s - loss: 5.1660 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 0s - loss: 3.4794 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 0s - loss: 2.3224 - acc: 0.4667\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.8 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 4.201\n",
      "Test accuracy: 0.200\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.911\n",
      "Test accuracy: 0.000\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "20/20 [==============================] - 12s - loss: 14.1354 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "20/20 [==============================] - 0s - loss: 12.1443 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "20/20 [==============================] - 0s - loss: 8.5568 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "20/20 [==============================] - 0s - loss: 5.6538 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "20/20 [==============================] - 0s - loss: 3.5813 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "20/20 [==============================] - 0s - loss: 2.5529 - acc: 0.3500\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 3.564\n",
      "Test accuracy: 0.200\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.593\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "100/100 [==============================] - 12s - loss: 14.3369 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "100/100 [==============================] - 0s - loss: 11.2155 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "100/100 [==============================] - 0s - loss: 7.5136 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "100/100 [==============================] - 0s - loss: 4.7108 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "100/100 [==============================] - 0s - loss: 3.0532 - acc: 0.0000e+00\n",
      "Epoch 6/6\n",
      "100/100 [==============================] - 0s - loss: 2.3129 - acc: 0.2400\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15.6 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 2.019\n",
      "Test accuracy: 0.228\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.584\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 12s - loss: 14.3992 - acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 0s - loss: 10.7796 - acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 0s - loss: 7.0244 - acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 0s - loss: 4.3849 - acc: 0.0000e+00\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 0s - loss: 2.9157 - acc: 0.0050\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 0s - loss: 2.2030 - acc: 0.2650\n",
      "CPU times: user 2min 39s, sys: 0 ns, total: 2min 39s\n",
      "Wall time: 16.2 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 1.949\n",
      "Test accuracy: 0.200\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.625\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "300/300 [==============================] - 13s - loss: 13.6869 - acc: 0.0000e+00    \n",
      "Epoch 2/6\n",
      "300/300 [==============================] - 0s - loss: 6.9618 - acc: 0.0000e+00     \n",
      "Epoch 3/6\n",
      "300/300 [==============================] - 0s - loss: 2.9519 - acc: 0.0333     \n",
      "Epoch 4/6\n",
      "300/300 [==============================] - 0s - loss: 1.9609 - acc: 0.2167     \n",
      "Epoch 5/6\n",
      "300/300 [==============================] - 0s - loss: 1.7135 - acc: 0.2467     \n",
      "Epoch 6/6\n",
      "300/300 [==============================] - 0s - loss: 1.6072 - acc: 0.3167     \n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 1.621\n",
      "Test accuracy: 0.200\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 7.408\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/6\n",
      "400/400 [==============================] - 13s - loss: 13.0236 - acc: 0.0000e+00    \n",
      "Epoch 2/6\n",
      "400/400 [==============================] - 0s - loss: 6.0899 - acc: 0.0000e+00     \n",
      "Epoch 3/6\n",
      "400/400 [==============================] - 0s - loss: 2.6858 - acc: 0.0625     \n",
      "Epoch 4/6\n",
      "400/400 [==============================] - 0s - loss: 1.8806 - acc: 0.2175     \n",
      "Epoch 5/6\n",
      "400/400 [==============================] - 0s - loss: 1.6744 - acc: 0.2500     \n",
      "Epoch 6/6\n",
      "400/400 [==============================] - 0s - loss: 1.6505 - acc: 0.2225     \n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17.6 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 1.607\n",
      "Test accuracy: 0.200\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 7.838\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 13s - loss: 12.5631 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 5.6685 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 2.5187 - acc: 0.1480     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 1.7924 - acc: 0.2660     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.6552 - acc: 0.2640     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6097 - acc: 0.2740     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.5580 - acc: 0.3140     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.5077 - acc: 0.3560     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.4316 - acc: 0.4080     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.3283 - acc: 0.4780     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.2308 - acc: 0.5160     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.1662 - acc: 0.5180     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 0.9868 - acc: 0.6140     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 0.8753 - acc: 0.6500     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 0.8016 - acc: 0.6820     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 0.6593 - acc: 0.7520     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 0.5646 - acc: 0.7960     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 0.4986 - acc: 0.8440     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 0.3857 - acc: 0.8720     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 0.3038 - acc: 0.9160     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.2164 - acc: 0.9420     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.1660 - acc: 0.9580     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.1174 - acc: 0.9700     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.0926 - acc: 0.9700     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.0596 - acc: 0.9820     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.0506 - acc: 0.9800     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.0436 - acc: 0.9860     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.0500 - acc: 0.9860     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.0532 - acc: 0.9820     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.0643 - acc: 0.9780     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.0535 - acc: 0.9800     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.0566 - acc: 0.9720     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.0406 - acc: 0.9900     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.0528 - acc: 0.9740     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.0332 - acc: 0.9840     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.0753 - acc: 0.9760     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.0297 - acc: 0.9940     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.0259 - acc: 0.9840     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.0276 - acc: 0.9940     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.0259 - acc: 0.9860     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0345 - acc: 0.9840     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0146 - acc: 0.9980     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.0163 - acc: 0.9980     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.0328 - acc: 0.9860     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0086 - acc: 0.9980     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0157 - acc: 0.9940     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0277 - acc: 0.9920     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0114 - acc: 0.9960     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0066 - acc: 1.0000     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0260 - acc: 0.9940     \n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 36.2 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 3.399\n",
      "Test accuracy: 0.574\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.100\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 500:\n",
    "        nb_epoch = 6\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    ### dreaming\n",
    "    # dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 13.594\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.620\n",
      "1 shot leaning, day time\n",
      "Loaded model from disk\n",
      "1 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 6s - loss: 0.3859 - acc: 0.8970 - val_loss: 0.3991 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3649 - acc: 0.9040 - val_loss: 0.4009 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3532 - acc: 0.9130 - val_loss: 0.4025 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3716 - acc: 0.8960 - val_loss: 0.4040 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3735 - acc: 0.8970 - val_loss: 0.4057 - val_acc: 0.9040\n",
      "CPU times: user 11.6 s, sys: 2.7 ms, total: 11.6 s\n",
      "Wall time: 13.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.490\n",
      "Test accuracy: 0.616\n",
      "1 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s - loss: 13.6718 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s - loss: 10.7120 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s - loss: 7.0689 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s - loss: 4.4938 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s - loss: 1.8911 - acc: 0.0000e+00\n",
      "CPU times: user 8.12 s, sys: 0 ns, total: 8.12 s\n",
      "Wall time: 7.81 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 8.831\n",
      "Test accuracy: 0.054\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.505\n",
      "Test accuracy: 0.266\n",
      "5 shot leaning, day time\n",
      "Loaded model from disk\n",
      "5 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 6s - loss: 0.3755 - acc: 0.9010 - val_loss: 0.3992 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4099 - acc: 0.8900 - val_loss: 0.4005 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3982 - acc: 0.8990 - val_loss: 0.4019 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3970 - acc: 0.8980 - val_loss: 0.4035 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3946 - acc: 0.8940 - val_loss: 0.4050 - val_acc: 0.9040\n",
      "CPU times: user 10.5 s, sys: 0 ns, total: 10.5 s\n",
      "Wall time: 12.4 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.489\n",
      "Test accuracy: 0.616\n",
      "5 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 5s - loss: 14.4881 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s - loss: 12.2229 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s - loss: 10.1761 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s - loss: 6.6304 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s - loss: 4.4142 - acc: 0.0000e+00\n",
      "CPU times: user 8.52 s, sys: 0 ns, total: 8.52 s\n",
      "Wall time: 8.17 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 4s     \n",
      "\n",
      "Test loss: 5.494\n",
      "Test accuracy: 0.042\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.148\n",
      "Test accuracy: 0.234\n",
      "10 shot leaning, day time\n",
      "Loaded model from disk\n",
      "10 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 7s - loss: 0.3834 - acc: 0.8940 - val_loss: 0.3989 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3874 - acc: 0.9040 - val_loss: 0.4004 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3634 - acc: 0.9140 - val_loss: 0.4021 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4127 - acc: 0.8910 - val_loss: 0.4035 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3982 - acc: 0.9030 - val_loss: 0.4051 - val_acc: 0.9040\n",
      "CPU times: user 10.9 s, sys: 0 ns, total: 10.9 s\n",
      "Wall time: 12.7 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.489\n",
      "Test accuracy: 0.616\n",
      "10 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 5s - loss: 13.7215 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s - loss: 11.5871 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s - loss: 7.8205 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s - loss: 5.3605 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s - loss: 2.8942 - acc: 0.3000\n",
      "CPU times: user 8.78 s, sys: 0 ns, total: 8.78 s\n",
      "Wall time: 8.44 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 5.100\n",
      "Test accuracy: 0.200\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.954\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, day time\n",
      "Loaded model from disk\n",
      "15 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 7s - loss: 0.3894 - acc: 0.8920 - val_loss: 0.3993 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3899 - acc: 0.8930 - val_loss: 0.4008 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3916 - acc: 0.9000 - val_loss: 0.4028 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3846 - acc: 0.9000 - val_loss: 0.4047 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3717 - acc: 0.9010 - val_loss: 0.4063 - val_acc: 0.9040\n",
      "CPU times: user 11.2 s, sys: 0 ns, total: 11.2 s\n",
      "Wall time: 13.1 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.491\n",
      "Test accuracy: 0.614\n",
      "15 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 6s - loss: 14.3365 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 0s - loss: 11.4973 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 0s - loss: 8.6760 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 0s - loss: 5.7142 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 0s - loss: 3.5093 - acc: 0.0000e+00\n",
      "CPU times: user 9.11 s, sys: 0 ns, total: 9.11 s\n",
      "Wall time: 8.79 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 3.953\n",
      "Test accuracy: 0.200\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.757\n",
      "Test accuracy: 0.000\n",
      "20 shot leaning, day time\n",
      "Loaded model from disk\n",
      "20 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 7s - loss: 0.3992 - acc: 0.8930 - val_loss: 0.3990 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4049 - acc: 0.8930 - val_loss: 0.4007 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3814 - acc: 0.9030 - val_loss: 0.4024 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3495 - acc: 0.9120 - val_loss: 0.4041 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4186 - acc: 0.8880 - val_loss: 0.4057 - val_acc: 0.9040\n",
      "CPU times: user 13.1 s, sys: 0 ns, total: 13.1 s\n",
      "Wall time: 14.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.490\n",
      "Test accuracy: 0.616\n",
      "20 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 6s - loss: 14.0026 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s - loss: 11.7952 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s - loss: 8.2717 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s - loss: 5.3362 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s - loss: 3.5285 - acc: 0.0500\n",
      "CPU times: user 9.35 s, sys: 0 ns, total: 9.35 s\n",
      "Wall time: 9.06 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 3.406\n",
      "Test accuracy: 0.198\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.701\n",
      "Test accuracy: 0.010\n",
      "100 shot leaning, day time\n",
      "Loaded model from disk\n",
      "100 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 8s - loss: 0.3848 - acc: 0.9050 - val_loss: 0.3989 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3998 - acc: 0.9050 - val_loss: 0.4004 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4032 - acc: 0.8910 - val_loss: 0.4020 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3895 - acc: 0.9010 - val_loss: 0.4034 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4055 - acc: 0.8940 - val_loss: 0.4050 - val_acc: 0.9040\n",
      "CPU times: user 11.9 s, sys: 0 ns, total: 11.9 s\n",
      "Wall time: 13.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.489\n",
      "Test accuracy: 0.614\n",
      "100 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 6s - loss: 14.3725 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s - loss: 10.8401 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s - loss: 7.2410 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s - loss: 4.5284 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s - loss: 2.9886 - acc: 0.0000e+00\n",
      "CPU times: user 9.85 s, sys: 0 ns, total: 9.85 s\n",
      "Wall time: 9.74 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 2.320\n",
      "Test accuracy: 0.198\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.779\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "Loaded model from disk\n",
      "200 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 8s - loss: 0.3964 - acc: 0.8960 - val_loss: 0.3992 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3797 - acc: 0.9050 - val_loss: 0.4010 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3989 - acc: 0.8930 - val_loss: 0.4024 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3835 - acc: 0.8970 - val_loss: 0.4039 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3947 - acc: 0.8980 - val_loss: 0.4056 - val_acc: 0.9040\n",
      "CPU times: user 12.3 s, sys: 86.3 ms, total: 12.4 s\n",
      "Wall time: 14.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.490\n",
      "Test accuracy: 0.616\n",
      "200 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 7s - loss: 14.3190 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s - loss: 10.6294 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 0s - loss: 6.9753 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 0s - loss: 4.2629 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 0s - loss: 2.8734 - acc: 0.0150\n",
      "CPU times: user 10.2 s, sys: 8.46 ms, total: 10.2 s\n",
      "Wall time: 10.4 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 2.221\n",
      "Test accuracy: 0.200\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.875\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "Loaded model from disk\n",
      "300 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 8s - loss: 0.3919 - acc: 0.9040 - val_loss: 0.3993 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4231 - acc: 0.8890 - val_loss: 0.4008 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4113 - acc: 0.8980 - val_loss: 0.4025 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.4202 - acc: 0.8950 - val_loss: 0.4042 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3934 - acc: 0.8900 - val_loss: 0.4057 - val_acc: 0.9040\n",
      "CPU times: user 12.5 s, sys: 335 ms, total: 12.9 s\n",
      "Wall time: 14.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.489\n",
      "Test accuracy: 0.614\n",
      "300 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 7s - loss: 13.7567 - acc: 0.0000e+00     \n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s - loss: 6.8222 - acc: 0.0000e+00     \n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s - loss: 2.8999 - acc: 0.0300     \n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s - loss: 1.9316 - acc: 0.2067     \n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s - loss: 1.7175 - acc: 0.2533     \n",
      "CPU times: user 10.8 s, sys: 70.3 ms, total: 10.9 s\n",
      "Wall time: 11.1 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 1.624\n",
      "Test accuracy: 0.206\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 6.377\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "Loaded model from disk\n",
      "400 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 9s - loss: 0.3741 - acc: 0.9000 - val_loss: 0.3989 - val_acc: 0.9040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3601 - acc: 0.9000 - val_loss: 0.4003 - val_acc: 0.9040\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3862 - acc: 0.9050 - val_loss: 0.4020 - val_acc: 0.9040\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3921 - acc: 0.8990 - val_loss: 0.4036 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s - loss: 0.3780 - acc: 0.9010 - val_loss: 0.4053 - val_acc: 0.9040\n",
      "CPU times: user 12.9 s, sys: 330 ms, total: 13.2 s\n",
      "Wall time: 14.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.489\n",
      "Test accuracy: 0.614\n",
      "400 shot leaning training, on second day task\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 8s - loss: 13.0192 - acc: 0.0000e+00     \n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 0s - loss: 6.1852 - acc: 0.0000e+00     \n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 0s - loss: 2.6904 - acc: 0.0975     \n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 0s - loss: 1.8803 - acc: 0.2400     \n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 0s - loss: 1.6545 - acc: 0.2500     \n",
      "CPU times: user 11.3 s, sys: 111 ms, total: 11.4 s\n",
      "Wall time: 11.8 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 1.628\n",
      "Test accuracy: 0.204\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 6.659\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "Loaded model from disk\n",
      "500 shot leaning, night time\n",
      "train with: \n",
      "1250\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 9s - loss: 0.3851 - acc: 0.9010 - val_loss: 0.3992 - val_acc: 0.9040\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3632 - acc: 0.9040 - val_loss: 0.4009 - val_acc: 0.9040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3850 - acc: 0.8990 - val_loss: 0.4023 - val_acc: 0.9040\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3932 - acc: 0.8990 - val_loss: 0.4037 - val_acc: 0.9040\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3923 - acc: 0.9070 - val_loss: 0.4053 - val_acc: 0.9040\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3868 - acc: 0.9040 - val_loss: 0.4070 - val_acc: 0.9040\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4010 - acc: 0.8940 - val_loss: 0.4084 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3888 - acc: 0.9020 - val_loss: 0.4101 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3956 - acc: 0.9000 - val_loss: 0.4117 - val_acc: 0.8920\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3856 - acc: 0.9030 - val_loss: 0.4133 - val_acc: 0.8880\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3670 - acc: 0.8960 - val_loss: 0.4146 - val_acc: 0.8880\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3723 - acc: 0.9020 - val_loss: 0.4164 - val_acc: 0.8880\n",
      "CPU times: user 16.5 s, sys: 1.11 s, total: 17.6 s\n",
      "Wall time: 21.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.514\n",
      "Test accuracy: 0.614\n",
      "500 shot leaning training, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 8s - loss: 12.6465 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 5.8271 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 2.5949 - acc: 0.1060     \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 1.8155 - acc: 0.2440     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.6512 - acc: 0.2520     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.5579 - acc: 0.2920     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.5039 - acc: 0.3420     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.5163 - acc: 0.3160     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.4481 - acc: 0.3720     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.3456 - acc: 0.4280     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.2254 - acc: 0.4980     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.1860 - acc: 0.4900     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.0648 - acc: 0.5840     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 0.9994 - acc: 0.5840     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 0.8851 - acc: 0.6340     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 0.7834 - acc: 0.6940     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 0.6654 - acc: 0.7840     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 0.5879 - acc: 0.7780     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 0.4682 - acc: 0.8640     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 0.3598 - acc: 0.9100     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.2590 - acc: 0.9300     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.1923 - acc: 0.9580     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.1752 - acc: 0.9560     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.0932 - acc: 0.9740     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.0981 - acc: 0.9760     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.0661 - acc: 0.9920     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.0658 - acc: 0.9820     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.0506 - acc: 0.9860     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.0501 - acc: 0.9860     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.0221 - acc: 0.9940     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.0355 - acc: 0.9900     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.0659 - acc: 0.9860     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.0528 - acc: 0.9800     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.1373 - acc: 0.9600     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.1148 - acc: 0.9680     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.0789 - acc: 0.9860     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.0906 - acc: 0.9760     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.1042 - acc: 0.9740     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.0430 - acc: 0.9880     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.0444 - acc: 0.9840     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0647 - acc: 0.9840     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0328 - acc: 0.9860     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.0261 - acc: 0.9940     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.0248 - acc: 0.9920     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0391 - acc: 0.9940     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0591 - acc: 0.9740     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0334 - acc: 0.9920     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0291 - acc: 0.9920     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0121 - acc: 0.9960     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0215 - acc: 0.9940     \n",
      "CPU times: user 24 s, sys: 2.36 s, total: 26.4 s\n",
      "Wall time: 32.7 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 3.534\n",
      "Test accuracy: 0.590\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.051\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('small_mammals_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_whole = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# nb_epoch = 30 is too long, causing catastrophic forgetting on A? is this the reason?\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 500:\n",
    "        nb_epoch = 5\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    \n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    " \n",
    "    # load json and create model\n",
    "    json_file = open('small_mammals_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_whole = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model_whole.load_weights(\"small_mammals_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning, night time\")\n",
    "    # dreaming\n",
    "    dream(model_whole, X_train_small_mammals_var, X_train_small_mammals, Y_train_small_mammals, X_test_small_mammals, Y_test_small_mammals)\n",
    "    \n",
    "    print(str(num_train_images) + \" shot leaning training, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d9b0ca53266e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# day with 30 epochs with adam learnign rate = 0.005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_whole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# test on yesterday episode -> totally forget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0897c4848202>\u001b[0m in \u001b[0;36mday\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"day time: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel_whole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# with default Adam as optimizer and softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-603a85754b7c>\u001b[0m in \u001b[0;36mload_model_whole\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# MODEL_PATH = os.path.join(HOME, 'katy/good_model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# vgg16 = load_model(MODEL_PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'good_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout_56'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0moptimizer_weight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moptimizer_weight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_weight_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m     '''\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1504\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'variables_initializer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 run_metadata):\n\u001b[1;32m    999\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1049\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "from keras.optimizers import SGD, Adam\n",
    "model_whole = day()\n",
    "# test on yesterday episode -> totally forget\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "nb_epoch = 30\n",
    "# nb_epoch = 10 is not enough to let 500 shot learning converge\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    "    model_whole = day()\n",
    "    print(str(num_train_images) + \" shot leaning, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    # batch_size removed 15:00\n",
    "    # default adam 0.64\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model with adjusted training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 14s - loss: 2.0001 - acc: 0.3045 - val_loss: 3.6633 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1289 - acc: 0.5390 - val_loss: 4.1699 - val_acc: 0.1940\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9676 - acc: 0.6240 - val_loss: 4.8009 - val_acc: 0.1940\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8236 - acc: 0.6920 - val_loss: 4.4575 - val_acc: 0.1940\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7413 - acc: 0.7270 - val_loss: 4.2181 - val_acc: 0.2140\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6707 - acc: 0.7580 - val_loss: 3.6062 - val_acc: 0.2740\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6190 - acc: 0.7810 - val_loss: 3.0244 - val_acc: 0.3280\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5590 - acc: 0.8060 - val_loss: 2.5706 - val_acc: 0.3820\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5376 - acc: 0.8145 - val_loss: 1.7081 - val_acc: 0.5260\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4780 - acc: 0.8455 - val_loss: 1.6286 - val_acc: 0.5440\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4425 - acc: 0.8605 - val_loss: 1.4959 - val_acc: 0.5660\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4084 - acc: 0.8715 - val_loss: 1.4327 - val_acc: 0.5720\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3732 - acc: 0.8795 - val_loss: 1.3956 - val_acc: 0.6020\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3474 - acc: 0.8885 - val_loss: 1.2905 - val_acc: 0.6160\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3457 - acc: 0.8880 - val_loss: 1.2596 - val_acc: 0.6300\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3006 - acc: 0.9040 - val_loss: 1.2325 - val_acc: 0.6380\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2878 - acc: 0.9165 - val_loss: 1.2293 - val_acc: 0.6400\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2585 - acc: 0.9245 - val_loss: 1.2011 - val_acc: 0.6440\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2441 - acc: 0.9275 - val_loss: 1.2235 - val_acc: 0.6580\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2398 - acc: 0.9315 - val_loss: 1.2408 - val_acc: 0.6500\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2308 - acc: 0.9250 - val_loss: 1.2167 - val_acc: 0.6580\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2071 - acc: 0.9395 - val_loss: 1.2104 - val_acc: 0.6680\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2044 - acc: 0.9420 - val_loss: 1.2660 - val_acc: 0.6700\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1929 - acc: 0.9415 - val_loss: 1.2800 - val_acc: 0.6580\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1712 - acc: 0.9540 - val_loss: 1.2806 - val_acc: 0.6600\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1631 - acc: 0.9535 - val_loss: 1.2986 - val_acc: 0.6620\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1666 - acc: 0.9505 - val_loss: 1.3039 - val_acc: 0.6640\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1498 - acc: 0.9600 - val_loss: 1.3197 - val_acc: 0.6740\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1543 - acc: 0.9545 - val_loss: 1.3356 - val_acc: 0.6700\n",
      "CPU times: user 44 s, sys: 5.31 s, total: 49.3 s\n",
      "Wall time: 1min 7s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.299\n",
      "Test accuracy: 0.668\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 13.260\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.299\n",
      "Test accuracy: 0.668\n",
      "1 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 15s - loss: 1.9603 - acc: 0.2780 - val_loss: 3.3224 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2516 - acc: 0.4800 - val_loss: 1.9633 - val_acc: 0.2760\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0406 - acc: 0.6045 - val_loss: 2.8210 - val_acc: 0.2560\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8934 - acc: 0.6720 - val_loss: 2.6772 - val_acc: 0.3120\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7791 - acc: 0.7205 - val_loss: 1.9556 - val_acc: 0.4180\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7150 - acc: 0.7405 - val_loss: 2.2278 - val_acc: 0.4160\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6179 - acc: 0.7840 - val_loss: 2.0013 - val_acc: 0.4580\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5734 - acc: 0.8070 - val_loss: 1.7048 - val_acc: 0.4920\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5257 - acc: 0.8250 - val_loss: 1.7193 - val_acc: 0.5080\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4615 - acc: 0.8455 - val_loss: 1.6556 - val_acc: 0.5160\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4537 - acc: 0.8490 - val_loss: 1.5680 - val_acc: 0.5460\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4008 - acc: 0.8675 - val_loss: 1.4818 - val_acc: 0.5560\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3913 - acc: 0.8715 - val_loss: 1.4032 - val_acc: 0.5820\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3529 - acc: 0.8865 - val_loss: 1.4295 - val_acc: 0.5740\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3286 - acc: 0.8985 - val_loss: 1.4077 - val_acc: 0.5920\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3009 - acc: 0.9020 - val_loss: 1.4044 - val_acc: 0.5960\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2852 - acc: 0.9150 - val_loss: 1.3637 - val_acc: 0.6080\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2641 - acc: 0.9230 - val_loss: 1.3747 - val_acc: 0.6080\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2516 - acc: 0.9185 - val_loss: 1.3651 - val_acc: 0.6120\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2339 - acc: 0.9315 - val_loss: 1.3585 - val_acc: 0.6180\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2242 - acc: 0.9300 - val_loss: 1.3810 - val_acc: 0.6220\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2144 - acc: 0.9365 - val_loss: 1.3992 - val_acc: 0.6200\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2116 - acc: 0.9435 - val_loss: 1.3911 - val_acc: 0.6240\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1966 - acc: 0.9375 - val_loss: 1.3733 - val_acc: 0.6400\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1891 - acc: 0.9455 - val_loss: 1.3654 - val_acc: 0.6360\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1812 - acc: 0.9430 - val_loss: 1.3917 - val_acc: 0.6280\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1784 - acc: 0.9455 - val_loss: 1.3903 - val_acc: 0.6360\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1696 - acc: 0.9515 - val_loss: 1.3975 - val_acc: 0.6420\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1656 - acc: 0.9495 - val_loss: 1.4743 - val_acc: 0.6260\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1557 - acc: 0.9560 - val_loss: 1.5186 - val_acc: 0.6180\n",
      "CPU times: user 45.8 s, sys: 5.64 s, total: 51.4 s\n",
      "Wall time: 1min 9s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.576\n",
      "Test accuracy: 0.606\n",
      "1 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 11s - loss: 13.5041 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s - loss: 10.7570 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s - loss: 7.2769 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s - loss: 4.5592 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s - loss: 2.4405 - acc: 0.0000e+00\n",
      "CPU times: user 14.5 s, sys: 109 ms, total: 14.6 s\n",
      "Wall time: 14.5 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 9.473\n",
      "Test accuracy: 0.010\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.611\n",
      "Test accuracy: 0.358\n",
      "5 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 15s - loss: 2.0457 - acc: 0.2855 - val_loss: 2.0337 - val_acc: 0.2120\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2264 - acc: 0.5175 - val_loss: 2.8332 - val_acc: 0.2420\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9944 - acc: 0.6185 - val_loss: 2.8301 - val_acc: 0.2220\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8541 - acc: 0.6810 - val_loss: 2.2543 - val_acc: 0.3160\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7523 - acc: 0.7370 - val_loss: 2.2642 - val_acc: 0.3400\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6602 - acc: 0.7580 - val_loss: 2.6866 - val_acc: 0.3380\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5933 - acc: 0.7980 - val_loss: 2.1974 - val_acc: 0.4320\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5559 - acc: 0.8130 - val_loss: 2.2058 - val_acc: 0.4360\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5080 - acc: 0.8365 - val_loss: 2.0181 - val_acc: 0.4980\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4697 - acc: 0.8475 - val_loss: 2.0387 - val_acc: 0.4940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4193 - acc: 0.8635 - val_loss: 1.6347 - val_acc: 0.5480\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3845 - acc: 0.8805 - val_loss: 1.4913 - val_acc: 0.5740\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3687 - acc: 0.8775 - val_loss: 1.4822 - val_acc: 0.5820\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3438 - acc: 0.8910 - val_loss: 1.2617 - val_acc: 0.6280\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3150 - acc: 0.9000 - val_loss: 1.2697 - val_acc: 0.6360\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2918 - acc: 0.9075 - val_loss: 1.3527 - val_acc: 0.6280\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2861 - acc: 0.9160 - val_loss: 1.3036 - val_acc: 0.6400\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2778 - acc: 0.9065 - val_loss: 1.2166 - val_acc: 0.6620\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2679 - acc: 0.9140 - val_loss: 1.1750 - val_acc: 0.6760\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2390 - acc: 0.9290 - val_loss: 1.1448 - val_acc: 0.6800\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2314 - acc: 0.9295 - val_loss: 1.1413 - val_acc: 0.6880\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2152 - acc: 0.9385 - val_loss: 1.1591 - val_acc: 0.6780\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2168 - acc: 0.9340 - val_loss: 1.1659 - val_acc: 0.6800\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2199 - acc: 0.9260 - val_loss: 1.1895 - val_acc: 0.6800\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1799 - acc: 0.9470 - val_loss: 1.1984 - val_acc: 0.6760\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1776 - acc: 0.9410 - val_loss: 1.1827 - val_acc: 0.6740\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1759 - acc: 0.9455 - val_loss: 1.2114 - val_acc: 0.6720\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1578 - acc: 0.9530 - val_loss: 1.2270 - val_acc: 0.6800\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1744 - acc: 0.9460 - val_loss: 1.2475 - val_acc: 0.6740\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1557 - acc: 0.9560 - val_loss: 1.2663 - val_acc: 0.6800\n",
      "CPU times: user 46.4 s, sys: 5.63 s, total: 52.1 s\n",
      "Wall time: 1min 10s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.294\n",
      "Test accuracy: 0.642\n",
      "5 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 12s - loss: 14.6866 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s - loss: 11.6870 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s - loss: 8.5804 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s - loss: 6.2541 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s - loss: 3.8218 - acc: 0.0000e+00\n",
      "CPU times: user 18 s, sys: 125 ms, total: 18.1 s\n",
      "Wall time: 17.9 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 10s    \n",
      "\n",
      "Test loss: 4.732\n",
      "Test accuracy: 0.060\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.348\n",
      "Test accuracy: 0.182\n",
      "10 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 16s - loss: 2.0208 - acc: 0.2985 - val_loss: 2.9342 - val_acc: 0.2000\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2875 - acc: 0.4555 - val_loss: 2.1155 - val_acc: 0.3380\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1303 - acc: 0.5315 - val_loss: 3.0433 - val_acc: 0.2980\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9914 - acc: 0.6085 - val_loss: 2.8202 - val_acc: 0.3320\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8642 - acc: 0.6670 - val_loss: 2.5741 - val_acc: 0.3620\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7993 - acc: 0.6995 - val_loss: 2.0544 - val_acc: 0.4220\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7200 - acc: 0.7360 - val_loss: 2.2284 - val_acc: 0.4220\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6510 - acc: 0.7760 - val_loss: 2.1253 - val_acc: 0.4520\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6011 - acc: 0.7890 - val_loss: 2.0468 - val_acc: 0.4420\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5577 - acc: 0.8055 - val_loss: 2.0290 - val_acc: 0.4700\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5106 - acc: 0.8320 - val_loss: 1.9855 - val_acc: 0.4640\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4905 - acc: 0.8305 - val_loss: 1.9685 - val_acc: 0.4820\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4413 - acc: 0.8550 - val_loss: 1.8002 - val_acc: 0.5080\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4073 - acc: 0.8665 - val_loss: 1.7297 - val_acc: 0.5320\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3836 - acc: 0.8805 - val_loss: 1.6545 - val_acc: 0.5540\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3642 - acc: 0.8860 - val_loss: 1.5896 - val_acc: 0.5900\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3348 - acc: 0.8985 - val_loss: 1.5530 - val_acc: 0.5960\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3089 - acc: 0.9080 - val_loss: 1.5141 - val_acc: 0.6080\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2958 - acc: 0.9085 - val_loss: 1.4220 - val_acc: 0.6300\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2750 - acc: 0.9150 - val_loss: 1.3981 - val_acc: 0.6380\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2678 - acc: 0.9170 - val_loss: 1.3877 - val_acc: 0.6320\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2551 - acc: 0.9205 - val_loss: 1.3262 - val_acc: 0.6480\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2474 - acc: 0.9260 - val_loss: 1.2792 - val_acc: 0.6460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2301 - acc: 0.9325 - val_loss: 1.2881 - val_acc: 0.6540\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2322 - acc: 0.9290 - val_loss: 1.2962 - val_acc: 0.6540\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2189 - acc: 0.9315 - val_loss: 1.3082 - val_acc: 0.6520\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2099 - acc: 0.9295 - val_loss: 1.2911 - val_acc: 0.6520\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2022 - acc: 0.9380 - val_loss: 1.2939 - val_acc: 0.6460\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1852 - acc: 0.9425 - val_loss: 1.3300 - val_acc: 0.6400\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1795 - acc: 0.9520 - val_loss: 1.3622 - val_acc: 0.6400\n",
      "CPU times: user 46.9 s, sys: 5.8 s, total: 52.7 s\n",
      "Wall time: 1min 11s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.428\n",
      "Test accuracy: 0.640\n",
      "10 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 12s - loss: 13.8973 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s - loss: 10.8322 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s - loss: 8.1878 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s - loss: 5.3156 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s - loss: 3.0910 - acc: 0.1000\n",
      "CPU times: user 15.4 s, sys: 151 ms, total: 15.5 s\n",
      "Wall time: 15.4 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 5.180\n",
      "Test accuracy: 0.200\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.344\n",
      "Test accuracy: 0.000\n",
      "15 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 16s - loss: 2.0217 - acc: 0.2910 - val_loss: 3.0095 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2697 - acc: 0.4530 - val_loss: 3.7090 - val_acc: 0.2000\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0854 - acc: 0.5595 - val_loss: 3.4449 - val_acc: 0.2220\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9195 - acc: 0.6445 - val_loss: 3.9560 - val_acc: 0.2080\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8070 - acc: 0.7125 - val_loss: 3.8500 - val_acc: 0.2360\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7102 - acc: 0.7570 - val_loss: 3.3326 - val_acc: 0.2820\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6548 - acc: 0.7740 - val_loss: 2.8491 - val_acc: 0.3480\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5851 - acc: 0.8020 - val_loss: 2.9202 - val_acc: 0.3460\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5397 - acc: 0.8265 - val_loss: 2.1006 - val_acc: 0.4400\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4916 - acc: 0.8450 - val_loss: 2.0132 - val_acc: 0.4720\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4594 - acc: 0.8600 - val_loss: 1.7417 - val_acc: 0.5080\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4213 - acc: 0.8635 - val_loss: 1.6704 - val_acc: 0.5340\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3754 - acc: 0.8835 - val_loss: 1.4273 - val_acc: 0.5920\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3473 - acc: 0.8990 - val_loss: 1.3742 - val_acc: 0.5980\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3299 - acc: 0.8995 - val_loss: 1.3097 - val_acc: 0.6120\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3150 - acc: 0.8970 - val_loss: 1.2613 - val_acc: 0.6260\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2927 - acc: 0.9125 - val_loss: 1.2322 - val_acc: 0.6400\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2666 - acc: 0.9245 - val_loss: 1.2115 - val_acc: 0.6520\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2650 - acc: 0.9220 - val_loss: 1.2358 - val_acc: 0.6420\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2350 - acc: 0.9340 - val_loss: 1.2509 - val_acc: 0.6500\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2359 - acc: 0.9320 - val_loss: 1.2796 - val_acc: 0.6480\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2194 - acc: 0.9345 - val_loss: 1.2806 - val_acc: 0.6400\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1998 - acc: 0.9460 - val_loss: 1.2912 - val_acc: 0.6460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1927 - acc: 0.9450 - val_loss: 1.3192 - val_acc: 0.6300\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1882 - acc: 0.9455 - val_loss: 1.3383 - val_acc: 0.6260\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1820 - acc: 0.9465 - val_loss: 1.3453 - val_acc: 0.6240\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1767 - acc: 0.9495 - val_loss: 1.3763 - val_acc: 0.6180\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1727 - acc: 0.9585 - val_loss: 1.4180 - val_acc: 0.6200\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1678 - acc: 0.9500 - val_loss: 1.4381 - val_acc: 0.6200\n",
      "CPU times: user 46.3 s, sys: 5.6 s, total: 51.9 s\n",
      "Wall time: 1min 9s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.311\n",
      "Test accuracy: 0.636\n",
      "15 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 13s - loss: 13.7271 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 0s - loss: 11.0659 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 0s - loss: 8.1603 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 0s - loss: 5.2160 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 0s - loss: 3.3698 - acc: 0.2000\n",
      "CPU times: user 15.9 s, sys: 149 ms, total: 16 s\n",
      "Wall time: 15.9 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 11s    \n",
      "\n",
      "Test loss: 4.409\n",
      "Test accuracy: 0.200\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.122\n",
      "Test accuracy: 0.000\n",
      "20 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 17s - loss: 2.0305 - acc: 0.3010 - val_loss: 2.1571 - val_acc: 0.2080\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2800 - acc: 0.4600 - val_loss: 1.8292 - val_acc: 0.3360\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0964 - acc: 0.5530 - val_loss: 1.6396 - val_acc: 0.4060\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9757 - acc: 0.6080 - val_loss: 1.7267 - val_acc: 0.4120\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8965 - acc: 0.6530 - val_loss: 1.8033 - val_acc: 0.4340\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8179 - acc: 0.7015 - val_loss: 2.0498 - val_acc: 0.3980\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7406 - acc: 0.7210 - val_loss: 2.0597 - val_acc: 0.4340\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6839 - acc: 0.7485 - val_loss: 2.1052 - val_acc: 0.4280\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6032 - acc: 0.7890 - val_loss: 2.1642 - val_acc: 0.4160\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5812 - acc: 0.7970 - val_loss: 2.2875 - val_acc: 0.3960\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5426 - acc: 0.8125 - val_loss: 2.2176 - val_acc: 0.4440\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5042 - acc: 0.8295 - val_loss: 2.2927 - val_acc: 0.4020\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4656 - acc: 0.8475 - val_loss: 2.3381 - val_acc: 0.3920\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4475 - acc: 0.8520 - val_loss: 2.1410 - val_acc: 0.4500\n",
      "CPU times: user 32.1 s, sys: 2.71 s, total: 34.8 s\n",
      "Wall time: 43.3 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.145\n",
      "Test accuracy: 0.434\n",
      "20 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 14s - loss: 12.5078 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s - loss: 9.7466 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s - loss: 6.8640 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s - loss: 4.8629 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s - loss: 3.2855 - acc: 0.1000\n",
      "CPU times: user 16.6 s, sys: 195 ms, total: 16.8 s\n",
      "Wall time: 16.7 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 3.223\n",
      "Test accuracy: 0.200\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.652\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 18s - loss: 1.9313 - acc: 0.3410 - val_loss: 2.5947 - val_acc: 0.2500\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1053 - acc: 0.5375 - val_loss: 2.6797 - val_acc: 0.3240\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9628 - acc: 0.6045 - val_loss: 2.3371 - val_acc: 0.3800\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8324 - acc: 0.6740 - val_loss: 2.4206 - val_acc: 0.3840\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7407 - acc: 0.7300 - val_loss: 2.3569 - val_acc: 0.3980\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6823 - acc: 0.7575 - val_loss: 2.3825 - val_acc: 0.4160\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6245 - acc: 0.7835 - val_loss: 2.3737 - val_acc: 0.4120\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5624 - acc: 0.8110 - val_loss: 2.3330 - val_acc: 0.4320\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5020 - acc: 0.8335 - val_loss: 2.1493 - val_acc: 0.4580\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4744 - acc: 0.8425 - val_loss: 2.0842 - val_acc: 0.4720\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4346 - acc: 0.8495 - val_loss: 1.9934 - val_acc: 0.4800\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4171 - acc: 0.8710 - val_loss: 1.7660 - val_acc: 0.5280\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3639 - acc: 0.8825 - val_loss: 1.6012 - val_acc: 0.5420\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3419 - acc: 0.8890 - val_loss: 1.5399 - val_acc: 0.5620\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3103 - acc: 0.9070 - val_loss: 1.5564 - val_acc: 0.5640\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2881 - acc: 0.9180 - val_loss: 1.4923 - val_acc: 0.5760\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2803 - acc: 0.9145 - val_loss: 1.4113 - val_acc: 0.6000\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2569 - acc: 0.9215 - val_loss: 1.3847 - val_acc: 0.6180\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2374 - acc: 0.9335 - val_loss: 1.3312 - val_acc: 0.6220\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2264 - acc: 0.9350 - val_loss: 1.3195 - val_acc: 0.6440\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2114 - acc: 0.9360 - val_loss: 1.3153 - val_acc: 0.6460\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2295 - acc: 0.9335 - val_loss: 1.3400 - val_acc: 0.6400\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2015 - acc: 0.9430 - val_loss: 1.2937 - val_acc: 0.6460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1914 - acc: 0.9450 - val_loss: 1.2792 - val_acc: 0.6600\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1743 - acc: 0.9540 - val_loss: 1.2999 - val_acc: 0.6520\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1664 - acc: 0.9560 - val_loss: 1.3066 - val_acc: 0.6600\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1544 - acc: 0.9570 - val_loss: 1.3045 - val_acc: 0.6680\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1477 - acc: 0.9600 - val_loss: 1.3114 - val_acc: 0.6680\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1756 - acc: 0.9445 - val_loss: 1.3494 - val_acc: 0.6660\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1478 - acc: 0.9595 - val_loss: 1.3543 - val_acc: 0.6660\n",
      "CPU times: user 49 s, sys: 5.74 s, total: 54.8 s\n",
      "Wall time: 1min 13s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.405\n",
      "Test accuracy: 0.652\n",
      "100 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 14s - loss: 13.6133 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s - loss: 10.3412 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s - loss: 6.9171 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s - loss: 4.3469 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s - loss: 2.8376 - acc: 0.0600\n",
      "CPU times: user 17.2 s, sys: 272 ms, total: 17.5 s\n",
      "Wall time: 17.6 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 12s    \n",
      "\n",
      "Test loss: 2.413\n",
      "Test accuracy: 0.200\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.011\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 18s - loss: 2.0905 - acc: 0.2440 - val_loss: 2.8948 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2952 - acc: 0.4510 - val_loss: 1.8072 - val_acc: 0.3560\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0979 - acc: 0.5570 - val_loss: 1.5943 - val_acc: 0.3840\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9884 - acc: 0.6090 - val_loss: 1.6773 - val_acc: 0.4280\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8928 - acc: 0.6565 - val_loss: 2.0555 - val_acc: 0.4040\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8112 - acc: 0.6925 - val_loss: 1.9249 - val_acc: 0.4340\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7339 - acc: 0.7245 - val_loss: 1.8700 - val_acc: 0.4560\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6746 - acc: 0.7520 - val_loss: 1.7805 - val_acc: 0.4620\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6171 - acc: 0.7850 - val_loss: 1.7567 - val_acc: 0.4680\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5684 - acc: 0.8020 - val_loss: 1.6927 - val_acc: 0.4940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 2s - loss: 0.5253 - acc: 0.8250 - val_loss: 1.5597 - val_acc: 0.5300\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4858 - acc: 0.8350 - val_loss: 1.5157 - val_acc: 0.5240\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4515 - acc: 0.8535 - val_loss: 1.4559 - val_acc: 0.5620\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 2s - loss: 0.4167 - acc: 0.8615 - val_loss: 1.4791 - val_acc: 0.5560\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4055 - acc: 0.8635 - val_loss: 1.5082 - val_acc: 0.5480\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3848 - acc: 0.8720 - val_loss: 1.4190 - val_acc: 0.5880\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3558 - acc: 0.8780 - val_loss: 1.4160 - val_acc: 0.5960\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3231 - acc: 0.8990 - val_loss: 1.5163 - val_acc: 0.5720\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3065 - acc: 0.9005 - val_loss: 1.5151 - val_acc: 0.5920\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2963 - acc: 0.9050 - val_loss: 1.4890 - val_acc: 0.6120\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2915 - acc: 0.9085 - val_loss: 1.4703 - val_acc: 0.6180\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2783 - acc: 0.9160 - val_loss: 1.4127 - val_acc: 0.6300\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2562 - acc: 0.9250 - val_loss: 1.4261 - val_acc: 0.6180\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 2s - loss: 0.2404 - acc: 0.9315 - val_loss: 1.4567 - val_acc: 0.6180\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2248 - acc: 0.9315 - val_loss: 1.4244 - val_acc: 0.6280\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2258 - acc: 0.9305 - val_loss: 1.4336 - val_acc: 0.6340\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2071 - acc: 0.9390 - val_loss: 1.4705 - val_acc: 0.6340\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2139 - acc: 0.9380 - val_loss: 1.4869 - val_acc: 0.6240\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2090 - acc: 0.9375 - val_loss: 1.4723 - val_acc: 0.6300\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1868 - acc: 0.9420 - val_loss: 1.4953 - val_acc: 0.6300\n",
      "CPU times: user 49.5 s, sys: 5.87 s, total: 55.4 s\n",
      "Wall time: 1min 15s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.381\n",
      "Test accuracy: 0.628\n",
      "200 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 16s - loss: 13.8414 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s - loss: 9.8466 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 0s - loss: 6.1611 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 0s - loss: 3.7312 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 0s - loss: 2.5840 - acc: 0.2150\n",
      "CPU times: user 18 s, sys: 394 ms, total: 18.4 s\n",
      "Wall time: 19.5 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 13s    \n",
      "\n",
      "Test loss: 2.144\n",
      "Test accuracy: 0.200\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.490\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 19s - loss: 2.0723 - acc: 0.2755 - val_loss: 2.0353 - val_acc: 0.2420\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2857 - acc: 0.4505 - val_loss: 2.1656 - val_acc: 0.2820\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1168 - acc: 0.5595 - val_loss: 1.7972 - val_acc: 0.3460\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9804 - acc: 0.6160 - val_loss: 1.9000 - val_acc: 0.3320\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8889 - acc: 0.6710 - val_loss: 1.9382 - val_acc: 0.3740\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7959 - acc: 0.7155 - val_loss: 2.0822 - val_acc: 0.3760\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7251 - acc: 0.7525 - val_loss: 1.9420 - val_acc: 0.4020\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6460 - acc: 0.7745 - val_loss: 2.0214 - val_acc: 0.4020\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6142 - acc: 0.7920 - val_loss: 1.9087 - val_acc: 0.4340\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5668 - acc: 0.8080 - val_loss: 1.8328 - val_acc: 0.4460\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5216 - acc: 0.8350 - val_loss: 1.7317 - val_acc: 0.4840\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4779 - acc: 0.8475 - val_loss: 1.5880 - val_acc: 0.5080\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4437 - acc: 0.8545 - val_loss: 1.5046 - val_acc: 0.5180\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4221 - acc: 0.8620 - val_loss: 1.4639 - val_acc: 0.5480\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4048 - acc: 0.8715 - val_loss: 1.4662 - val_acc: 0.5560\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3855 - acc: 0.8740 - val_loss: 1.3473 - val_acc: 0.5860\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3602 - acc: 0.8870 - val_loss: 1.3409 - val_acc: 0.6000\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3314 - acc: 0.8970 - val_loss: 1.2890 - val_acc: 0.6180\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3143 - acc: 0.8995 - val_loss: 1.2244 - val_acc: 0.6320\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3035 - acc: 0.9115 - val_loss: 1.2237 - val_acc: 0.6400\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2702 - acc: 0.9225 - val_loss: 1.1920 - val_acc: 0.6400\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2613 - acc: 0.9245 - val_loss: 1.1947 - val_acc: 0.6460\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2542 - acc: 0.9240 - val_loss: 1.2193 - val_acc: 0.6480\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2437 - acc: 0.9280 - val_loss: 1.2221 - val_acc: 0.6540\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2298 - acc: 0.9345 - val_loss: 1.2167 - val_acc: 0.6600\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2250 - acc: 0.9330 - val_loss: 1.2085 - val_acc: 0.6800\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2118 - acc: 0.9435 - val_loss: 1.2324 - val_acc: 0.6700\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2104 - acc: 0.9400 - val_loss: 1.2632 - val_acc: 0.6660\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2107 - acc: 0.9390 - val_loss: 1.2645 - val_acc: 0.6680\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1943 - acc: 0.9465 - val_loss: 1.2551 - val_acc: 0.6700\n",
      "CPU times: user 50.6 s, sys: 5.71 s, total: 56.3 s\n",
      "Wall time: 1min 15s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.303\n",
      "Test accuracy: 0.636\n",
      "300 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 16s - loss: 12.8646 - acc: 0.0000e+00    \n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 0s - loss: 6.2325 - acc: 0.0000e+00     \n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 0s - loss: 2.8047 - acc: 0.0600     \n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 0s - loss: 1.9367 - acc: 0.2667     \n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 0s - loss: 1.6994 - acc: 0.2633     \n",
      "CPU times: user 18.8 s, sys: 312 ms, total: 19.1 s\n",
      "Wall time: 19.8 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 13s    \n",
      "\n",
      "Test loss: 1.646\n",
      "Test accuracy: 0.218\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 6.543\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 20s - loss: 1.9835 - acc: 0.3020 - val_loss: 2.6902 - val_acc: 0.2220\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1988 - acc: 0.4920 - val_loss: 1.9896 - val_acc: 0.3460\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0117 - acc: 0.6110 - val_loss: 2.1103 - val_acc: 0.3840\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8746 - acc: 0.6730 - val_loss: 2.0255 - val_acc: 0.3880\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7738 - acc: 0.7200 - val_loss: 2.1132 - val_acc: 0.3960\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6659 - acc: 0.7645 - val_loss: 2.0390 - val_acc: 0.4260\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6125 - acc: 0.7855 - val_loss: 2.1635 - val_acc: 0.4200\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5594 - acc: 0.8100 - val_loss: 2.1264 - val_acc: 0.4320\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5046 - acc: 0.8285 - val_loss: 2.0994 - val_acc: 0.4520\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4794 - acc: 0.8370 - val_loss: 1.8613 - val_acc: 0.4840\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4409 - acc: 0.8600 - val_loss: 1.8580 - val_acc: 0.4960\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4093 - acc: 0.8670 - val_loss: 1.7531 - val_acc: 0.5200\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3825 - acc: 0.8735 - val_loss: 1.6893 - val_acc: 0.5460\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3677 - acc: 0.8825 - val_loss: 1.5885 - val_acc: 0.5620\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3422 - acc: 0.8860 - val_loss: 1.4685 - val_acc: 0.5820\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3247 - acc: 0.8925 - val_loss: 1.4758 - val_acc: 0.5880\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3005 - acc: 0.9045 - val_loss: 1.4373 - val_acc: 0.5980\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2891 - acc: 0.9105 - val_loss: 1.3704 - val_acc: 0.6260\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2717 - acc: 0.9175 - val_loss: 1.3703 - val_acc: 0.6340\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2508 - acc: 0.9240 - val_loss: 1.3252 - val_acc: 0.6340\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2573 - acc: 0.9210 - val_loss: 1.3023 - val_acc: 0.6400\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2469 - acc: 0.9270 - val_loss: 1.3258 - val_acc: 0.6420\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2483 - acc: 0.9175 - val_loss: 1.3078 - val_acc: 0.6460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2181 - acc: 0.9330 - val_loss: 1.3101 - val_acc: 0.6420\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2032 - acc: 0.9350 - val_loss: 1.3203 - val_acc: 0.6380\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2012 - acc: 0.9435 - val_loss: 1.3280 - val_acc: 0.6400\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2131 - acc: 0.9365 - val_loss: 1.3276 - val_acc: 0.6520\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1984 - acc: 0.9440 - val_loss: 1.3297 - val_acc: 0.6580\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1763 - acc: 0.9505 - val_loss: 1.3365 - val_acc: 0.6520\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1767 - acc: 0.9465 - val_loss: 1.3565 - val_acc: 0.6420\n",
      "CPU times: user 54.9 s, sys: 6.02 s, total: 1min\n",
      "Wall time: 1min 21s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.258\n",
      "Test accuracy: 0.674\n",
      "400 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 16s - loss: 12.8461 - acc: 0.0000e+00    \n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 0s - loss: 6.0252 - acc: 0.0000e+00     \n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 0s - loss: 2.6343 - acc: 0.1325     \n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 0s - loss: 1.8178 - acc: 0.2650     \n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 0s - loss: 1.6750 - acc: 0.2350     \n",
      "CPU times: user 19.2 s, sys: 374 ms, total: 19.6 s\n",
      "Wall time: 20.1 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 13s    \n",
      "\n",
      "Test loss: 1.614\n",
      "Test accuracy: 0.244\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 6.723\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 20s - loss: 1.9025 - acc: 0.3335 - val_loss: 3.6126 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1991 - acc: 0.4990 - val_loss: 4.2769 - val_acc: 0.1940\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0367 - acc: 0.5705 - val_loss: 4.1677 - val_acc: 0.2140\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9228 - acc: 0.6280 - val_loss: 3.5164 - val_acc: 0.2780\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8002 - acc: 0.6990 - val_loss: 3.3917 - val_acc: 0.2780\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7140 - acc: 0.7455 - val_loss: 2.5598 - val_acc: 0.3600\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6396 - acc: 0.7700 - val_loss: 2.3719 - val_acc: 0.3780\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5921 - acc: 0.7905 - val_loss: 2.2669 - val_acc: 0.4180\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5174 - acc: 0.8300 - val_loss: 2.2172 - val_acc: 0.4340\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4996 - acc: 0.8330 - val_loss: 2.0303 - val_acc: 0.4620\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4493 - acc: 0.8610 - val_loss: 1.9014 - val_acc: 0.5060\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4187 - acc: 0.8740 - val_loss: 1.7702 - val_acc: 0.5220\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3897 - acc: 0.8700 - val_loss: 1.7580 - val_acc: 0.5300\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3593 - acc: 0.8865 - val_loss: 1.7054 - val_acc: 0.5460\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3440 - acc: 0.8890 - val_loss: 1.5957 - val_acc: 0.5540\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3047 - acc: 0.9100 - val_loss: 1.4768 - val_acc: 0.5800\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2869 - acc: 0.9130 - val_loss: 1.4383 - val_acc: 0.5980\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2794 - acc: 0.9130 - val_loss: 1.4204 - val_acc: 0.6040\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2625 - acc: 0.9205 - val_loss: 1.4109 - val_acc: 0.6180\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2562 - acc: 0.9210 - val_loss: 1.4134 - val_acc: 0.6260\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2191 - acc: 0.9390 - val_loss: 1.4101 - val_acc: 0.6220\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2194 - acc: 0.9360 - val_loss: 1.3821 - val_acc: 0.6220\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2100 - acc: 0.9390 - val_loss: 1.3691 - val_acc: 0.6480\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2087 - acc: 0.9420 - val_loss: 1.3462 - val_acc: 0.6620\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1824 - acc: 0.9465 - val_loss: 1.3651 - val_acc: 0.6380\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1790 - acc: 0.9510 - val_loss: 1.4033 - val_acc: 0.6340\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1665 - acc: 0.9535 - val_loss: 1.4199 - val_acc: 0.6400\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1746 - acc: 0.9530 - val_loss: 1.4326 - val_acc: 0.6500\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1694 - acc: 0.9550 - val_loss: 1.4494 - val_acc: 0.6460\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1461 - acc: 0.9645 - val_loss: 1.4527 - val_acc: 0.6460\n",
      "CPU times: user 51.3 s, sys: 5.66 s, total: 56.9 s\n",
      "Wall time: 1min 15s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.486\n",
      "Test accuracy: 0.650\n",
      "500 shot leaning, on second day task\n",
      "Epoch 1/30\n",
      "500/500 [==============================] - 16s - loss: 12.1771 - acc: 0.0000e+00     \n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 0s - loss: 5.6217 - acc: 0.0000e+00     \n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 0s - loss: 2.6745 - acc: 0.1620     \n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 0s - loss: 1.9256 - acc: 0.2360     \n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 0s - loss: 1.6907 - acc: 0.2520     \n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 0s - loss: 1.6659 - acc: 0.1880     \n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 0s - loss: 1.6308 - acc: 0.2200     \n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 0s - loss: 1.6120 - acc: 0.2380     \n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 0s - loss: 1.5439 - acc: 0.3120     \n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 0s - loss: 1.5207 - acc: 0.3000     \n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 0s - loss: 1.4711 - acc: 0.2980     \n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 0s - loss: 1.4094 - acc: 0.3660     \n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 0s - loss: 1.3362 - acc: 0.3880     \n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 0s - loss: 1.3250 - acc: 0.3880     \n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 0s - loss: 1.1850 - acc: 0.4640     \n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 0s - loss: 1.1268 - acc: 0.4840     \n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 0s - loss: 0.9804 - acc: 0.6020     \n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 0s - loss: 0.9099 - acc: 0.5940     \n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 0s - loss: 0.7743 - acc: 0.6620     \n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 0s - loss: 0.6820 - acc: 0.7100     \n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 0s - loss: 0.6217 - acc: 0.7340     \n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 0s - loss: 0.5115 - acc: 0.7880     \n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 0s - loss: 0.4646 - acc: 0.7940     \n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 0s - loss: 0.4184 - acc: 0.8460     \n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 0s - loss: 0.3961 - acc: 0.8380     \n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 0s - loss: 0.2831 - acc: 0.8980     \n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 0s - loss: 0.2593 - acc: 0.9140     \n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 0s - loss: 0.3304 - acc: 0.8780     \n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 0s - loss: 0.1725 - acc: 0.9440     \n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 0s - loss: 0.1455 - acc: 0.9500     \n",
      "CPU times: user 26 s, sys: 1.49 s, total: 27.4 s\n",
      "Wall time: 31.6 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 13s    \n",
      "\n",
      "Test loss: 8.499\n",
      "Test accuracy: 0.208\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.117\n",
      "Test accuracy: 0.000\n",
      "1000 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 21s - loss: 1.9722 - acc: 0.3350 - val_loss: 2.8139 - val_acc: 0.2300\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1555 - acc: 0.5140 - val_loss: 4.0590 - val_acc: 0.1980\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9839 - acc: 0.6140 - val_loss: 3.0016 - val_acc: 0.2940\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8569 - acc: 0.6695 - val_loss: 3.2369 - val_acc: 0.2380\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7562 - acc: 0.7180 - val_loss: 3.1877 - val_acc: 0.2680\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6913 - acc: 0.7415 - val_loss: 2.8870 - val_acc: 0.3160\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6207 - acc: 0.7825 - val_loss: 2.5381 - val_acc: 0.3840\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5788 - acc: 0.7990 - val_loss: 2.0344 - val_acc: 0.4660\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5296 - acc: 0.8245 - val_loss: 1.9214 - val_acc: 0.4920\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4988 - acc: 0.8300 - val_loss: 1.7402 - val_acc: 0.5120\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4550 - acc: 0.8515 - val_loss: 1.5203 - val_acc: 0.5800\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4293 - acc: 0.8600 - val_loss: 1.4302 - val_acc: 0.6080\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4159 - acc: 0.8600 - val_loss: 1.3527 - val_acc: 0.6260\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3675 - acc: 0.8915 - val_loss: 1.2220 - val_acc: 0.6660\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3543 - acc: 0.8955 - val_loss: 1.1648 - val_acc: 0.6780\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3352 - acc: 0.8970 - val_loss: 1.1151 - val_acc: 0.6740\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3079 - acc: 0.9065 - val_loss: 1.0776 - val_acc: 0.6760\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3034 - acc: 0.9135 - val_loss: 1.1137 - val_acc: 0.6680\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2883 - acc: 0.9105 - val_loss: 1.1167 - val_acc: 0.6700\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2669 - acc: 0.9260 - val_loss: 1.1063 - val_acc: 0.6640\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2625 - acc: 0.9260 - val_loss: 1.0913 - val_acc: 0.6700\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2361 - acc: 0.9290 - val_loss: 1.0872 - val_acc: 0.6700\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2259 - acc: 0.9340 - val_loss: 1.0769 - val_acc: 0.6820\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2097 - acc: 0.9370 - val_loss: 1.0852 - val_acc: 0.6800\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2053 - acc: 0.9395 - val_loss: 1.0978 - val_acc: 0.6780\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2044 - acc: 0.9425 - val_loss: 1.1115 - val_acc: 0.6720\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1986 - acc: 0.9435 - val_loss: 1.1209 - val_acc: 0.6720\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1726 - acc: 0.9485 - val_loss: 1.1268 - val_acc: 0.6680\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1741 - acc: 0.9450 - val_loss: 1.1412 - val_acc: 0.6780\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1736 - acc: 0.9495 - val_loss: 1.1752 - val_acc: 0.6700\n",
      "CPU times: user 51.7 s, sys: 5.75 s, total: 57.5 s\n",
      "Wall time: 1min 17s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.274\n",
      "Test accuracy: 0.654\n",
      "1000 shot leaning, on second day task\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 17s - loss: 9.2729 - acc: 0.0000e+00     \n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s - loss: 2.4529 - acc: 0.1390     \n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.7120 - acc: 0.2030     \n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.6671 - acc: 0.2160     \n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.6372 - acc: 0.2360     \n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.6341 - acc: 0.2160     \n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.6243 - acc: 0.2240     \n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 1s - loss: 1.5872 - acc: 0.2570     \n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.5007 - acc: 0.3470     \n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.3631 - acc: 0.3990     \n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.2243 - acc: 0.4800     \n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.1088 - acc: 0.4990     \n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s - loss: 1.0097 - acc: 0.5670     \n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.8418 - acc: 0.6610     \n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.6927 - acc: 0.7410     \n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.5272 - acc: 0.8070     \n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.4025 - acc: 0.8620     \n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.3665 - acc: 0.8670     \n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.2474 - acc: 0.9260     \n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.2067 - acc: 0.9390     \n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1336 - acc: 0.9590     \n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1389 - acc: 0.9620     \n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1287 - acc: 0.9550     \n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0738 - acc: 0.9790     \n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0899 - acc: 0.9700     \n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.1030 - acc: 0.9690     \n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0480 - acc: 0.9870     \n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0492 - acc: 0.9870     \n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0534 - acc: 0.9840     \n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s - loss: 0.0580 - acc: 0.9820     \n",
      "CPU times: user 33.7 s, sys: 2.98 s, total: 36.7 s\n",
      "Wall time: 44.6 s\n",
      "1000 shot leaning, test on B task\n",
      "500/500 [==============================] - 14s    \n",
      "\n",
      "Test loss: 3.962\n",
      "Test accuracy: 0.510\n",
      "1000 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.282\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "# dream with less epoch on fewer shots to see if we can prevent task A from catastriphic forgetting\n",
    "from keras.optimizers import SGD, Adam\n",
    "model_whole = day()\n",
    "# test on yesterday episode -> totally forget -> shouldn't totally forget when few shot\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "nb_epoch = 30\n",
    "# nb_epoch = 10 is not enough to let 500 shot learning converge\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 500:\n",
    "        nb_epoch = 5\n",
    "    else:\n",
    "        nb_epoch = 30\n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    "    model_whole = day()\n",
    "    print(str(num_train_images) + \" shot leaning, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    # batch_size removed 15:00\n",
    "    # default adam 0.64\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment on having more epochs on later shots ( no dream )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 8s - loss: 1.9755 - acc: 0.3195 - val_loss: 1.7093 - val_acc: 0.2220\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2128 - acc: 0.4900 - val_loss: 1.6424 - val_acc: 0.3560\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0326 - acc: 0.5850 - val_loss: 1.6899 - val_acc: 0.4020\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8914 - acc: 0.6555 - val_loss: 1.9387 - val_acc: 0.3920\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7926 - acc: 0.7055 - val_loss: 1.8348 - val_acc: 0.4280\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7023 - acc: 0.7390 - val_loss: 1.7686 - val_acc: 0.5060\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6402 - acc: 0.7725 - val_loss: 1.8772 - val_acc: 0.4900\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5841 - acc: 0.7960 - val_loss: 1.8071 - val_acc: 0.5120\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5356 - acc: 0.8190 - val_loss: 1.8748 - val_acc: 0.4880\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4824 - acc: 0.8320 - val_loss: 1.7510 - val_acc: 0.5260\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4379 - acc: 0.8580 - val_loss: 1.7352 - val_acc: 0.5140\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3976 - acc: 0.8675 - val_loss: 1.6235 - val_acc: 0.5440\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3658 - acc: 0.8850 - val_loss: 1.5952 - val_acc: 0.5440\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3436 - acc: 0.8945 - val_loss: 1.5925 - val_acc: 0.5400\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3071 - acc: 0.9060 - val_loss: 1.5124 - val_acc: 0.5640\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3003 - acc: 0.9050 - val_loss: 1.4791 - val_acc: 0.5680\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2732 - acc: 0.9135 - val_loss: 1.4825 - val_acc: 0.5740\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2520 - acc: 0.9225 - val_loss: 1.4400 - val_acc: 0.5880\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2308 - acc: 0.9290 - val_loss: 1.4064 - val_acc: 0.5960\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2357 - acc: 0.9290 - val_loss: 1.4247 - val_acc: 0.6040\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2127 - acc: 0.9365 - val_loss: 1.4364 - val_acc: 0.6020\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1949 - acc: 0.9435 - val_loss: 1.3991 - val_acc: 0.6100\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2010 - acc: 0.9385 - val_loss: 1.3215 - val_acc: 0.6120\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1957 - acc: 0.9375 - val_loss: 1.3395 - val_acc: 0.6040\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1655 - acc: 0.9495 - val_loss: 1.3340 - val_acc: 0.6180\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1560 - acc: 0.9615 - val_loss: 1.3180 - val_acc: 0.6240\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1457 - acc: 0.9595 - val_loss: 1.3048 - val_acc: 0.6380\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1425 - acc: 0.9605 - val_loss: 1.2867 - val_acc: 0.6440\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1436 - acc: 0.9580 - val_loss: 1.2853 - val_acc: 0.6440\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1226 - acc: 0.9640 - val_loss: 1.2823 - val_acc: 0.6480\n",
      "CPU times: user 38 s, sys: 5.81 s, total: 43.8 s\n",
      "Wall time: 1min 2s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.201\n",
      "Test accuracy: 0.672\n",
      "zero shot learning\n",
      "zero shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 14.128\n",
      "Test accuracy: 0.000\n",
      "zero shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.201\n",
      "Test accuracy: 0.672\n",
      "1 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 8s - loss: 2.0173 - acc: 0.3260 - val_loss: 2.2380 - val_acc: 0.2300\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2305 - acc: 0.4770 - val_loss: 2.2707 - val_acc: 0.3260\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0859 - acc: 0.5525 - val_loss: 1.9275 - val_acc: 0.3760\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9707 - acc: 0.6085 - val_loss: 1.8646 - val_acc: 0.3900\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8797 - acc: 0.6565 - val_loss: 2.0085 - val_acc: 0.4020\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8053 - acc: 0.7015 - val_loss: 1.9225 - val_acc: 0.4260\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7260 - acc: 0.7270 - val_loss: 1.8035 - val_acc: 0.4460\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6740 - acc: 0.7605 - val_loss: 1.9203 - val_acc: 0.4440\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6132 - acc: 0.7875 - val_loss: 1.9537 - val_acc: 0.4500\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5736 - acc: 0.7980 - val_loss: 1.9441 - val_acc: 0.4560\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5283 - acc: 0.8165 - val_loss: 1.8290 - val_acc: 0.4660\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5021 - acc: 0.8245 - val_loss: 1.6836 - val_acc: 0.4840\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4735 - acc: 0.8375 - val_loss: 1.6255 - val_acc: 0.5120\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4365 - acc: 0.8570 - val_loss: 1.5295 - val_acc: 0.5420\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4151 - acc: 0.8550 - val_loss: 1.4570 - val_acc: 0.5640\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3998 - acc: 0.8650 - val_loss: 1.4172 - val_acc: 0.5740\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3581 - acc: 0.8815 - val_loss: 1.3648 - val_acc: 0.5820\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3344 - acc: 0.8905 - val_loss: 1.3108 - val_acc: 0.6040\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3199 - acc: 0.8985 - val_loss: 1.2400 - val_acc: 0.6220\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3002 - acc: 0.9095 - val_loss: 1.1775 - val_acc: 0.6420\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2879 - acc: 0.9105 - val_loss: 1.1779 - val_acc: 0.6480\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2575 - acc: 0.9160 - val_loss: 1.2036 - val_acc: 0.6500\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2500 - acc: 0.9165 - val_loss: 1.1927 - val_acc: 0.6580\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2679 - acc: 0.9130 - val_loss: 1.1681 - val_acc: 0.6540\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2362 - acc: 0.9235 - val_loss: 1.1484 - val_acc: 0.6640\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2044 - acc: 0.9405 - val_loss: 1.1646 - val_acc: 0.6740\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2088 - acc: 0.9375 - val_loss: 1.1887 - val_acc: 0.6760\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1911 - acc: 0.9465 - val_loss: 1.1957 - val_acc: 0.6720\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2004 - acc: 0.9330 - val_loss: 1.2082 - val_acc: 0.6840\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1760 - acc: 0.9460 - val_loss: 1.2481 - val_acc: 0.6780\n",
      "CPU times: user 38.8 s, sys: 5.95 s, total: 44.8 s\n",
      "Wall time: 1min 3s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.352\n",
      "Test accuracy: 0.632\n",
      "1 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 6s - loss: 12.4202 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s - loss: 8.6113 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s - loss: 6.1323 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s - loss: 4.1832 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s - loss: 1.5258 - acc: 0.0000e+00\n",
      "CPU times: user 10.6 s, sys: 89.1 ms, total: 10.7 s\n",
      "Wall time: 10.5 s\n",
      "1 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 8.547\n",
      "Test accuracy: 0.058\n",
      "1 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.027\n",
      "Test accuracy: 0.306\n",
      "5 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 9s - loss: 1.9157 - acc: 0.3285 - val_loss: 2.7409 - val_acc: 0.1960\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1276 - acc: 0.5495 - val_loss: 3.3958 - val_acc: 0.2080\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9699 - acc: 0.6330 - val_loss: 1.9395 - val_acc: 0.3860\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8657 - acc: 0.6715 - val_loss: 1.9553 - val_acc: 0.4300\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7386 - acc: 0.7330 - val_loss: 1.8985 - val_acc: 0.4100\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6699 - acc: 0.7580 - val_loss: 1.9802 - val_acc: 0.4600\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6019 - acc: 0.7860 - val_loss: 1.9823 - val_acc: 0.4700\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5542 - acc: 0.8120 - val_loss: 2.0793 - val_acc: 0.4700\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5101 - acc: 0.8355 - val_loss: 2.0688 - val_acc: 0.5040\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4863 - acc: 0.8360 - val_loss: 2.1013 - val_acc: 0.4720\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4321 - acc: 0.8595 - val_loss: 1.9782 - val_acc: 0.5100\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4008 - acc: 0.8725 - val_loss: 2.0281 - val_acc: 0.5020\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3805 - acc: 0.8860 - val_loss: 1.9474 - val_acc: 0.5180\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3509 - acc: 0.8955 - val_loss: 1.7589 - val_acc: 0.5540\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3284 - acc: 0.9035 - val_loss: 1.6921 - val_acc: 0.5600\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3109 - acc: 0.9045 - val_loss: 1.5895 - val_acc: 0.5760\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3111 - acc: 0.9045 - val_loss: 1.4964 - val_acc: 0.5940\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2754 - acc: 0.9115 - val_loss: 1.4744 - val_acc: 0.5980\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2518 - acc: 0.9265 - val_loss: 1.4002 - val_acc: 0.6160\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2404 - acc: 0.9310 - val_loss: 1.3419 - val_acc: 0.6300\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2272 - acc: 0.9285 - val_loss: 1.3207 - val_acc: 0.6500\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2185 - acc: 0.9350 - val_loss: 1.3143 - val_acc: 0.6540\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2187 - acc: 0.9340 - val_loss: 1.2919 - val_acc: 0.6620\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2013 - acc: 0.9425 - val_loss: 1.2686 - val_acc: 0.6720\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1896 - acc: 0.9445 - val_loss: 1.2402 - val_acc: 0.6860\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1936 - acc: 0.9435 - val_loss: 1.2236 - val_acc: 0.6900\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1855 - acc: 0.9445 - val_loss: 1.2253 - val_acc: 0.6860\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1613 - acc: 0.9530 - val_loss: 1.2214 - val_acc: 0.6940\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1712 - acc: 0.9550 - val_loss: 1.2365 - val_acc: 0.6960\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1626 - acc: 0.9580 - val_loss: 1.2610 - val_acc: 0.6880\n",
      "CPU times: user 39.4 s, sys: 5.98 s, total: 45.4 s\n",
      "Wall time: 1min 3s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.309\n",
      "Test accuracy: 0.646\n",
      "5 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 6s - loss: 13.0998 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s - loss: 11.9815 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s - loss: 8.7383 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s - loss: 5.8807 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s - loss: 3.7196 - acc: 0.0000e+00\n",
      "CPU times: user 9.51 s, sys: 82.8 ms, total: 9.59 s\n",
      "Wall time: 9.48 s\n",
      "5 shot leaning, test on B task\n",
      "500/500 [==============================] - 5s     \n",
      "\n",
      "Test loss: 5.651\n",
      "Test accuracy: 0.162\n",
      "5 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.233\n",
      "Test accuracy: 0.106\n",
      "10 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 9s - loss: 2.0211 - acc: 0.2920 - val_loss: 2.7420 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2107 - acc: 0.4890 - val_loss: 2.8740 - val_acc: 0.2380\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0468 - acc: 0.5710 - val_loss: 2.4556 - val_acc: 0.3400\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9206 - acc: 0.6250 - val_loss: 2.4906 - val_acc: 0.3780\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8216 - acc: 0.6765 - val_loss: 2.2139 - val_acc: 0.4380\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7687 - acc: 0.7095 - val_loss: 2.3723 - val_acc: 0.4480\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6863 - acc: 0.7475 - val_loss: 2.2036 - val_acc: 0.4680\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6496 - acc: 0.7650 - val_loss: 2.0472 - val_acc: 0.4640\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5905 - acc: 0.7925 - val_loss: 2.0378 - val_acc: 0.4860\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5536 - acc: 0.8070 - val_loss: 1.8715 - val_acc: 0.4980\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5125 - acc: 0.8215 - val_loss: 1.7436 - val_acc: 0.5180\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4930 - acc: 0.8290 - val_loss: 1.7245 - val_acc: 0.5400\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4486 - acc: 0.8520 - val_loss: 1.5280 - val_acc: 0.5700\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4435 - acc: 0.8500 - val_loss: 1.5280 - val_acc: 0.5720\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4069 - acc: 0.8675 - val_loss: 1.5187 - val_acc: 0.5920\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3952 - acc: 0.8780 - val_loss: 1.3921 - val_acc: 0.6120\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3454 - acc: 0.8930 - val_loss: 1.3072 - val_acc: 0.6280\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3343 - acc: 0.8945 - val_loss: 1.2586 - val_acc: 0.6420\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3099 - acc: 0.9030 - val_loss: 1.2347 - val_acc: 0.6420\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3061 - acc: 0.9020 - val_loss: 1.2259 - val_acc: 0.6440\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3050 - acc: 0.9105 - val_loss: 1.1965 - val_acc: 0.6440\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2873 - acc: 0.9115 - val_loss: 1.1989 - val_acc: 0.6400\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2691 - acc: 0.9160 - val_loss: 1.2109 - val_acc: 0.6420\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2685 - acc: 0.9195 - val_loss: 1.2165 - val_acc: 0.6520\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2494 - acc: 0.9280 - val_loss: 1.2227 - val_acc: 0.6520\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2373 - acc: 0.9325 - val_loss: 1.2062 - val_acc: 0.6620\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2207 - acc: 0.9345 - val_loss: 1.2226 - val_acc: 0.6640\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2229 - acc: 0.9380 - val_loss: 1.2307 - val_acc: 0.6660\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2070 - acc: 0.9395 - val_loss: 1.2702 - val_acc: 0.6500\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2030 - acc: 0.9425 - val_loss: 1.2871 - val_acc: 0.6500\n",
      "CPU times: user 40.1 s, sys: 5.96 s, total: 46.1 s\n",
      "Wall time: 1min 4s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.472\n",
      "Test accuracy: 0.608\n",
      "10 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 7s - loss: 13.4196 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s - loss: 11.3651 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s - loss: 8.0907 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s - loss: 5.2038 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s - loss: 3.1429 - acc: 0.0000e+00\n",
      "CPU times: user 10 s, sys: 71.5 ms, total: 10.1 s\n",
      "Wall time: 9.98 s\n",
      "10 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 4.951\n",
      "Test accuracy: 0.198\n",
      "10 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.814\n",
      "Test accuracy: 0.010\n",
      "15 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 10s - loss: 1.9942 - acc: 0.2870 - val_loss: 3.5330 - val_acc: 0.1940\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2732 - acc: 0.4600 - val_loss: 3.1410 - val_acc: 0.2440\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0772 - acc: 0.5560 - val_loss: 4.0231 - val_acc: 0.2360\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9394 - acc: 0.6445 - val_loss: 3.4106 - val_acc: 0.2640\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8200 - acc: 0.6950 - val_loss: 3.1163 - val_acc: 0.3100\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7354 - acc: 0.7275 - val_loss: 2.6966 - val_acc: 0.3680\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6682 - acc: 0.7555 - val_loss: 2.5633 - val_acc: 0.4200\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6258 - acc: 0.7920 - val_loss: 2.6254 - val_acc: 0.4200\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5766 - acc: 0.8055 - val_loss: 2.3338 - val_acc: 0.4640\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5182 - acc: 0.8300 - val_loss: 1.9878 - val_acc: 0.4980\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4940 - acc: 0.8350 - val_loss: 1.9752 - val_acc: 0.5120\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4388 - acc: 0.8580 - val_loss: 1.9635 - val_acc: 0.5180\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4147 - acc: 0.8695 - val_loss: 1.8829 - val_acc: 0.5360\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3917 - acc: 0.8765 - val_loss: 1.8542 - val_acc: 0.5460\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3621 - acc: 0.8915 - val_loss: 1.8794 - val_acc: 0.5360\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3503 - acc: 0.8865 - val_loss: 1.6084 - val_acc: 0.5740\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3268 - acc: 0.9060 - val_loss: 1.5590 - val_acc: 0.5800\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3129 - acc: 0.9090 - val_loss: 1.5111 - val_acc: 0.5820\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2893 - acc: 0.9095 - val_loss: 1.3930 - val_acc: 0.6180\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2720 - acc: 0.9135 - val_loss: 1.3857 - val_acc: 0.6260\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2584 - acc: 0.9255 - val_loss: 1.3563 - val_acc: 0.6320\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2420 - acc: 0.9280 - val_loss: 1.3570 - val_acc: 0.6380\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2282 - acc: 0.9355 - val_loss: 1.3480 - val_acc: 0.6400\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2177 - acc: 0.9350 - val_loss: 1.3625 - val_acc: 0.6420\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2051 - acc: 0.9410 - val_loss: 1.3581 - val_acc: 0.6380\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1988 - acc: 0.9415 - val_loss: 1.3535 - val_acc: 0.6380\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1895 - acc: 0.9415 - val_loss: 1.3407 - val_acc: 0.6420\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1858 - acc: 0.9420 - val_loss: 1.3428 - val_acc: 0.6400\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1844 - acc: 0.9435 - val_loss: 1.3765 - val_acc: 0.6260\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1696 - acc: 0.9530 - val_loss: 1.4234 - val_acc: 0.6240\n",
      "CPU times: user 40.7 s, sys: 5.95 s, total: 46.6 s\n",
      "Wall time: 1min 5s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.266\n",
      "Test accuracy: 0.642\n",
      "15 shot leaning, on second day task\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 7s - loss: 13.5139 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 0s - loss: 11.2987 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 0s - loss: 7.7899 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 0s - loss: 4.9169 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 0s - loss: 3.1254 - acc: 0.0000e+00\n",
      "CPU times: user 10.4 s, sys: 108 ms, total: 10.5 s\n",
      "Wall time: 10.4 s\n",
      "15 shot leaning, test on B task\n",
      "500/500 [==============================] - 6s     \n",
      "\n",
      "Test loss: 3.181\n",
      "Test accuracy: 0.196\n",
      "15 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.592\n",
      "Test accuracy: 0.018\n",
      "20 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 10s - loss: 2.0340 - acc: 0.2835 - val_loss: 3.8410 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2195 - acc: 0.4875 - val_loss: 3.6837 - val_acc: 0.1940\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0587 - acc: 0.5865 - val_loss: 4.0245 - val_acc: 0.1940\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9162 - acc: 0.6565 - val_loss: 3.8263 - val_acc: 0.2200\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8051 - acc: 0.7030 - val_loss: 2.9246 - val_acc: 0.2600\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7252 - acc: 0.7455 - val_loss: 2.7732 - val_acc: 0.3200\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6684 - acc: 0.7615 - val_loss: 2.4897 - val_acc: 0.3540\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6285 - acc: 0.7755 - val_loss: 2.1762 - val_acc: 0.4280\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5586 - acc: 0.8170 - val_loss: 1.9778 - val_acc: 0.4620\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5245 - acc: 0.8230 - val_loss: 1.8223 - val_acc: 0.4880\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4737 - acc: 0.8465 - val_loss: 1.6399 - val_acc: 0.5420\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4394 - acc: 0.8670 - val_loss: 1.5175 - val_acc: 0.5800\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4189 - acc: 0.8660 - val_loss: 1.3956 - val_acc: 0.6040\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3993 - acc: 0.8720 - val_loss: 1.3294 - val_acc: 0.6120\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3743 - acc: 0.8825 - val_loss: 1.2507 - val_acc: 0.6300\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3343 - acc: 0.8910 - val_loss: 1.2167 - val_acc: 0.6380\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3203 - acc: 0.9035 - val_loss: 1.1956 - val_acc: 0.6600\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2952 - acc: 0.9125 - val_loss: 1.1876 - val_acc: 0.6520\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2879 - acc: 0.9160 - val_loss: 1.1843 - val_acc: 0.6620\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2758 - acc: 0.9170 - val_loss: 1.1740 - val_acc: 0.6580\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2473 - acc: 0.9270 - val_loss: 1.1756 - val_acc: 0.6760\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2310 - acc: 0.9360 - val_loss: 1.1976 - val_acc: 0.6660\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2192 - acc: 0.9405 - val_loss: 1.2116 - val_acc: 0.6700\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2082 - acc: 0.9390 - val_loss: 1.2360 - val_acc: 0.6700\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2057 - acc: 0.9370 - val_loss: 1.2505 - val_acc: 0.6700\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2034 - acc: 0.9415 - val_loss: 1.2287 - val_acc: 0.6760\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1944 - acc: 0.9510 - val_loss: 1.2227 - val_acc: 0.6680\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1780 - acc: 0.9475 - val_loss: 1.2317 - val_acc: 0.6680\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1770 - acc: 0.9540 - val_loss: 1.2435 - val_acc: 0.6660\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1565 - acc: 0.9565 - val_loss: 1.2556 - val_acc: 0.6700\n",
      "CPU times: user 43.2 s, sys: 5.86 s, total: 49.1 s\n",
      "Wall time: 1min 7s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.215\n",
      "Test accuracy: 0.668\n",
      "20 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 8s - loss: 13.1226 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s - loss: 10.4753 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s - loss: 7.3702 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s - loss: 4.9473 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s - loss: 3.3133 - acc: 0.1000\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s - loss: 2.4480 - acc: 0.4000\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s - loss: 2.0864 - acc: 0.4000\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s - loss: 1.8969 - acc: 0.4000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s - loss: 1.6878 - acc: 0.4500\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s - loss: 1.6024 - acc: 0.4000\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s - loss: 1.5575 - acc: 0.4500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s - loss: 1.4639 - acc: 0.4500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s - loss: 1.3169 - acc: 0.5500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s - loss: 1.1974 - acc: 0.4500\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s - loss: 1.1609 - acc: 0.4000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s - loss: 1.2064 - acc: 0.5500\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s - loss: 0.9788 - acc: 0.5500\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s - loss: 0.8493 - acc: 0.8000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s - loss: 0.7482 - acc: 0.7500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s - loss: 0.7516 - acc: 0.6500\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s - loss: 0.6887 - acc: 0.7000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s - loss: 0.6802 - acc: 0.6500\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s - loss: 0.5979 - acc: 0.7000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s - loss: 0.4617 - acc: 0.9000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s - loss: 0.4573 - acc: 0.8500\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s - loss: 0.4007 - acc: 0.8500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s - loss: 0.3362 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s - loss: 0.2813 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s - loss: 0.2292 - acc: 0.9500\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s - loss: 0.1946 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s - loss: 0.2014 - acc: 0.9500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s - loss: 0.1143 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s - loss: 0.0894 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s - loss: 0.0439 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s - loss: 0.0382 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s - loss: 0.0340 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s - loss: 0.0016 - acc: 1.0000\n",
      "CPU times: user 12.6 s, sys: 214 ms, total: 12.9 s\n",
      "Wall time: 12.5 s\n",
      "20 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 8.801\n",
      "Test accuracy: 0.200\n",
      "20 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.048\n",
      "Test accuracy: 0.000\n",
      "100 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 11s - loss: 2.0300 - acc: 0.2960 - val_loss: 3.2700 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1872 - acc: 0.4930 - val_loss: 4.3212 - val_acc: 0.1920\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0010 - acc: 0.6175 - val_loss: 4.2552 - val_acc: 0.1940\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8695 - acc: 0.6765 - val_loss: 4.0989 - val_acc: 0.2000\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7726 - acc: 0.7295 - val_loss: 3.5878 - val_acc: 0.2540\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6887 - acc: 0.7565 - val_loss: 3.0616 - val_acc: 0.3120\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6472 - acc: 0.7785 - val_loss: 2.8361 - val_acc: 0.3440\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5860 - acc: 0.7990 - val_loss: 2.4436 - val_acc: 0.4020\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5398 - acc: 0.8235 - val_loss: 2.0928 - val_acc: 0.4340\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4900 - acc: 0.8390 - val_loss: 1.8860 - val_acc: 0.4720\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4395 - acc: 0.8540 - val_loss: 1.7529 - val_acc: 0.5020\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4056 - acc: 0.8730 - val_loss: 1.6024 - val_acc: 0.5480\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3902 - acc: 0.8765 - val_loss: 1.5205 - val_acc: 0.5700\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3444 - acc: 0.8970 - val_loss: 1.5710 - val_acc: 0.5780\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3328 - acc: 0.8955 - val_loss: 1.5188 - val_acc: 0.5840\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3041 - acc: 0.9090 - val_loss: 1.3978 - val_acc: 0.6100\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3080 - acc: 0.9005 - val_loss: 1.3624 - val_acc: 0.6360\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2650 - acc: 0.9180 - val_loss: 1.4513 - val_acc: 0.6220\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2514 - acc: 0.9250 - val_loss: 1.3970 - val_acc: 0.6320\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2340 - acc: 0.9340 - val_loss: 1.3436 - val_acc: 0.6360\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2131 - acc: 0.9375 - val_loss: 1.3123 - val_acc: 0.6440\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2083 - acc: 0.9365 - val_loss: 1.3289 - val_acc: 0.6480\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1924 - acc: 0.9430 - val_loss: 1.3295 - val_acc: 0.6460\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1897 - acc: 0.9425 - val_loss: 1.3229 - val_acc: 0.6460\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1848 - acc: 0.9405 - val_loss: 1.3497 - val_acc: 0.6480\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1868 - acc: 0.9445 - val_loss: 1.3766 - val_acc: 0.6460\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1670 - acc: 0.9565 - val_loss: 1.3888 - val_acc: 0.6380\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1599 - acc: 0.9530 - val_loss: 1.4097 - val_acc: 0.6560\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1444 - acc: 0.9565 - val_loss: 1.4277 - val_acc: 0.6560\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1403 - acc: 0.9605 - val_loss: 1.4101 - val_acc: 0.6540\n",
      "CPU times: user 41.6 s, sys: 6 s, total: 47.6 s\n",
      "Wall time: 1min 6s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.517\n",
      "Test accuracy: 0.628\n",
      "100 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 8s - loss: 13.9233 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s - loss: 10.3663 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s - loss: 6.9070 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s - loss: 4.3914 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s - loss: 2.8755 - acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s - loss: 2.1541 - acc: 0.3000\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s - loss: 1.8593 - acc: 0.2600\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s - loss: 1.7039 - acc: 0.2700\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s - loss: 1.5953 - acc: 0.3200\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s - loss: 1.6001 - acc: 0.3300\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s - loss: 1.5308 - acc: 0.3400\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s - loss: 1.5296 - acc: 0.2900\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s - loss: 1.4548 - acc: 0.3800\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s - loss: 1.4528 - acc: 0.3500\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s - loss: 1.4921 - acc: 0.2800\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s - loss: 1.4470 - acc: 0.3400\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s - loss: 1.3887 - acc: 0.4200\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s - loss: 1.3450 - acc: 0.4300\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s - loss: 1.3498 - acc: 0.3300\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s - loss: 1.2441 - acc: 0.4600\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s - loss: 1.3031 - acc: 0.3800\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s - loss: 1.2092 - acc: 0.3900\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s - loss: 1.1620 - acc: 0.4000\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s - loss: 1.1338 - acc: 0.4900\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s - loss: 1.1052 - acc: 0.4700\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s - loss: 1.0469 - acc: 0.4400\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s - loss: 1.0462 - acc: 0.4400\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s - loss: 1.0143 - acc: 0.4500\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s - loss: 0.9549 - acc: 0.5100\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s - loss: 0.9345 - acc: 0.4800\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s - loss: 0.8019 - acc: 0.6400\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s - loss: 0.8524 - acc: 0.6000\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s - loss: 0.8484 - acc: 0.6100\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s - loss: 0.7675 - acc: 0.6500\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s - loss: 0.6904 - acc: 0.7600\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s - loss: 0.6322 - acc: 0.7700\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s - loss: 0.5242 - acc: 0.8300\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s - loss: 0.5241 - acc: 0.8300\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s - loss: 0.4458 - acc: 0.9100\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s - loss: 0.3443 - acc: 0.9300\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s - loss: 0.3150 - acc: 0.9500\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s - loss: 0.2673 - acc: 0.9500\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s - loss: 0.1691 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s - loss: 0.1336 - acc: 0.9900\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s - loss: 0.1212 - acc: 0.9800\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s - loss: 0.0827 - acc: 0.9900\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s - loss: 0.0695 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s - loss: 0.0391 - acc: 0.9900\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s - loss: 0.0280 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s - loss: 0.0193 - acc: 1.0000\n",
      "CPU times: user 14.4 s, sys: 541 ms, total: 15 s\n",
      "Wall time: 15.8 s\n",
      "100 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 6.154\n",
      "Test accuracy: 0.216\n",
      "100 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.866\n",
      "Test accuracy: 0.000\n",
      "200 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 11s - loss: 2.1481 - acc: 0.2210 - val_loss: 2.2906 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.3546 - acc: 0.4225 - val_loss: 3.0904 - val_acc: 0.2040\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1731 - acc: 0.5190 - val_loss: 3.2058 - val_acc: 0.2440\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0555 - acc: 0.5705 - val_loss: 2.6121 - val_acc: 0.2820\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9441 - acc: 0.6330 - val_loss: 2.0059 - val_acc: 0.3760\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8573 - acc: 0.6660 - val_loss: 2.2533 - val_acc: 0.3680\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7757 - acc: 0.7045 - val_loss: 2.0569 - val_acc: 0.4100\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7221 - acc: 0.7435 - val_loss: 2.1244 - val_acc: 0.4260\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6418 - acc: 0.7760 - val_loss: 1.9741 - val_acc: 0.4560\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6150 - acc: 0.7790 - val_loss: 1.8302 - val_acc: 0.4820\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5797 - acc: 0.8020 - val_loss: 1.8032 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5438 - acc: 0.8065 - val_loss: 1.6451 - val_acc: 0.5100\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5022 - acc: 0.8320 - val_loss: 1.5521 - val_acc: 0.5220\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4606 - acc: 0.8470 - val_loss: 1.4375 - val_acc: 0.5560\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4361 - acc: 0.8605 - val_loss: 1.4432 - val_acc: 0.5660\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3938 - acc: 0.8730 - val_loss: 1.4371 - val_acc: 0.5740\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3894 - acc: 0.8730 - val_loss: 1.4616 - val_acc: 0.5720\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3552 - acc: 0.8875 - val_loss: 1.4027 - val_acc: 0.5860\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3475 - acc: 0.8915 - val_loss: 1.3927 - val_acc: 0.5780\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3184 - acc: 0.9025 - val_loss: 1.4038 - val_acc: 0.5900\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3060 - acc: 0.9060 - val_loss: 1.4132 - val_acc: 0.5960\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3022 - acc: 0.9050 - val_loss: 1.4053 - val_acc: 0.6020\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2871 - acc: 0.9020 - val_loss: 1.4278 - val_acc: 0.6120\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2624 - acc: 0.9210 - val_loss: 1.4316 - val_acc: 0.6180\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2536 - acc: 0.9200 - val_loss: 1.4937 - val_acc: 0.6020\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2533 - acc: 0.9265 - val_loss: 1.4811 - val_acc: 0.5940\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2497 - acc: 0.9235 - val_loss: 1.4561 - val_acc: 0.6120\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2397 - acc: 0.9290 - val_loss: 1.5091 - val_acc: 0.6020\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2210 - acc: 0.9300 - val_loss: 1.5704 - val_acc: 0.5840\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2091 - acc: 0.9400 - val_loss: 1.5910 - val_acc: 0.5860\n",
      "CPU times: user 42.1 s, sys: 6.15 s, total: 48.2 s\n",
      "Wall time: 1min 6s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.566\n",
      "Test accuracy: 0.586\n",
      "200 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 9s - loss: 14.2660 - acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s - loss: 10.8559 - acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s - loss: 7.2101 - acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s - loss: 4.6118 - acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s - loss: 3.0530 - acc: 0.0350\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s - loss: 2.3307 - acc: 0.2650\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s - loss: 1.9821 - acc: 0.2400\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s - loss: 1.7169 - acc: 0.3000\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s - loss: 1.6578 - acc: 0.3100\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s - loss: 1.6447 - acc: 0.2650\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s - loss: 1.6442 - acc: 0.2350\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s - loss: 1.5855 - acc: 0.2450\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s - loss: 1.5709 - acc: 0.2900\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s - loss: 1.5441 - acc: 0.3050\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s - loss: 1.5294 - acc: 0.3400\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s - loss: 1.5102 - acc: 0.3550\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s - loss: 1.4731 - acc: 0.3400\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s - loss: 1.4272 - acc: 0.3650\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s - loss: 1.3865 - acc: 0.4100\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s - loss: 1.3393 - acc: 0.4750\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s - loss: 1.2668 - acc: 0.4700\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s - loss: 1.2247 - acc: 0.5300\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s - loss: 1.2101 - acc: 0.4900\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s - loss: 1.0947 - acc: 0.6200\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s - loss: 0.9803 - acc: 0.7000\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s - loss: 0.8667 - acc: 0.7400\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s - loss: 0.7499 - acc: 0.7900\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s - loss: 0.7040 - acc: 0.7750\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s - loss: 0.6036 - acc: 0.8100\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s - loss: 0.5242 - acc: 0.8350\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s - loss: 0.4413 - acc: 0.8950\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s - loss: 0.3921 - acc: 0.9000\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s - loss: 0.3028 - acc: 0.9250\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s - loss: 0.2693 - acc: 0.9300\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s - loss: 0.2142 - acc: 0.9650\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s - loss: 0.1632 - acc: 0.9800\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s - loss: 0.1255 - acc: 0.9750\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s - loss: 0.1125 - acc: 0.9850\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s - loss: 0.1191 - acc: 0.9700\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s - loss: 0.0757 - acc: 0.9800\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s - loss: 0.0702 - acc: 0.9900\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s - loss: 0.0582 - acc: 0.9900\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s - loss: 0.0409 - acc: 0.9850\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s - loss: 0.0414 - acc: 0.9850\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s - loss: 0.0284 - acc: 0.9900\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s - loss: 0.0085 - acc: 0.9950\n",
      "CPU times: user 16.6 s, sys: 1.08 s, total: 17.7 s\n",
      "Wall time: 20 s\n",
      "200 shot leaning, test on B task\n",
      "500/500 [==============================] - 7s     \n",
      "\n",
      "Test loss: 8.017\n",
      "Test accuracy: 0.208\n",
      "200 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.111\n",
      "Test accuracy: 0.000\n",
      "300 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 12s - loss: 1.9968 - acc: 0.2905 - val_loss: 3.1961 - val_acc: 0.1920\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1598 - acc: 0.5265 - val_loss: 3.2735 - val_acc: 0.1940\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9960 - acc: 0.6030 - val_loss: 4.0913 - val_acc: 0.2020\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8933 - acc: 0.6495 - val_loss: 4.0935 - val_acc: 0.2040\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7888 - acc: 0.7065 - val_loss: 3.5785 - val_acc: 0.2180\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7208 - acc: 0.7320 - val_loss: 3.4740 - val_acc: 0.2400\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6586 - acc: 0.7805 - val_loss: 3.0881 - val_acc: 0.2660\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5943 - acc: 0.7925 - val_loss: 2.5285 - val_acc: 0.3340\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5582 - acc: 0.8105 - val_loss: 2.1649 - val_acc: 0.4040\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5206 - acc: 0.8225 - val_loss: 1.7641 - val_acc: 0.4660\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4979 - acc: 0.8320 - val_loss: 1.5159 - val_acc: 0.5120\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4501 - acc: 0.8570 - val_loss: 1.4149 - val_acc: 0.5600\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4140 - acc: 0.8750 - val_loss: 1.3398 - val_acc: 0.5800\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3880 - acc: 0.8725 - val_loss: 1.2491 - val_acc: 0.6120\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3705 - acc: 0.8780 - val_loss: 1.2208 - val_acc: 0.6200\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3426 - acc: 0.8935 - val_loss: 1.1775 - val_acc: 0.6280\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3226 - acc: 0.9100 - val_loss: 1.1231 - val_acc: 0.6440\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3192 - acc: 0.9035 - val_loss: 1.1334 - val_acc: 0.6580\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2962 - acc: 0.9110 - val_loss: 1.1531 - val_acc: 0.6640\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2815 - acc: 0.9175 - val_loss: 1.1471 - val_acc: 0.6560\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2796 - acc: 0.9215 - val_loss: 1.1562 - val_acc: 0.6640\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2585 - acc: 0.9250 - val_loss: 1.1634 - val_acc: 0.6640\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2436 - acc: 0.9365 - val_loss: 1.1708 - val_acc: 0.6700\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2535 - acc: 0.9270 - val_loss: 1.1828 - val_acc: 0.6800\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2261 - acc: 0.9345 - val_loss: 1.1736 - val_acc: 0.6820\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2225 - acc: 0.9410 - val_loss: 1.1788 - val_acc: 0.6860\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2282 - acc: 0.9330 - val_loss: 1.1869 - val_acc: 0.6820\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2136 - acc: 0.9395 - val_loss: 1.1915 - val_acc: 0.6820\n",
      "CPU times: user 40.7 s, sys: 5.66 s, total: 46.4 s\n",
      "Wall time: 1min 3s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.224\n",
      "Test accuracy: 0.650\n",
      "300 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 9s - loss: 12.7787 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s - loss: 6.1864 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s - loss: 2.7863 - acc: 0.0433     \n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s - loss: 1.9010 - acc: 0.2467     \n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s - loss: 1.6878 - acc: 0.2267     \n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s - loss: 1.6279 - acc: 0.2500     \n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s - loss: 1.6078 - acc: 0.2733     \n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s - loss: 1.5599 - acc: 0.3000     \n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s - loss: 1.5792 - acc: 0.2767     \n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s - loss: 1.5064 - acc: 0.3333     \n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s - loss: 1.4386 - acc: 0.3600     \n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s - loss: 1.4351 - acc: 0.3367     \n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s - loss: 1.3387 - acc: 0.3600     \n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s - loss: 1.3195 - acc: 0.3667     \n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s - loss: 1.2625 - acc: 0.4200     \n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s - loss: 1.1271 - acc: 0.4833     \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s - loss: 1.0963 - acc: 0.4867     \n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s - loss: 1.0391 - acc: 0.4833     \n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s - loss: 1.0065 - acc: 0.5467     \n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s - loss: 0.8970 - acc: 0.6100     \n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s - loss: 0.7779 - acc: 0.7067     \n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s - loss: 0.7069 - acc: 0.7467     \n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s - loss: 0.5817 - acc: 0.8233     \n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s - loss: 0.4796 - acc: 0.8333     \n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s - loss: 0.3843 - acc: 0.8667     \n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s - loss: 0.3447 - acc: 0.9000     \n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s - loss: 0.3070 - acc: 0.9033     \n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s - loss: 0.3416 - acc: 0.9167     \n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s - loss: 0.2631 - acc: 0.9233     \n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s - loss: 0.1479 - acc: 0.9567     \n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s - loss: 0.1800 - acc: 0.9433     \n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s - loss: 0.1843 - acc: 0.9367     \n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s - loss: 0.1276 - acc: 0.9533     \n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s - loss: 0.1592 - acc: 0.9533     \n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s - loss: 0.1432 - acc: 0.9533     \n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s - loss: 0.1389 - acc: 0.9700     \n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s - loss: 0.1135 - acc: 0.9667     \n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s - loss: 0.1272 - acc: 0.9567     \n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s - loss: 0.1170 - acc: 0.9633     \n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s - loss: 0.0763 - acc: 0.9800     \n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s - loss: 0.1578 - acc: 0.9567     \n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s - loss: 0.1330 - acc: 0.9467     \n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s - loss: 0.0666 - acc: 0.9900     \n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s - loss: 0.1516 - acc: 0.9667     \n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s - loss: 0.1366 - acc: 0.9700     \n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s - loss: 0.0872 - acc: 0.9700     \n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s - loss: 0.1519 - acc: 0.9567     \n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s - loss: 0.1888 - acc: 0.9433     \n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s - loss: 0.1158 - acc: 0.9633     \n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s - loss: 0.1207 - acc: 0.9633     \n",
      "CPU times: user 20.3 s, sys: 1.6 s, total: 21.9 s\n",
      "Wall time: 25.2 s\n",
      "300 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 2.670\n",
      "Test accuracy: 0.566\n",
      "300 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.565\n",
      "Test accuracy: 0.000\n",
      "400 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 12s - loss: 2.0028 - acc: 0.3080 - val_loss: 2.5355 - val_acc: 0.1980\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.1432 - acc: 0.5285 - val_loss: 2.8145 - val_acc: 0.2160\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9292 - acc: 0.6530 - val_loss: 3.2235 - val_acc: 0.2200\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8068 - acc: 0.7055 - val_loss: 2.7803 - val_acc: 0.2460\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7219 - acc: 0.7320 - val_loss: 2.2644 - val_acc: 0.3700\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6397 - acc: 0.7665 - val_loss: 2.1752 - val_acc: 0.4380\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5783 - acc: 0.8030 - val_loss: 2.1655 - val_acc: 0.4400\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5184 - acc: 0.8275 - val_loss: 2.0353 - val_acc: 0.4640\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4889 - acc: 0.8320 - val_loss: 1.7650 - val_acc: 0.5100\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4446 - acc: 0.8550 - val_loss: 1.6724 - val_acc: 0.5300\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4061 - acc: 0.8715 - val_loss: 1.5863 - val_acc: 0.5480\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3611 - acc: 0.8885 - val_loss: 1.5244 - val_acc: 0.5740\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3376 - acc: 0.8905 - val_loss: 1.4778 - val_acc: 0.5900\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3048 - acc: 0.9015 - val_loss: 1.5153 - val_acc: 0.5860\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2831 - acc: 0.9225 - val_loss: 1.4803 - val_acc: 0.5860\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2689 - acc: 0.9205 - val_loss: 1.4528 - val_acc: 0.6120\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2471 - acc: 0.9280 - val_loss: 1.4891 - val_acc: 0.5900\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2350 - acc: 0.9295 - val_loss: 1.4648 - val_acc: 0.6040\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2078 - acc: 0.9440 - val_loss: 1.4497 - val_acc: 0.6160\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2022 - acc: 0.9380 - val_loss: 1.4400 - val_acc: 0.6160\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1853 - acc: 0.9470 - val_loss: 1.4533 - val_acc: 0.6300\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1937 - acc: 0.9450 - val_loss: 1.4373 - val_acc: 0.6280\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1682 - acc: 0.9525 - val_loss: 1.4150 - val_acc: 0.6280\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1553 - acc: 0.9575 - val_loss: 1.3669 - val_acc: 0.6340\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1510 - acc: 0.9545 - val_loss: 1.3781 - val_acc: 0.6440\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1531 - acc: 0.9560 - val_loss: 1.3784 - val_acc: 0.6560\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1481 - acc: 0.9580 - val_loss: 1.3794 - val_acc: 0.6480\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1290 - acc: 0.9630 - val_loss: 1.3802 - val_acc: 0.6420\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1359 - acc: 0.9640 - val_loss: 1.4043 - val_acc: 0.6440\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1287 - acc: 0.9640 - val_loss: 1.4153 - val_acc: 0.6480\n",
      "CPU times: user 43.2 s, sys: 5.94 s, total: 49.1 s\n",
      "Wall time: 1min 7s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.300\n",
      "Test accuracy: 0.660\n",
      "400 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "400/400 [==============================] - 10s - loss: 13.1206 - acc: 0.0000e+00    \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 0s - loss: 5.9492 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 0s - loss: 2.5985 - acc: 0.0850     \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 0s - loss: 1.8208 - acc: 0.2225     \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 0s - loss: 1.6070 - acc: 0.2925     \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 0s - loss: 1.5583 - acc: 0.3100     \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 0s - loss: 1.5441 - acc: 0.3150     \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 0s - loss: 1.4961 - acc: 0.3100     \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 0s - loss: 1.4188 - acc: 0.3800     \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 0s - loss: 1.3639 - acc: 0.4025     \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 0s - loss: 1.2783 - acc: 0.4650     \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 0s - loss: 1.1751 - acc: 0.4925     \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 0s - loss: 1.0635 - acc: 0.5450     \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 0s - loss: 1.0166 - acc: 0.5575     \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 0s - loss: 0.8812 - acc: 0.6350     \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 0s - loss: 0.8098 - acc: 0.6725     \n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 0s - loss: 0.7406 - acc: 0.6825     \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 0s - loss: 0.6715 - acc: 0.6950     \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 0s - loss: 0.6088 - acc: 0.7250     \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 0s - loss: 0.5478 - acc: 0.7725     \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 0s - loss: 0.4480 - acc: 0.8625     \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 0s - loss: 0.4024 - acc: 0.8600     \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 0s - loss: 0.3419 - acc: 0.8875     \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 0s - loss: 0.2991 - acc: 0.9175     \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 0s - loss: 0.2468 - acc: 0.9325     \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 0s - loss: 0.1945 - acc: 0.9525     \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 0s - loss: 0.1619 - acc: 0.9525     \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 0s - loss: 0.1344 - acc: 0.9725     \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 0s - loss: 0.0898 - acc: 0.9825     \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 0s - loss: 0.0909 - acc: 0.9750     \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 0s - loss: 0.0462 - acc: 0.9875     \n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 0s - loss: 0.0505 - acc: 0.9950     \n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 0s - loss: 0.0380 - acc: 0.9875     \n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 0s - loss: 0.0564 - acc: 0.9825     \n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 0s - loss: 0.0516 - acc: 0.9900     \n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 0s - loss: 0.0252 - acc: 0.9925     \n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 0s - loss: 0.0414 - acc: 0.9900     \n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 0s - loss: 0.0363 - acc: 0.9875     \n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 0s - loss: 0.0605 - acc: 0.9750     \n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 0s - loss: 0.0346 - acc: 0.9875     \n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 0s - loss: 0.0369 - acc: 0.9850     \n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 0s - loss: 0.0342 - acc: 0.9875     \n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 0s - loss: 0.0972 - acc: 0.9600     \n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 0s - loss: 0.0625 - acc: 0.9875     \n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 0s - loss: 0.0187 - acc: 0.9925     \n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 0s - loss: 0.1059 - acc: 0.9750     \n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 0s - loss: 0.0638 - acc: 0.9750     \n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 0s - loss: 0.0460 - acc: 0.9825     \n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 0s - loss: 0.0461 - acc: 0.9875     \n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 0s - loss: 0.0909 - acc: 0.9675     \n",
      "CPU times: user 25.1 s, sys: 2.02 s, total: 27.1 s\n",
      "Wall time: 31.9 s\n",
      "400 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 4.334\n",
      "Test accuracy: 0.514\n",
      "400 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.074\n",
      "Test accuracy: 0.000\n",
      "500 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 13s - loss: 2.1389 - acc: 0.3060 - val_loss: 2.3285 - val_acc: 0.2020\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2554 - acc: 0.4700 - val_loss: 4.0861 - val_acc: 0.2000\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0651 - acc: 0.5795 - val_loss: 3.5327 - val_acc: 0.2280\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9522 - acc: 0.6340 - val_loss: 2.9832 - val_acc: 0.3000\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8504 - acc: 0.6790 - val_loss: 2.7853 - val_acc: 0.3220\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7780 - acc: 0.7190 - val_loss: 2.8281 - val_acc: 0.3500\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6926 - acc: 0.7610 - val_loss: 1.9579 - val_acc: 0.4460\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6502 - acc: 0.7600 - val_loss: 1.6974 - val_acc: 0.4800\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5823 - acc: 0.7965 - val_loss: 1.6784 - val_acc: 0.4840\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5467 - acc: 0.8195 - val_loss: 1.4572 - val_acc: 0.5200\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5123 - acc: 0.8220 - val_loss: 1.4558 - val_acc: 0.5140\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4815 - acc: 0.8340 - val_loss: 1.4315 - val_acc: 0.5400\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4386 - acc: 0.8545 - val_loss: 1.3691 - val_acc: 0.5540\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3972 - acc: 0.8730 - val_loss: 1.3119 - val_acc: 0.5580\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3739 - acc: 0.8765 - val_loss: 1.2960 - val_acc: 0.5660\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3295 - acc: 0.9000 - val_loss: 1.3560 - val_acc: 0.5720\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3088 - acc: 0.9065 - val_loss: 1.3594 - val_acc: 0.5680\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3123 - acc: 0.9035 - val_loss: 1.3714 - val_acc: 0.5780\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2714 - acc: 0.9175 - val_loss: 1.3667 - val_acc: 0.5920\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2658 - acc: 0.9195 - val_loss: 1.3757 - val_acc: 0.5980\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2727 - acc: 0.9115 - val_loss: 1.4262 - val_acc: 0.5720\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2355 - acc: 0.9235 - val_loss: 1.4403 - val_acc: 0.5900\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2187 - acc: 0.9365 - val_loss: 1.4127 - val_acc: 0.5940\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2124 - acc: 0.9365 - val_loss: 1.3984 - val_acc: 0.5960\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1963 - acc: 0.9435 - val_loss: 1.3976 - val_acc: 0.6020\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1898 - acc: 0.9430 - val_loss: 1.4171 - val_acc: 0.6040\n",
      "CPU times: user 39.8 s, sys: 5.28 s, total: 45.1 s\n",
      "Wall time: 1min 1s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.469\n",
      "Test accuracy: 0.576\n",
      "500 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 10s - loss: 11.8804 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 0s - loss: 5.3369 - acc: 0.0000e+00     \n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 0s - loss: 2.6472 - acc: 0.1340         \n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 0s - loss: 1.8682 - acc: 0.2700     \n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 0s - loss: 1.7059 - acc: 0.2260     \n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 0s - loss: 1.6478 - acc: 0.2300     \n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 0s - loss: 1.6459 - acc: 0.2480     \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 0s - loss: 1.6113 - acc: 0.3000     \n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 0s - loss: 1.6200 - acc: 0.2860     \n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 0s - loss: 1.5615 - acc: 0.3220     \n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 0s - loss: 1.5452 - acc: 0.3080     \n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 0s - loss: 1.4834 - acc: 0.3820     \n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 0s - loss: 1.4146 - acc: 0.3880     \n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 0s - loss: 1.3552 - acc: 0.4100     \n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 0s - loss: 1.2947 - acc: 0.4180     \n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 0s - loss: 1.2233 - acc: 0.4340     \n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 0s - loss: 1.1193 - acc: 0.4760     \n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 0s - loss: 1.0667 - acc: 0.4960     \n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 0s - loss: 0.9932 - acc: 0.5460     \n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 0s - loss: 0.9205 - acc: 0.6320     \n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 0s - loss: 0.8229 - acc: 0.6520     \n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 0s - loss: 0.7186 - acc: 0.6860     \n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 0s - loss: 0.6910 - acc: 0.7180     \n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 0s - loss: 0.5631 - acc: 0.7580     \n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 0s - loss: 0.5056 - acc: 0.7700     \n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 0s - loss: 0.5299 - acc: 0.7300     \n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 0s - loss: 0.5169 - acc: 0.7980     \n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 0s - loss: 0.4834 - acc: 0.8180     \n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 0s - loss: 0.3667 - acc: 0.8520     \n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 0s - loss: 0.3369 - acc: 0.8560     \n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 0s - loss: 0.3454 - acc: 0.8860     \n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 0s - loss: 0.2723 - acc: 0.9020     \n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 0s - loss: 0.1922 - acc: 0.9220     \n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 0s - loss: 0.1554 - acc: 0.9520     \n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 0s - loss: 0.1467 - acc: 0.9420     \n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 0s - loss: 0.0760 - acc: 0.9840     \n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 0s - loss: 0.0546 - acc: 0.9860     \n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 0s - loss: 0.0274 - acc: 0.9920     \n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 0s - loss: 0.0244 - acc: 0.9960     \n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 0s - loss: 0.0231 - acc: 0.9940     \n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 0s - loss: 0.0111 - acc: 0.9980     \n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 0s - loss: 0.0428 - acc: 0.9880     \n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 0s - loss: 0.0217 - acc: 0.9960     \n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 0s - loss: 0.0225 - acc: 0.9920     \n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 0s - loss: 0.0560 - acc: 0.9820     \n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 0s - loss: 0.0357 - acc: 0.9880     \n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 0s - loss: 0.0281 - acc: 0.9920     \n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 0s - loss: 0.0345 - acc: 0.9900     \n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 0s - loss: 0.0228 - acc: 0.9900     \n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 0s - loss: 0.0269 - acc: 0.9940     \n",
      "CPU times: user 24.7 s, sys: 2.59 s, total: 27.3 s\n",
      "Wall time: 33.7 s\n",
      "500 shot leaning, test on B task\n",
      "500/500 [==============================] - 8s     \n",
      "\n",
      "Test loss: 4.094\n",
      "Test accuracy: 0.566\n",
      "500 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.061\n",
      "Test accuracy: 0.000\n",
      "1000 shot leaning, day time\n",
      "day time: \n",
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 13s - loss: 2.0196 - acc: 0.2680 - val_loss: 2.0268 - val_acc: 0.2120\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.2209 - acc: 0.4940 - val_loss: 2.2697 - val_acc: 0.2800\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.0164 - acc: 0.5860 - val_loss: 1.7998 - val_acc: 0.3880\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.8988 - acc: 0.6565 - val_loss: 1.8697 - val_acc: 0.3920\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7841 - acc: 0.7110 - val_loss: 2.1245 - val_acc: 0.3900\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6925 - acc: 0.7480 - val_loss: 2.2710 - val_acc: 0.3960\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6381 - acc: 0.7810 - val_loss: 2.2852 - val_acc: 0.4100\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5820 - acc: 0.7990 - val_loss: 2.3291 - val_acc: 0.3960\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.5328 - acc: 0.8250 - val_loss: 2.3586 - val_acc: 0.4000\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4789 - acc: 0.8485 - val_loss: 2.2559 - val_acc: 0.4400\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4423 - acc: 0.8610 - val_loss: 2.2023 - val_acc: 0.4480\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4150 - acc: 0.8635 - val_loss: 2.2098 - val_acc: 0.4500\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3724 - acc: 0.8905 - val_loss: 2.2330 - val_acc: 0.4520\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3555 - acc: 0.8855 - val_loss: 1.9528 - val_acc: 0.4940\n",
      "CPU times: user 28.2 s, sys: 2.94 s, total: 31.1 s\n",
      "Wall time: 39.7 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.016\n",
      "Test accuracy: 0.480\n",
      "1000 shot leaning, on second day task\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s - loss: 8.9515 - acc: 0.0000e+00     \n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.3479 - acc: 0.1470     \n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.6844 - acc: 0.2140     \n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.6549 - acc: 0.2180     \n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.6318 - acc: 0.2490     \n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.5649 - acc: 0.3050     \n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.4829 - acc: 0.3450     \n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.3763 - acc: 0.3760     \n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.3164 - acc: 0.3870     \n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.1869 - acc: 0.4540     \n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.0712 - acc: 0.5020     \n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.9263 - acc: 0.5950     \n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.8047 - acc: 0.7010     \n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5908 - acc: 0.8060     \n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3974 - acc: 0.8640     \n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3226 - acc: 0.8940     \n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2481 - acc: 0.9190     \n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2662 - acc: 0.9110     \n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1908 - acc: 0.9380     \n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1571 - acc: 0.9500     \n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1524 - acc: 0.9530     \n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0770 - acc: 0.9790     \n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.1133 - acc: 0.9640     \n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0546 - acc: 0.9840     \n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0528 - acc: 0.9830     \n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0467 - acc: 0.9850     \n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0319 - acc: 0.9870     \n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0400 - acc: 0.9900     \n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0520 - acc: 0.9870     \n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0423 - acc: 0.9900     \n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0515 - acc: 0.9880     \n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0332 - acc: 0.9900     \n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0515 - acc: 0.9860     \n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0301 - acc: 0.9940     \n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0342 - acc: 0.9910     \n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0263 - acc: 0.9890     \n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0278 - acc: 0.9880     \n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0400 - acc: 0.9900     \n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0179 - acc: 0.9920     \n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0215 - acc: 0.9920     \n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0272 - acc: 0.9900     \n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0203 - acc: 0.9940     \n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0487 - acc: 0.9870     \n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0368 - acc: 0.9900     \n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0219 - acc: 0.9940     \n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0193 - acc: 0.9940     \n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0153 - acc: 0.9960     \n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0250 - acc: 0.9910     \n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0221 - acc: 0.9890     \n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.0171 - acc: 0.9930     \n",
      "CPU times: user 36.8 s, sys: 5 s, total: 41.8 s\n",
      "Wall time: 54.6 s\n",
      "1000 shot leaning, test on B task\n",
      "500/500 [==============================] - 9s     \n",
      "\n",
      "Test loss: 2.234\n",
      "Test accuracy: 0.682\n",
      "1000 shot leaning, test on A task\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 15.595\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# day with 30 epochs with adam learnign rate = 0.005\n",
    "# with less epoch on fewer shots to see if we can prevent task A from catastriphic forgetting\n",
    "from keras.optimizers import SGD, Adam\n",
    "model_whole = day()\n",
    "# test on yesterday episode -> totally forget -> shouldn't totally forget when few shot\n",
    "\n",
    "# zero shot learning\n",
    "print(\"zero shot learning\")\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"zero shot leaning, test on B task\")\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "print(\"zero shot leaning, test on A task\")\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "nb_epoch = 30\n",
    "# nb_epoch = 10 is not enough to let 500 shot learning converge\n",
    "\n",
    "# few shot learning on the next episode (500 image) No dream\n",
    "nums_train_images = [1, 5, 10, 15, 20, 100, 200, 300, 400, 500, 1000]\n",
    "for num_train_images in nums_train_images:\n",
    "    # adjust training epoch\n",
    "    if num_train_images < 20:\n",
    "        nb_epoch = 5\n",
    "    else:\n",
    "        nb_epoch = 50\n",
    "    # first initialize the model and let in train on the day time task (task A)\n",
    "    print(str(num_train_images) + \" shot leaning, day time\")\n",
    "    model_whole = day()\n",
    "    print(str(num_train_images) + \" shot leaning, on second day task\")\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train_medium_sized_mammals[:num_train_images], Y_train_medium_sized_mammals[:num_train_images], \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              shuffle=True)\n",
    "    # batch_size removed 15:00\n",
    "    # default adam 0.64\n",
    "    print(str(num_train_images) + \" shot leaning, test on B task\")\n",
    "    score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "    print(str(num_train_images) + \" shot leaning, test on A task\")\n",
    "    # test on yesterday episode -> totally forget\n",
    "    score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A = medium B = aquatic(not working so well)\n",
    "## no dreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def day(X_train, Y_train, X_test, Y_test):\n",
    "    model_whole = load_model_whole()\n",
    "    # with default Adam as optimizer and softmax\n",
    "    from keras.optimizers import SGD, Adam\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "    model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    %time his = model_whole.fit(X_train, Y_train, \\\n",
    "              batch_size=batch_size, \\\n",
    "              nb_epoch=nb_epoch, \\\n",
    "              validation_split=0.2, \\\n",
    "              callbacks=[early_stop], \\\n",
    "              shuffle=True)\n",
    "    # 0.468 with sigmoid activation function at the last layer\n",
    "    # 0.66 if use softmax\n",
    "    score = model_whole.evaluate(X_test, Y_test, verbose=1, batch_size=batch_size)\n",
    "    print('\\nTest loss: %.3f' % score[0])\n",
    "    print('Test accuracy: %.3f' % score[1])\n",
    "    return model_whole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "800/800 [==============================] - 12s - loss: 13.1847 - acc: 0.0000e+00 - val_loss: 5.2750 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 0s - loss: 3.8284 - acc: 0.0525 - val_loss: 1.8316 - val_acc: 0.1950\n",
      "Epoch 3/30\n",
      "800/800 [==============================] - 0s - loss: 1.7527 - acc: 0.2325 - val_loss: 1.7293 - val_acc: 0.1700\n",
      "Epoch 4/30\n",
      "800/800 [==============================] - 0s - loss: 1.6834 - acc: 0.2112 - val_loss: 1.6684 - val_acc: 0.2100\n",
      "Epoch 5/30\n",
      "800/800 [==============================] - 0s - loss: 1.7055 - acc: 0.2263 - val_loss: 1.6628 - val_acc: 0.2050\n",
      "Epoch 6/30\n",
      "800/800 [==============================] - 0s - loss: 1.6715 - acc: 0.2250 - val_loss: 1.6321 - val_acc: 0.2200\n",
      "Epoch 7/30\n",
      "800/800 [==============================] - 0s - loss: 1.6717 - acc: 0.2412 - val_loss: 1.5925 - val_acc: 0.2550\n",
      "Epoch 8/30\n",
      "800/800 [==============================] - 0s - loss: 1.6034 - acc: 0.2525 - val_loss: 1.5595 - val_acc: 0.2800\n",
      "Epoch 9/30\n",
      "800/800 [==============================] - 0s - loss: 1.4748 - acc: 0.3337 - val_loss: 1.4337 - val_acc: 0.3400\n",
      "Epoch 10/30\n",
      "800/800 [==============================] - 0s - loss: 1.3642 - acc: 0.4000 - val_loss: 1.3549 - val_acc: 0.3550\n",
      "Epoch 11/30\n",
      "800/800 [==============================] - 0s - loss: 1.2734 - acc: 0.3837 - val_loss: 1.3344 - val_acc: 0.2950\n",
      "Epoch 12/30\n",
      "800/800 [==============================] - 0s - loss: 1.1939 - acc: 0.4100 - val_loss: 1.2985 - val_acc: 0.3950\n",
      "Epoch 13/30\n",
      "800/800 [==============================] - 0s - loss: 1.1480 - acc: 0.4437 - val_loss: 1.1962 - val_acc: 0.4350\n",
      "Epoch 14/30\n",
      "800/800 [==============================] - 0s - loss: 1.0859 - acc: 0.4475 - val_loss: 1.1855 - val_acc: 0.4400\n",
      "Epoch 15/30\n",
      "800/800 [==============================] - 0s - loss: 1.0167 - acc: 0.5050 - val_loss: 1.1035 - val_acc: 0.4650\n",
      "Epoch 16/30\n",
      "800/800 [==============================] - 0s - loss: 1.0052 - acc: 0.5175 - val_loss: 1.2195 - val_acc: 0.4450\n",
      "Epoch 17/30\n",
      "800/800 [==============================] - 0s - loss: 0.9769 - acc: 0.5188 - val_loss: 1.1813 - val_acc: 0.4700\n",
      "Epoch 18/30\n",
      "800/800 [==============================] - 0s - loss: 0.9863 - acc: 0.5400 - val_loss: 1.1865 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "800/800 [==============================] - 0s - loss: 0.8999 - acc: 0.5663 - val_loss: 1.2480 - val_acc: 0.4900\n",
      "Epoch 20/30\n",
      "800/800 [==============================] - 0s - loss: 0.8666 - acc: 0.5887 - val_loss: 1.1981 - val_acc: 0.4550\n",
      "Epoch 21/30\n",
      "800/800 [==============================] - 0s - loss: 0.8054 - acc: 0.6062 - val_loss: 1.2012 - val_acc: 0.4500\n",
      "Epoch 22/30\n",
      "800/800 [==============================] - 0s - loss: 0.7795 - acc: 0.6475 - val_loss: 1.3076 - val_acc: 0.4850\n",
      "Epoch 23/30\n",
      "800/800 [==============================] - 0s - loss: 0.7926 - acc: 0.6125 - val_loss: 1.2729 - val_acc: 0.4750\n",
      "Epoch 24/30\n",
      "800/800 [==============================] - 0s - loss: 0.7710 - acc: 0.6625 - val_loss: 1.1799 - val_acc: 0.4900\n",
      "Epoch 25/30\n",
      "800/800 [==============================] - 0s - loss: 0.6600 - acc: 0.7075 - val_loss: 1.2173 - val_acc: 0.5250\n",
      "Epoch 26/30\n",
      "800/800 [==============================] - 0s - loss: 0.6668 - acc: 0.7050 - val_loss: 1.2307 - val_acc: 0.5200\n",
      "Epoch 27/30\n",
      "800/800 [==============================] - 0s - loss: 0.6179 - acc: 0.7250 - val_loss: 1.2030 - val_acc: 0.5650\n",
      "Epoch 28/30\n",
      "800/800 [==============================] - 0s - loss: 0.5766 - acc: 0.7350 - val_loss: 1.3761 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "800/800 [==============================] - 0s - loss: 0.5473 - acc: 0.7525 - val_loss: 1.6326 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "800/800 [==============================] - 0s - loss: 0.6342 - acc: 0.7188 - val_loss: 1.4684 - val_acc: 0.5100\n",
      "CPU times: user 25.6 s, sys: 2.56 s, total: 28.2 s\n",
      "Wall time: 33.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.387\n",
      "Test accuracy: 0.534\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.176\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# few shot learning on the next episode (1000 image) No dream\n",
    "num_train_images = 1000\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his = model_whole.fit(X_train_aquatic_mammals[:num_train_images], Y_train_aquatic_mammals[:num_train_images], \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          shuffle=True)\n",
    "\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# default adam 0.64\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "datalab pretrained model loaded\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 14s - loss: 1.9254 - acc: 0.4230 - val_loss: 0.9455 - val_acc: 0.6800\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6837 - acc: 0.7645 - val_loss: 0.7808 - val_acc: 0.7540\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4517 - acc: 0.8550 - val_loss: 0.5943 - val_acc: 0.8280\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3177 - acc: 0.9015 - val_loss: 0.6152 - val_acc: 0.8360\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2252 - acc: 0.9305 - val_loss: 0.5932 - val_acc: 0.8420\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1517 - acc: 0.9520 - val_loss: 0.5640 - val_acc: 0.8460\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1088 - acc: 0.9640 - val_loss: 0.7052 - val_acc: 0.8400\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0884 - acc: 0.9735 - val_loss: 0.7910 - val_acc: 0.8300\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 0.9690 - val_loss: 0.7512 - val_acc: 0.8420\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1040 - acc: 0.9710 - val_loss: 0.8388 - val_acc: 0.8160\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0805 - acc: 0.9765 - val_loss: 0.6693 - val_acc: 0.8500\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0884 - acc: 0.9730 - val_loss: 0.7865 - val_acc: 0.8140\n",
      "CPU times: user 25.8 s, sys: 2.37 s, total: 28.1 s\n",
      "Wall time: 34 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 0.907\n",
      "Test accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "model_whole = day(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "800/800 [==============================] - 13s - loss: 13.6553 - acc: 0.0000e+00 - val_loss: 6.7170 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "800/800 [==============================] - 0s - loss: 4.9289 - acc: 0.0025 - val_loss: 2.1236 - val_acc: 0.1700\n",
      "Epoch 3/30\n",
      "800/800 [==============================] - 0s - loss: 1.8712 - acc: 0.2412 - val_loss: 1.7295 - val_acc: 0.1950\n",
      "Epoch 4/30\n",
      "800/800 [==============================] - 0s - loss: 1.7327 - acc: 0.2250 - val_loss: 1.6077 - val_acc: 0.2500\n",
      "Epoch 5/30\n",
      "800/800 [==============================] - 0s - loss: 1.5855 - acc: 0.2475 - val_loss: 1.5901 - val_acc: 0.2450\n",
      "Epoch 6/30\n",
      "800/800 [==============================] - 0s - loss: 1.4919 - acc: 0.3163 - val_loss: 1.4815 - val_acc: 0.2950\n",
      "Epoch 7/30\n",
      "800/800 [==============================] - 0s - loss: 1.3811 - acc: 0.3663 - val_loss: 1.3691 - val_acc: 0.3750\n",
      "Epoch 8/30\n",
      "800/800 [==============================] - 0s - loss: 1.2913 - acc: 0.3675 - val_loss: 1.4384 - val_acc: 0.3500\n",
      "Epoch 9/30\n",
      "800/800 [==============================] - 0s - loss: 1.2486 - acc: 0.4150 - val_loss: 1.3801 - val_acc: 0.3350\n",
      "Epoch 10/30\n",
      "800/800 [==============================] - 0s - loss: 1.1703 - acc: 0.4300 - val_loss: 1.3196 - val_acc: 0.3750\n",
      "Epoch 11/30\n",
      "800/800 [==============================] - 0s - loss: 1.1562 - acc: 0.4362 - val_loss: 1.3806 - val_acc: 0.3450\n",
      "Epoch 12/30\n",
      "800/800 [==============================] - 0s - loss: 1.1212 - acc: 0.4662 - val_loss: 1.4385 - val_acc: 0.3900\n",
      "Epoch 13/30\n",
      "800/800 [==============================] - 0s - loss: 1.1028 - acc: 0.4488 - val_loss: 1.3768 - val_acc: 0.3700\n",
      "Epoch 14/30\n",
      "800/800 [==============================] - 0s - loss: 1.1154 - acc: 0.4525 - val_loss: 1.3227 - val_acc: 0.3500\n",
      "Epoch 15/30\n",
      "800/800 [==============================] - 0s - loss: 1.0403 - acc: 0.4688 - val_loss: 1.2983 - val_acc: 0.4100\n",
      "Epoch 16/30\n",
      "800/800 [==============================] - 0s - loss: 0.9865 - acc: 0.5012 - val_loss: 1.3092 - val_acc: 0.4000\n",
      "Epoch 17/30\n",
      "800/800 [==============================] - 0s - loss: 0.9641 - acc: 0.5400 - val_loss: 1.2106 - val_acc: 0.4350\n",
      "Epoch 18/30\n",
      "800/800 [==============================] - 0s - loss: 0.9496 - acc: 0.5025 - val_loss: 1.3155 - val_acc: 0.4350\n",
      "Epoch 19/30\n",
      "800/800 [==============================] - 0s - loss: 0.9342 - acc: 0.5100 - val_loss: 1.3920 - val_acc: 0.3900\n",
      "Epoch 20/30\n",
      "800/800 [==============================] - 0s - loss: 0.8749 - acc: 0.5775 - val_loss: 1.2900 - val_acc: 0.4200\n",
      "Epoch 21/30\n",
      "800/800 [==============================] - 0s - loss: 0.8421 - acc: 0.5587 - val_loss: 1.4326 - val_acc: 0.4200\n",
      "Epoch 22/30\n",
      "800/800 [==============================] - 0s - loss: 0.8485 - acc: 0.5900 - val_loss: 1.4320 - val_acc: 0.3850\n",
      "Epoch 23/30\n",
      "800/800 [==============================] - 0s - loss: 0.8331 - acc: 0.5813 - val_loss: 1.5020 - val_acc: 0.4100\n",
      "Epoch 24/30\n",
      "800/800 [==============================] - 0s - loss: 0.8203 - acc: 0.5962 - val_loss: 1.4722 - val_acc: 0.4250\n",
      "Epoch 25/30\n",
      "800/800 [==============================] - 0s - loss: 0.8348 - acc: 0.6112 - val_loss: 1.4012 - val_acc: 0.4600\n",
      "Epoch 26/30\n",
      "800/800 [==============================] - 0s - loss: 0.7889 - acc: 0.5950 - val_loss: 1.3496 - val_acc: 0.4700\n",
      "Epoch 27/30\n",
      "800/800 [==============================] - 0s - loss: 0.7649 - acc: 0.5950 - val_loss: 1.3570 - val_acc: 0.4650\n",
      "Epoch 28/30\n",
      "800/800 [==============================] - 0s - loss: 0.7259 - acc: 0.6500 - val_loss: 1.4605 - val_acc: 0.4450\n",
      "Epoch 29/30\n",
      "800/800 [==============================] - 0s - loss: 0.7783 - acc: 0.6187 - val_loss: 1.5323 - val_acc: 0.4300\n",
      "Epoch 30/30\n",
      "800/800 [==============================] - 0s - loss: 0.6985 - acc: 0.6462 - val_loss: 1.3064 - val_acc: 0.5000\n",
      "CPU times: user 26.9 s, sys: 2.46 s, total: 29.4 s\n",
      "Wall time: 34.9 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.278\n",
      "Test accuracy: 0.476\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.261\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# few shot learning on the next episode (500 image) with dream\n",
    "# dream(model_whole, X_train_medium_sized_mammals_var, X_train_medium_sized_mammals, Y_train_medium_sized_mammals, X_test_medium_sized_mammals, Y_test_medium_sized_mammals)\n",
    "num_train_images = 1000\n",
    "nb_epoch = 50\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model_whole.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his = model_whole.fit(X_train_aquatic_mammals[:num_train_images], Y_train_aquatic_mammals[:num_train_images], \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          shuffle=True)\n",
    "\n",
    "# test on yesterday episode -> totally forget\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# default adam 0.64\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s - loss: 2.8283 - acc: 0.2330 - val_loss: 1.6318 - val_acc: 0.2140\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.3252 - acc: 0.3665 - val_loss: 1.4312 - val_acc: 0.3600\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.2761 - acc: 0.3975 - val_loss: 1.4627 - val_acc: 0.2760\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.2552 - acc: 0.3990 - val_loss: 1.3453 - val_acc: 0.3660\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.2249 - acc: 0.4340 - val_loss: 1.3434 - val_acc: 0.4320\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.2028 - acc: 0.4385 - val_loss: 1.3201 - val_acc: 0.4800\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.1720 - acc: 0.4445 - val_loss: 1.2957 - val_acc: 0.4860\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.1506 - acc: 0.4655 - val_loss: 1.2782 - val_acc: 0.4780\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.1332 - acc: 0.5005 - val_loss: 1.2918 - val_acc: 0.4720\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.1020 - acc: 0.5150 - val_loss: 1.2366 - val_acc: 0.5020\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.0971 - acc: 0.5240 - val_loss: 1.2138 - val_acc: 0.5160\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.0632 - acc: 0.5600 - val_loss: 1.2173 - val_acc: 0.5100\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.0308 - acc: 0.5675 - val_loss: 1.1950 - val_acc: 0.5180\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.0311 - acc: 0.5655 - val_loss: 1.1592 - val_acc: 0.5420\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.0062 - acc: 0.5760 - val_loss: 1.1363 - val_acc: 0.5520\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9986 - acc: 0.5840 - val_loss: 1.1401 - val_acc: 0.5460\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9932 - acc: 0.5905 - val_loss: 1.1222 - val_acc: 0.5500\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9780 - acc: 0.5960 - val_loss: 1.1213 - val_acc: 0.5500\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9738 - acc: 0.6040 - val_loss: 1.1094 - val_acc: 0.5520\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9581 - acc: 0.6030 - val_loss: 1.1128 - val_acc: 0.5560\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9410 - acc: 0.6175 - val_loss: 1.0971 - val_acc: 0.5600\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9462 - acc: 0.6060 - val_loss: 1.1001 - val_acc: 0.5440\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9276 - acc: 0.6265 - val_loss: 1.0903 - val_acc: 0.5600\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9154 - acc: 0.6300 - val_loss: 1.0863 - val_acc: 0.5580\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9151 - acc: 0.6260 - val_loss: 1.0764 - val_acc: 0.5660\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9001 - acc: 0.6245 - val_loss: 1.0756 - val_acc: 0.5560\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9032 - acc: 0.6400 - val_loss: 1.0743 - val_acc: 0.5620\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8933 - acc: 0.6320 - val_loss: 1.0703 - val_acc: 0.5560\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8891 - acc: 0.6455 - val_loss: 1.0722 - val_acc: 0.5620\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8741 - acc: 0.6485 - val_loss: 1.0667 - val_acc: 0.5720\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8789 - acc: 0.6390 - val_loss: 1.0636 - val_acc: 0.5660\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8677 - acc: 0.6525 - val_loss: 1.0588 - val_acc: 0.5660\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8530 - acc: 0.6620 - val_loss: 1.0611 - val_acc: 0.5700\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8466 - acc: 0.6545 - val_loss: 1.0559 - val_acc: 0.5760\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8575 - acc: 0.6630 - val_loss: 1.0477 - val_acc: 0.5680\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8539 - acc: 0.6640 - val_loss: 1.0462 - val_acc: 0.5760\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8430 - acc: 0.6690 - val_loss: 1.0470 - val_acc: 0.5820\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8336 - acc: 0.6595 - val_loss: 1.0510 - val_acc: 0.5820\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8423 - acc: 0.6675 - val_loss: 1.0420 - val_acc: 0.5880\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8224 - acc: 0.6800 - val_loss: 1.0419 - val_acc: 0.5820\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8318 - acc: 0.6620 - val_loss: 1.0427 - val_acc: 0.5840\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8118 - acc: 0.6760 - val_loss: 1.0452 - val_acc: 0.5860\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8051 - acc: 0.6820 - val_loss: 1.0429 - val_acc: 0.5840\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8050 - acc: 0.6810 - val_loss: 1.0387 - val_acc: 0.5900\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8110 - acc: 0.6805 - val_loss: 1.0457 - val_acc: 0.5860\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8058 - acc: 0.6805 - val_loss: 1.0419 - val_acc: 0.5900\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7822 - acc: 0.6900 - val_loss: 1.0421 - val_acc: 0.5800\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7916 - acc: 0.6810 - val_loss: 1.0405 - val_acc: 0.5820\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7825 - acc: 0.6890 - val_loss: 1.0466 - val_acc: 0.5840\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7902 - acc: 0.6865 - val_loss: 1.0456 - val_acc: 0.5960\n",
      "CPU times: user 1min 19s, sys: 10.7 s, total: 1min 30s\n",
      "Wall time: 1min 56s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 1.013\n",
      "Test accuracy: 0.606\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 9.274\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s - loss: 3.9754 - acc: 0.1175 - val_loss: 1.6344 - val_acc: 0.2600\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.4900 - acc: 0.3080 - val_loss: 1.8243 - val_acc: 0.2660\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.3386 - acc: 0.3880 - val_loss: 1.6966 - val_acc: 0.3060\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.2264 - acc: 0.4125 - val_loss: 1.7119 - val_acc: 0.2980\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.1346 - acc: 0.4735 - val_loss: 2.3626 - val_acc: 0.2380\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.0606 - acc: 0.4990 - val_loss: 1.8315 - val_acc: 0.2960\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9846 - acc: 0.5185 - val_loss: 2.0905 - val_acc: 0.2560\n",
      "CPU times: user 22.8 s, sys: 1.59 s, total: 24.4 s\n",
      "Wall time: 28 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 11.706\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.030\n",
      "Test accuracy: 0.262\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 22.7 s, sys: 1.5 s, total: 24.2 s\n",
      "Wall time: 27.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 2.084\n",
      "Test accuracy: 0.332\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 22.9 s, sys: 1.47 s, total: 24.4 s\n",
      "Wall time: 28 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "small_mammals\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 13s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 23.2 s, sys: 1.43 s, total: 24.6 s\n",
      "Wall time: 28.2 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# with Adam as optimizer\n",
    "nb_epoch = 50\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_fish = model_whole.fit(X_train_fish, Y_train_fish, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_fish, Y_test_fish, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_people = model_whole.fit(X_train_people, Y_train_people, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_fish, Y_test_fish, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam 0.001 -> acc=0.4\n",
    "# with Adam as optimizer\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_aquatic_mammals = model_whole.fit(X_train_aquatic_mammals, Y_train_aquatic_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_small = model_whole.fit(X_train_small_mammals, Y_train_small_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "print('small_mammals')\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# with Adam as optimizer\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_medium = model_whole.fit(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 5s - loss: 10.1634 - acc: 0.0000e+00 - val_loss: 5.2568 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s - loss: 3.2636 - acc: 0.1360 - val_loss: 3.4379 - val_acc: 0.2240\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s - loss: 1.5708 - acc: 0.5130 - val_loss: 3.4227 - val_acc: 0.2260\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.9949 - acc: 0.6155 - val_loss: 2.9184 - val_acc: 0.2760\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.7679 - acc: 0.7045 - val_loss: 2.2897 - val_acc: 0.3300\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.6329 - acc: 0.7705 - val_loss: 1.7022 - val_acc: 0.4400\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.4949 - acc: 0.8295 - val_loss: 1.1488 - val_acc: 0.5960\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.3882 - acc: 0.8590 - val_loss: 1.0181 - val_acc: 0.6620\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2970 - acc: 0.8975 - val_loss: 0.9433 - val_acc: 0.6800\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.2073 - acc: 0.9315 - val_loss: 0.8232 - val_acc: 0.7460\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1626 - acc: 0.9490 - val_loss: 0.8970 - val_acc: 0.7720\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1281 - acc: 0.9625 - val_loss: 0.7003 - val_acc: 0.8060\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9680 - val_loss: 0.7697 - val_acc: 0.7900\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 0.9695 - val_loss: 0.8324 - val_acc: 0.8040\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0689 - acc: 0.9780 - val_loss: 1.0341 - val_acc: 0.7460\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0508 - acc: 0.9850 - val_loss: 0.8300 - val_acc: 0.8020\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0386 - acc: 0.9900 - val_loss: 0.7926 - val_acc: 0.8060\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s - loss: 0.0266 - acc: 0.9915 - val_loss: 0.7550 - val_acc: 0.8280\n",
      "CPU times: user 27.9 s, sys: 3.86 s, total: 31.7 s\n",
      "Wall time: 41 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 0.782\n",
      "Test accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "# with Adam as optimizer\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his = model_whole.fit(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 14s - loss: 3.1902 - acc: 0.2870 - val_loss: 2.1512 - val_acc: 0.3140\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.1710 - acc: 0.5250 - val_loss: 1.7936 - val_acc: 0.4040\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 1.0721 - acc: 0.5685 - val_loss: 1.7925 - val_acc: 0.4140\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9976 - acc: 0.5920 - val_loss: 1.5504 - val_acc: 0.4800\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9508 - acc: 0.6140 - val_loss: 1.4149 - val_acc: 0.4860\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9307 - acc: 0.6270 - val_loss: 1.2989 - val_acc: 0.5080\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 0.9077 - acc: 0.6255 - val_loss: 1.2173 - val_acc: 0.5320\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8767 - acc: 0.6460 - val_loss: 1.0686 - val_acc: 0.5800\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8692 - acc: 0.6555 - val_loss: 1.0461 - val_acc: 0.5940\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8578 - acc: 0.6470 - val_loss: 1.0109 - val_acc: 0.6080\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8234 - acc: 0.6660 - val_loss: 0.9909 - val_acc: 0.6080\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8232 - acc: 0.6715 - val_loss: 0.9939 - val_acc: 0.6160\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8149 - acc: 0.6695 - val_loss: 0.9979 - val_acc: 0.6120\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7933 - acc: 0.6780 - val_loss: 1.0033 - val_acc: 0.6120\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8018 - acc: 0.6900 - val_loss: 1.0117 - val_acc: 0.6100\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7803 - acc: 0.6810 - val_loss: 1.0273 - val_acc: 0.6080\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7653 - acc: 0.7095 - val_loss: 1.0241 - val_acc: 0.6040\n",
      "CPU times: user 37 s, sys: 3.83 s, total: 40.9 s\n",
      "Wall time: 49.5 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 0.972\n",
      "Test accuracy: 0.604\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 8.090\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 15s - loss: 3.6956 - acc: 0.0825 - val_loss: 1.7262 - val_acc: 0.2220\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.4181 - acc: 0.3615 - val_loss: 2.4405 - val_acc: 0.2200\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 2s - loss: 1.1245 - acc: 0.4730 - val_loss: 4.0539 - val_acc: 0.2100\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.9085 - acc: 0.5520 - val_loss: 3.6102 - val_acc: 0.2500\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.8348 - acc: 0.5920 - val_loss: 4.1903 - val_acc: 0.2260\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7710 - acc: 0.6165 - val_loss: 4.6799 - val_acc: 0.2220\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 2s - loss: 0.7268 - acc: 0.6240 - val_loss: 5.8056 - val_acc: 0.2240\n",
      "CPU times: user 24.7 s, sys: 1.62 s, total: 26.3 s\n",
      "Wall time: 29.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 14.500\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 5.909\n",
      "Test accuracy: 0.206\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 15s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 24.5 s, sys: 1.59 s, total: 26.1 s\n",
      "Wall time: 29.6 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.184\n",
      "Test accuracy: 0.314\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 15s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 24.7 s, sys: 1.6 s, total: 26.3 s\n",
      "Wall time: 29.8 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "small_mammals\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 15s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s - loss: 16.1181 - acc: 0.0000e+00 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 24.8 s, sys: 1.6 s, total: 26.4 s\n",
      "Wall time: 30 s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# with Adam as optimizer\n",
    "nb_epoch = 50\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_fish = model_whole.fit(X_train_fish, Y_train_fish, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_fish, Y_test_fish, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_people = model_whole.fit(X_train_people, Y_train_people, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_fish, Y_test_fish, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam 0.001 -> acc=0.4\n",
    "# with Adam as optimizer\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_aquatic_mammals = model_whole.fit(X_train_aquatic_mammals, Y_train_aquatic_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_people, Y_test_people, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_small = model_whole.fit(X_train_small_mammals, Y_train_small_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "print('small_mammals')\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# with Adam as optimizer\n",
    "from keras.optimizers import SGD, Adam\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model_whole.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "%time his_medium = model_whole.fit(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True)\n",
    "score = model_whole.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_whole.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1, batch_size=batch_size)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54231471  0.31023795  0.43598823  0.81954595  0.22352404  0.1622244\n",
      "   0.64465787  0.43645331  0.67760913  0.24153832  0.33627425  0.50640838\n",
      "   0.00974586  0.90043205  0.41529917  0.55301516  0.37952037  0.67799617\n",
      "   0.93166581  0.01558411  0.37545502  0.6369016   0.78943881  0.97170373\n",
      "   0.04646597  0.94154568  0.40585277  0.9898262   0.76643067  0.97475879\n",
      "   0.92065995  0.71561763  0.09326926  0.9892496   0.66929815  0.09108353\n",
      "   0.75456398  0.27724357  0.3146112   0.54251524  0.46691219  0.30771952\n",
      "   0.55409226  0.31354999  0.72551269  0.37550116  0.76649713  0.92303626\n",
      "   0.12260844  0.44741025  0.11078386  0.27568374  0.42407826  0.00288281\n",
      "   0.52620482  0.65797478  0.21412428  0.80806639  0.45012092  0.62624036\n",
      "   0.01097225  0.47536545  0.38575254  0.48959981  0.20333594  0.0077105\n",
      "   0.38977264  0.94621528  0.77915417  0.30415377  0.63362173  0.81139641\n",
      "   0.29997307  0.40689954  0.27817098  0.9615996   0.49269924  0.91395192\n",
      "   0.33702111  0.04457065  0.49594532  0.86960939  0.79063657  0.56054703\n",
      "   0.39113073  0.20292315  0.08711035  0.6545564   0.67114179  0.74060866\n",
      "   0.8744276   0.14370742  0.51093816  0.31326894  0.3915829   0.8904981\n",
      "   0.94171372  0.66481993  0.78079322  0.55360601]]\n",
      "Start of iteration 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: moments_128/sufficient_statistics/Gather/_14409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1079_moments_128/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'keras_learning_phase', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-17f6f4880caa>\", line 16, in <module>\n    vgg16 = load_model('good_model.h5')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 140, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 190, in model_from_config\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/utils/layer_utils.py\", line 40, in layer_from_config\n    return layer_class.from_config(config['config'])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 1080, in from_config\n    model.add(layer)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 327, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 569, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 632, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 164, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/layers/normalization.py\", line 140, in call\n    x_normed = K.in_train_phase(x_normed, x_normed_running)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1877, in in_train_phase\n    if learning_phase() is 1:\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 73, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: moments_128/sufficient_statistics/Gather/_14409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1079_moments_128/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: moments_128/sufficient_statistics/Gather/_14409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1079_moments_128/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-acd3c1d347ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdeep_dream_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_whole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deepdream_cifar100/grey'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-1ea4a51f9336>\u001b[0m in \u001b[0;36mdeep_dream_and_fit\u001b[0;34m(input_model, input_img, input_prefix)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finish dreaming image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdream_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set learning phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-1ea4a51f9336>\u001b[0m in \u001b[0;36mdeepdream_image\u001b[0;34m(img, result_prefix)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# Run L-BFGS for 7 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n\u001b[0;32m--> 242\u001b[0;31m                                              fprime=evaluator.grads, maxfun=7)\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current loss value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Decode the dream and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-1ea4a51f9336>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-1ea4a51f9336>\u001b[0m in \u001b[0;36meval_loss_and_grads\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: moments_128/sufficient_statistics/Gather/_14409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1079_moments_128/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'keras_learning_phase', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-17f6f4880caa>\", line 16, in <module>\n    vgg16 = load_model('good_model.h5')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 140, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 190, in model_from_config\n    return layer_from_config(config, custom_objects=custom_objects)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/utils/layer_utils.py\", line 40, in layer_from_config\n    return layer_class.from_config(config['config'])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 1080, in from_config\n    model.add(layer)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/models.py\", line 327, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 569, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 632, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 164, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/layers/normalization.py\", line 140, in call\n    x_normed = K.in_train_phase(x_normed, x_normed_running)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1877, in in_train_phase\n    if learning_phase() is 1:\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 73, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool\n\t [[Node: keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: moments_128/sufficient_statistics/Gather/_14409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1079_moments_128/sufficient_statistics/Gather\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "K.keras_learning_phase = tf.constant(0)\n",
    "deep_dream_and_fit(model_whole, random_init_image(32, 32), 'deepdream_cifar100/grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40670691  0.22584213  0.85347991  0.36440725  0.94042201  0.25906687\n",
      "   0.20616478  0.77325257  0.28498231  0.33620024  0.15504764  0.29802292\n",
      "   0.97810746  0.28550533  0.94665469  0.12558555  0.48820574  0.66912461\n",
      "   0.99989687  0.46593362  0.98936288  0.58609181  0.91203462  0.27622918\n",
      "   0.02535864  0.07897098  0.16029382  0.82526203  0.68014262  0.91930492\n",
      "   0.24381048  0.79438372  0.1373752   0.21510389  0.02080192  0.19316827\n",
      "   0.34772062  0.18230709  0.59107653  0.78348845  0.4821944   0.49452403\n",
      "   0.83229532  0.86212189  0.35787441  0.46790503  0.54418334  0.83967318\n",
      "   0.55560158  0.10172846  0.03389174  0.60838807  0.98298495  0.4272163\n",
      "   0.36002527  0.96827285  0.17539042  0.23324614  0.39776409  0.93445324\n",
      "   0.61964931  0.09762377  0.92308958  0.17671126  0.529704    0.46797102\n",
      "   0.7742384   0.63575597  0.1585015   0.50828611  0.85819791  0.30646651\n",
      "   0.13750862  0.00867029  0.74286945  0.74445719  0.1611972   0.05573085\n",
      "   0.43232196  0.78029442  0.86313854  0.59449979  0.00334711  0.72589564\n",
      "   0.39644232  0.84979104  0.57160632  0.73422615  0.57447049  0.40314936\n",
      "   0.64960036  0.54839045  0.89399086  0.23453009  0.4359863   0.43443926\n",
      "   0.0532725   0.27029206  0.06738405  0.43110421]]\n",
      "Start of iteration 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'input_12' with dtype float and shape [1,32,32,3]\n\t [[Node: input_12 = Placeholder[dtype=DT_FLOAT, shape=[1,32,32,3], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: add_2889/_8821 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_305_add_2889\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'input_12', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-9bc9c176aba3>\", line 5, in <module>\n    dream = Input(batch_shape=(batch_size,) + img_size)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 1193, in Input\n    input_tensor=tensor)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 1111, in __init__\n    name=self.name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 310, in placeholder\n    x = tf.placeholder(dtype, shape=shape, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_12' with dtype float and shape [1,32,32,3]\n\t [[Node: input_12 = Placeholder[dtype=DT_FLOAT, shape=[1,32,32,3], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: add_2889/_8821 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_305_add_2889\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input_12' with dtype float and shape [1,32,32,3]\n\t [[Node: input_12 = Placeholder[dtype=DT_FLOAT, shape=[1,32,32,3], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: add_2889/_8821 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_305_add_2889\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-4729d0dc2f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeep_dream_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pooling_256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deepdream_cifar100/grey'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-d06648cc9ed4>\u001b[0m in \u001b[0;36mdeep_dream_and_fit\u001b[0;34m(input_model, input_img, input_prefix)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finish dreaming image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdream_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set learning phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-d06648cc9ed4>\u001b[0m in \u001b[0;36mdeepdream_image\u001b[0;34m(img, result_prefix)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# Run L-BFGS for 7 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n\u001b[0;32m--> 242\u001b[0;31m                                              fprime=evaluator.grads, maxfun=7)\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current loss value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Decode the dream and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 193\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    195\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-d06648cc9ed4>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-d06648cc9ed4>\u001b[0m in \u001b[0;36meval_loss_and_grads\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input_12' with dtype float and shape [1,32,32,3]\n\t [[Node: input_12 = Placeholder[dtype=DT_FLOAT, shape=[1,32,32,3], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: add_2889/_8821 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_305_add_2889\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'input_12', defined at:\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/assistant/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-9bc9c176aba3>\", line 5, in <module>\n    dream = Input(batch_shape=(batch_size,) + img_size)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 1193, in Input\n    input_tensor=tensor)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\", line 1111, in __init__\n    name=self.name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 310, in placeholder\n    x = tf.placeholder(dtype, shape=shape, name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_12' with dtype float and shape [1,32,32,3]\n\t [[Node: input_12 = Placeholder[dtype=DT_FLOAT, shape=[1,32,32,3], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\t [[Node: add_2889/_8821 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_305_add_2889\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "deep_dream_and_fit(model_pooling_256, random_init_image(32, 32), 'deepdream_cifar100/grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continous training with dream here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    # Adjust vertical spacing if we need to print ensemble and best-net.\n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Interpolation type.\n",
    "        if smooth:\n",
    "            interpolation = 'spline16'\n",
    "        else:\n",
    "            interpolation = 'nearest'\n",
    "\n",
    "        # Plot image.\n",
    "        ax.imshow(toimage(images[i, :, :, :]),\n",
    "                  interpolation=interpolation)\n",
    "            \n",
    "        # Name of the true class.\n",
    "        cls_true_name = cls_names[cls_true[i]]\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true_name)\n",
    "        else:\n",
    "            # Name of the predicted class.\n",
    "            cls_pred_name = cls_names[cls_pred[i]]\n",
    "\n",
    "            xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ae541667bbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_true_cls_pred_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_people\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_cls_pred_people\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "y_true_cls_pred_people = np.argmax(pred_people, axis=1)\n",
    "print(y_true_cls_pred_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_people' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a6b2c1319e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_people\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_class_people\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_cls_pred_people\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_people' is not defined"
     ]
    }
   ],
   "source": [
    "plot_images(X_test_people[:9], Y_test_class_people[:9], y_true_cls_pred_people[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and test on 25 category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 11s - loss: 2.3327 - acc: 0.5403 - val_loss: 0.6671 - val_acc: 0.8580\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 8s - loss: 2.0582 - acc: 0.5791 - val_loss: 0.6930 - val_acc: 0.8540\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 8s - loss: 1.8360 - acc: 0.6177 - val_loss: 0.7358 - val_acc: 0.8484\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 8s - loss: 1.6862 - acc: 0.6391 - val_loss: 0.7620 - val_acc: 0.8468\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 8s - loss: 1.5586 - acc: 0.6636 - val_loss: 0.7465 - val_acc: 0.8484\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 8s - loss: 1.4641 - acc: 0.6730 - val_loss: 0.7547 - val_acc: 0.8460\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 8s - loss: 1.3837 - acc: 0.6956 - val_loss: 0.7766 - val_acc: 0.8440\n",
      "CPU times: user 41.8 s, sys: 8.06 s, total: 49.9 s\n",
      "Wall time: 1min 6s\n",
      "2500/2500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 3.185\n",
      "Test accuracy: 0.435\n"
     ]
    }
   ],
   "source": [
    "# using good model\n",
    "from keras.optimizers import Adam, SGD\n",
    "model_whole.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "%time his = model_whole.fit(X_train_five, Y_train_five, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_whole.evaluate(X_test_five, Y_test_five, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# achieve blah blah acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 11s - loss: 2.9484 - acc: 0.0822 - val_loss: 11.1403 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 9s - loss: 2.5632 - acc: 0.1301 - val_loss: 12.7947 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 9s - loss: 2.3674 - acc: 0.1664 - val_loss: 13.1084 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 9s - loss: 2.2302 - acc: 0.1945 - val_loss: 13.7520 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 9s - loss: 2.0615 - acc: 0.2490 - val_loss: 13.8784 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 9s - loss: 1.9583 - acc: 0.2786 - val_loss: 13.2374 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 9s - loss: 1.8564 - acc: 0.3016 - val_loss: 14.2540 - val_acc: 0.0000e+00\n",
      "CPU times: user 45.9 s, sys: 7.67 s, total: 53.6 s\n",
      "Wall time: 1min 11s\n",
      "2496/2500 [============================>.] - ETA: 0s\n",
      "Test loss: 4.631\n",
      "Test accuracy: 0.218\n"
     ]
    }
   ],
   "source": [
    "# using good model\n",
    "from keras.optimizers import Adam, SGD\n",
    "model_whole.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "%time his = model_whole.fit(X_train_five, Y_train_five, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_whole.evaluate(X_test_five, Y_test_five, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# achieve blah blah acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 40s - loss: 3.9111 - acc: 0.0856 - val_loss: 3.6208 - val_acc: 0.1021\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 39s - loss: 3.0497 - acc: 0.1816 - val_loss: 2.8533 - val_acc: 0.2335\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 39s - loss: 2.6024 - acc: 0.2790 - val_loss: 3.0496 - val_acc: 0.2303\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 39s - loss: 2.3152 - acc: 0.3442 - val_loss: 2.5068 - val_acc: 0.3230\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 38s - loss: 2.0760 - acc: 0.4080 - val_loss: 2.4969 - val_acc: 0.3238\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.8963 - acc: 0.4499 - val_loss: 2.4952 - val_acc: 0.3510\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.7426 - acc: 0.4900 - val_loss: 2.4272 - val_acc: 0.3614\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.6173 - acc: 0.5200 - val_loss: 2.7614 - val_acc: 0.3237\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.4992 - acc: 0.5501 - val_loss: 2.1662 - val_acc: 0.4261\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.3869 - acc: 0.5824 - val_loss: 2.1248 - val_acc: 0.4457\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.2901 - acc: 0.6087 - val_loss: 2.4235 - val_acc: 0.4139\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.1994 - acc: 0.6308 - val_loss: 2.1232 - val_acc: 0.4521\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.1205 - acc: 0.6529 - val_loss: 2.2161 - val_acc: 0.4479\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 38s - loss: 1.0404 - acc: 0.6780 - val_loss: 2.0685 - val_acc: 0.4825\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.9698 - acc: 0.6949 - val_loss: 2.0657 - val_acc: 0.4766\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.9079 - acc: 0.7114 - val_loss: 2.1933 - val_acc: 0.4686\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.8578 - acc: 0.7266 - val_loss: 2.0918 - val_acc: 0.4908\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.7999 - acc: 0.7429 - val_loss: 2.3144 - val_acc: 0.4784\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.7533 - acc: 0.7582 - val_loss: 2.3309 - val_acc: 0.4891\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.7091 - acc: 0.7717 - val_loss: 2.4017 - val_acc: 0.4667\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 38s - loss: 0.6635 - acc: 0.7841 - val_loss: 2.3330 - val_acc: 0.4939\n",
      "CPU times: user 8min 30s, sys: 1min 29s, total: 9min 59s\n",
      "Wall time: 13min 40s\n",
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 2.275\n",
      "Test accuracy: 0.508\n"
     ]
    }
   ],
   "source": [
    "# using good model\n",
    "from keras.optimizers import Adam, SGD\n",
    "model_whole.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "%time his = model_whole.fit(X_train, Y_train, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_whole.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# achieve blah blah acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using good model\n",
    "from keras.optimizers import Adam, SGD\n",
    "model_whole.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "%time his = model_whole.fit(X_train_five, Y_train_five, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "\n",
    "# evaluate our model\n",
    "score = model_whole.evaluate(X_test_five, Y_test_five, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# achieve blah blah acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 35s - loss: 0.4646 - acc: 0.8473 - val_loss: 2.3376 - val_acc: 0.5243\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 33s - loss: 0.3992 - acc: 0.8692 - val_loss: 2.2198 - val_acc: 0.5535\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 33s - loss: 0.3725 - acc: 0.8744 - val_loss: 2.3901 - val_acc: 0.5423\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 34s - loss: 0.3433 - acc: 0.8860 - val_loss: 2.4508 - val_acc: 0.5361\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 34s - loss: 0.3324 - acc: 0.8896 - val_loss: 2.3346 - val_acc: 0.5487\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 34s - loss: 0.3185 - acc: 0.8944 - val_loss: 2.2794 - val_acc: 0.5515\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 34s - loss: 0.2988 - acc: 0.9007 - val_loss: 2.3484 - val_acc: 0.5594\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 34s - loss: 0.2900 - acc: 0.9018 - val_loss: 2.3545 - val_acc: 0.5562\n",
      "CPU times: user 2min 29s, sys: 35.1 s, total: 3min 4s\n",
      "Wall time: 4min 36s\n",
      " 9952/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 2.332\n",
      "Test accuracy: 0.557\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "# model_whole.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model_whole.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 256\n",
    "nb_epoch = 50\n",
    "\n",
    "%time his = model_whole.fit(X_train, Y_train, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_whole.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "\n",
    "# achieve blah blah acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fixed weight, with finetuning, train and test on five coarse label (batch size: 128)\n",
    "Test accuracy: 0.530\n",
    "last epoch:\n",
    "Epoch 20/50\n",
    "40000/40000 [==============================] - 37s - loss: 0.5848 - acc: 0.8103 - val_loss: 2.1541 - val_acc: 0.5241\n",
    "(over fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test on 5 category subsequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cond_109/Merge:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import applications\n",
    "from os.path import expanduser\n",
    "import os\n",
    "vgg16 = load_model('good_model.h5')\n",
    "# vgg16.summary()\n",
    "# vgg16 = VGG16(weights='imagenet', include_top=False)\n",
    "# model.summary()\n",
    "# fc2 = vgg16.get_layer('fc2').output\n",
    "top = vgg16.get_layer('dropout_56').output\n",
    "print(top)\n",
    "prediction = Dense(output_dim=100, activation='sigmoid', name='logit', input_shape=(512, None, None))(top)\n",
    "model_baby = Model(input=vgg16.input, output=prediction)\n",
    "# model_baby.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496/2500 [============================>.] - ETA: 0s\n",
      "pretrained model without any finetuning, test on the five coarse catefory\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "model_baby.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score = model_baby.evaluate(X_test_five, Y_test_five, verbose=1)\n",
    "pred_five = model_baby.predict(X_test_five)\n",
    "y_true_cls_pred_five = np.argmax(pred_five, axis=1)\n",
    "print('\\npretrained model without any finetuning, test on the five coarse catefory')\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s     \n",
      "\n",
      "pretrained model without any finetuning, test on the five coarse catefory\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "model_baby.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "pred_five = model_baby.predict(X_test_five)\n",
    "y_true_cls_pred_five = np.argmax(pred_five, axis=1)\n",
    "print('\\npretrained model without any finetuning, test on the five coarse catefory')\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAFvCAYAAAAmOO6XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f2146c438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(X_test_five[:9], Y_test_class_five[:9], y_true_cls_pred_five[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s\n",
      "pretrained model without any finetuning, test on the whole cofar100 dataset\n",
      "Test accuracy: 0.010\n"
     ]
    }
   ],
   "source": [
    "model_baby.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score = model_baby.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\npretrained model without any finetuning, test on the whole cifar100 dataset')\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/80\n",
      "2000/2000 [==============================] - 5s - loss: 1.6096 - acc: 0.2075 - val_loss: 1.6095 - val_acc: 0.1820\n",
      "Epoch 2/80\n",
      "2000/2000 [==============================] - 1s - loss: 1.6092 - acc: 0.2055 - val_loss: 1.6091 - val_acc: 0.1820\n",
      "Epoch 3/80\n",
      "2000/2000 [==============================] - 1s - loss: 1.5947 - acc: 0.2035 - val_loss: 1.7339 - val_acc: 0.1820\n",
      "Epoch 4/80\n",
      "2000/2000 [==============================] - 1s - loss: 1.5895 - acc: 0.2060 - val_loss: 1.6399 - val_acc: 0.1820\n",
      "Epoch 5/80\n",
      "2000/2000 [==============================] - 1s - loss: 1.5719 - acc: 0.2070 - val_loss: 1.9504 - val_acc: 0.1820\n",
      "Epoch 6/80\n",
      "2000/2000 [==============================] - 1s - loss: 1.5113 - acc: 0.2005 - val_loss: 2.5367 - val_acc: 0.1960\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-80c3b8ecd22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time small_mammals_his = model_baby.fit(X_train_small_mammals, Y_train_small_mammals,           batch_size=batch_size,           nb_epoch=nb_epoch,           validation_split=0.2,           shuffle=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# evaluate our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_baby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_people\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_people\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/assistant/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# with decay, sometimes the model can't overfit the data\n",
    "model_baby.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "# batch size 100, 128 ok\n",
    "batch_size = 256\n",
    "nb_epoch = 80\n",
    "\n",
    "%time small_mammals_his = model_baby.fit(X_train_small_mammals, Y_train_small_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# test-acc = 0.65 in this setting now: 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/80\n",
      "2250/2250 [==============================] - 8s - loss: 0.0306 - acc: 0.9920 - val_loss: 0.8685 - val_acc: 0.8120\n",
      "Epoch 2/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0593 - acc: 0.9884 - val_loss: 1.0879 - val_acc: 0.7760\n",
      "Epoch 3/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0311 - acc: 0.9920 - val_loss: 2.6398 - val_acc: 0.5400\n",
      "Epoch 4/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0191 - acc: 0.9938 - val_loss: 0.6265 - val_acc: 0.8680\n",
      "Epoch 5/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0235 - acc: 0.9924 - val_loss: 1.1535 - val_acc: 0.7880\n",
      "Epoch 6/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0154 - acc: 0.9969 - val_loss: 2.6872 - val_acc: 0.5360\n",
      "Epoch 7/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0161 - acc: 0.9951 - val_loss: 2.2871 - val_acc: 0.5920\n",
      "Epoch 8/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0076 - acc: 0.9987 - val_loss: 1.7027 - val_acc: 0.6800\n",
      "Epoch 9/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0086 - acc: 0.9964 - val_loss: 1.3244 - val_acc: 0.7560\n",
      "Epoch 10/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0112 - acc: 0.9973 - val_loss: 1.5514 - val_acc: 0.7160\n",
      "Epoch 11/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0223 - acc: 0.9924 - val_loss: 1.8147 - val_acc: 0.6560\n",
      "Epoch 12/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0169 - acc: 0.9933 - val_loss: 1.0337 - val_acc: 0.8080\n",
      "Epoch 13/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0117 - acc: 0.9947 - val_loss: 2.9299 - val_acc: 0.5480\n",
      "Epoch 14/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0157 - acc: 0.9951 - val_loss: 1.2249 - val_acc: 0.7800\n",
      "Epoch 15/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0123 - acc: 0.9947 - val_loss: 1.3905 - val_acc: 0.7520\n",
      "Epoch 16/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0250 - acc: 0.9907 - val_loss: 2.9546 - val_acc: 0.5080\n",
      "Epoch 17/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0269 - acc: 0.9929 - val_loss: 1.1331 - val_acc: 0.7800\n",
      "Epoch 18/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0202 - acc: 0.9920 - val_loss: 0.5336 - val_acc: 0.8760\n",
      "Epoch 19/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0320 - acc: 0.9911 - val_loss: 1.0295 - val_acc: 0.7960\n",
      "Epoch 20/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0190 - acc: 0.9933 - val_loss: 1.6113 - val_acc: 0.6880\n",
      "Epoch 21/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0167 - acc: 0.9942 - val_loss: 1.5506 - val_acc: 0.6920\n",
      "Epoch 22/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0146 - acc: 0.9969 - val_loss: 1.6177 - val_acc: 0.6720\n",
      "Epoch 23/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0116 - acc: 0.9973 - val_loss: 1.5524 - val_acc: 0.6960\n",
      "Epoch 24/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0100 - acc: 0.9978 - val_loss: 1.7139 - val_acc: 0.6760\n",
      "Epoch 25/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0074 - acc: 0.9982 - val_loss: 1.8709 - val_acc: 0.6600\n",
      "Epoch 26/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0040 - acc: 0.9987 - val_loss: 1.7437 - val_acc: 0.6920\n",
      "Epoch 27/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0061 - acc: 0.9982 - val_loss: 1.7634 - val_acc: 0.6840\n",
      "Epoch 28/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0049 - acc: 0.9987 - val_loss: 1.7148 - val_acc: 0.7240\n",
      "Epoch 29/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0104 - acc: 0.9969 - val_loss: 2.5001 - val_acc: 0.6160\n",
      "Epoch 30/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0059 - acc: 0.9978 - val_loss: 1.8282 - val_acc: 0.7240\n",
      "Epoch 31/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0151 - acc: 0.9956 - val_loss: 1.3138 - val_acc: 0.7720\n",
      "Epoch 32/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0143 - acc: 0.9947 - val_loss: 3.8778 - val_acc: 0.4320\n",
      "Epoch 33/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0131 - acc: 0.9956 - val_loss: 1.0730 - val_acc: 0.8000\n",
      "Epoch 34/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0096 - acc: 0.9960 - val_loss: 0.7485 - val_acc: 0.8600\n",
      "Epoch 35/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0075 - acc: 0.9978 - val_loss: 1.4300 - val_acc: 0.7400\n",
      "Epoch 36/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0016 - acc: 1.0000 - val_loss: 1.5377 - val_acc: 0.7200\n",
      "Epoch 37/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0056 - acc: 0.9978 - val_loss: 0.6699 - val_acc: 0.8720\n",
      "Epoch 38/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0042 - acc: 0.9996 - val_loss: 1.1376 - val_acc: 0.8080\n",
      "Epoch 39/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0051 - acc: 0.9978 - val_loss: 2.0023 - val_acc: 0.6800\n",
      "Epoch 40/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0029 - acc: 0.9987 - val_loss: 1.9907 - val_acc: 0.6800\n",
      "Epoch 41/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0095 - acc: 0.9973 - val_loss: 2.2460 - val_acc: 0.6560\n",
      "Epoch 42/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0096 - acc: 0.9978 - val_loss: 0.5701 - val_acc: 0.8960\n",
      "Epoch 43/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0150 - acc: 0.9964 - val_loss: 0.1827 - val_acc: 0.9560\n",
      "Epoch 44/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0194 - acc: 0.9938 - val_loss: 2.3198 - val_acc: 0.6280\n",
      "Epoch 45/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0226 - acc: 0.9951 - val_loss: 1.6672 - val_acc: 0.7080\n",
      "Epoch 46/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0224 - acc: 0.9947 - val_loss: 1.9304 - val_acc: 0.6440\n",
      "Epoch 47/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0158 - acc: 0.9956 - val_loss: 0.6456 - val_acc: 0.8720\n",
      "Epoch 48/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0078 - acc: 0.9982 - val_loss: 0.5331 - val_acc: 0.9040\n",
      "Epoch 49/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0054 - acc: 0.9982 - val_loss: 0.8975 - val_acc: 0.8320\n",
      "Epoch 50/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0041 - acc: 0.9991 - val_loss: 1.7067 - val_acc: 0.6920\n",
      "Epoch 51/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0039 - acc: 0.9987 - val_loss: 1.4165 - val_acc: 0.7280\n",
      "Epoch 52/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0052 - acc: 0.9982 - val_loss: 0.5878 - val_acc: 0.9040\n",
      "Epoch 53/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0064 - acc: 0.9978 - val_loss: 0.4287 - val_acc: 0.9200\n",
      "Epoch 54/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0061 - acc: 0.9973 - val_loss: 0.8658 - val_acc: 0.8520\n",
      "Epoch 55/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0037 - acc: 0.9987 - val_loss: 2.3239 - val_acc: 0.6320\n",
      "Epoch 56/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0036 - acc: 0.9991 - val_loss: 1.9386 - val_acc: 0.6840\n",
      "Epoch 57/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0124 - acc: 0.9964 - val_loss: 2.5963 - val_acc: 0.6080\n",
      "Epoch 58/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0085 - acc: 0.9969 - val_loss: 2.8719 - val_acc: 0.5800\n",
      "Epoch 59/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0021 - acc: 0.9996 - val_loss: 2.5521 - val_acc: 0.6280\n",
      "Epoch 60/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0043 - acc: 0.9978 - val_loss: 1.3064 - val_acc: 0.7920\n",
      "Epoch 61/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0130 - acc: 0.9964 - val_loss: 2.2831 - val_acc: 0.6200\n",
      "Epoch 62/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0058 - acc: 0.9973 - val_loss: 2.5304 - val_acc: 0.5840\n",
      "Epoch 63/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0105 - acc: 0.9969 - val_loss: 1.3294 - val_acc: 0.7640\n",
      "Epoch 64/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0087 - acc: 0.9964 - val_loss: 1.0619 - val_acc: 0.8080\n",
      "Epoch 65/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0082 - acc: 0.9978 - val_loss: 1.4507 - val_acc: 0.7400\n",
      "Epoch 66/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0223 - acc: 0.9960 - val_loss: 1.3009 - val_acc: 0.7640\n",
      "Epoch 67/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0133 - acc: 0.9951 - val_loss: 1.8649 - val_acc: 0.6640\n",
      "Epoch 68/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0047 - acc: 0.9991 - val_loss: 1.0591 - val_acc: 0.8160\n",
      "Epoch 69/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0029 - acc: 0.9991 - val_loss: 1.3043 - val_acc: 0.7720\n",
      "Epoch 70/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0059 - acc: 0.9978 - val_loss: 1.7390 - val_acc: 0.6480\n",
      "Epoch 71/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0106 - acc: 0.9973 - val_loss: 1.7082 - val_acc: 0.7160\n",
      "Epoch 72/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0058 - acc: 0.9987 - val_loss: 0.8989 - val_acc: 0.8400\n",
      "Epoch 73/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0070 - acc: 0.9973 - val_loss: 1.3187 - val_acc: 0.7760\n",
      "Epoch 74/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0081 - acc: 0.9987 - val_loss: 2.5168 - val_acc: 0.5800\n",
      "Epoch 75/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0046 - acc: 0.9987 - val_loss: 1.8178 - val_acc: 0.6880\n",
      "Epoch 76/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0048 - acc: 0.9987 - val_loss: 3.8398 - val_acc: 0.4280\n",
      "Epoch 77/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0031 - acc: 0.9991 - val_loss: 2.6610 - val_acc: 0.5440\n",
      "Epoch 78/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0079 - acc: 0.9982 - val_loss: 1.1274 - val_acc: 0.8000\n",
      "Epoch 79/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0037 - acc: 0.9987 - val_loss: 1.2780 - val_acc: 0.7920\n",
      "Epoch 80/80\n",
      "2250/2250 [==============================] - 1s - loss: 0.0079 - acc: 0.9982 - val_loss: 2.4951 - val_acc: 0.5800\n",
      "CPU times: user 1min 41s, sys: 6.48 s, total: 1min 48s\n",
      "Wall time: 2min 36s\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 0.900\n",
      "Test accuracy: 0.850\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "Test loss: 13.311\n",
      "Test accuracy: 0.122\n"
     ]
    }
   ],
   "source": [
    "# over-fitting here, X_train_easy\n",
    "from keras.optimizers import Adam, SGD\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# with decay, sometimes the model can't overfit the data\n",
    "model_baby.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# VGG_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=adam,\n",
    "#               metrics=['accuracy'])\n",
    "# batch size 100, 128 ok\n",
    "batch_size = 256\n",
    "nb_epoch = 80\n",
    "\n",
    "%time small_mammals_his = model_baby.fit(X_train_easy, Y_train_easy, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_test_easy, Y_test_easy, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# test-acc = 0.65 in this setting now: 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_mammals_his' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-cc92e497f66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplot_validation_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_mammals_his\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"baby_model_small_mammal_adam_with_decay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'small_mammals_his' is not defined"
     ]
    }
   ],
   "source": [
    "########### plot validation history ############\n",
    "def plot_validation_history(his, fig_path):\n",
    "    train_loss = his.history['loss']\n",
    "    val_loss = his.history['val_loss']\n",
    "\n",
    "    # visualize training history\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.show()\n",
    "# plot_validation_history(small_mammals_his, \"baby_model_small_mammal_adam_with_decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_people[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to fit the data using a more simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 5s - loss: 12.9080 - acc: 0.1916 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 0s - loss: 12.5363 - acc: 0.2222 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "CPU times: user 23.3 s, sys: 0 ns, total: 23.3 s\n",
      "Wall time: 25.6 s\n",
      "416/500 [=======================>......] - ETA: 0s\n",
      "Test loss: 12.894\n",
      "Test accuracy: 0.200\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 256\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_easy, Y_train_easy, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_easy, Y_test_easy, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 7s - loss: 3.6653 - acc: 0.1711 - val_loss: 2.2855 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6481 - acc: 0.3809 - val_loss: 2.3860 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3712 - acc: 0.4107 - val_loss: 2.1051 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2200 - acc: 0.4982 - val_loss: 1.8531 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0480 - acc: 0.5880 - val_loss: 1.5184 - val_acc: 0.0240\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9359 - acc: 0.6316 - val_loss: 1.8502 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8076 - acc: 0.6809 - val_loss: 1.9762 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7580 - acc: 0.6987 - val_loss: 2.0316 - val_acc: 0.0040\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7246 - acc: 0.7102 - val_loss: 1.6865 - val_acc: 0.0240\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6407 - acc: 0.7538 - val_loss: 1.4556 - val_acc: 0.1360\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5979 - acc: 0.7631 - val_loss: 1.5778 - val_acc: 0.1080\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5508 - acc: 0.7822 - val_loss: 1.6492 - val_acc: 0.0960\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5797 - acc: 0.7613 - val_loss: 2.0857 - val_acc: 0.0200\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5404 - acc: 0.7987 - val_loss: 1.3695 - val_acc: 0.3520\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4470 - acc: 0.8356 - val_loss: 0.9741 - val_acc: 0.5960\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.3907 - acc: 0.8569 - val_loss: 1.4600 - val_acc: 0.3600\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4794 - acc: 0.8333 - val_loss: 3.2739 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5676 - acc: 0.8031 - val_loss: 1.0384 - val_acc: 0.6080\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4529 - acc: 0.8236 - val_loss: 1.3671 - val_acc: 0.2320\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4538 - acc: 0.8147 - val_loss: 1.6780 - val_acc: 0.2280\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4147 - acc: 0.8369 - val_loss: 0.9024 - val_acc: 0.7280\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.3445 - acc: 0.8822 - val_loss: 1.6165 - val_acc: 0.3440\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.3194 - acc: 0.8791 - val_loss: 1.6873 - val_acc: 0.3560\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2519 - acc: 0.9027 - val_loss: 1.4048 - val_acc: 0.4400\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2169 - acc: 0.9262 - val_loss: 1.3997 - val_acc: 0.4320\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1920 - acc: 0.9307 - val_loss: 1.1120 - val_acc: 0.5600\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1523 - acc: 0.9511 - val_loss: 1.0665 - val_acc: 0.6080\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1251 - acc: 0.9542 - val_loss: 0.9831 - val_acc: 0.6920\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1032 - acc: 0.9653 - val_loss: 1.0816 - val_acc: 0.6440\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0869 - acc: 0.9702 - val_loss: 1.0858 - val_acc: 0.6320\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0608 - acc: 0.9809 - val_loss: 0.5708 - val_acc: 0.8160\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0630 - acc: 0.9800 - val_loss: 1.0824 - val_acc: 0.6520\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0464 - acc: 0.9853 - val_loss: 0.6841 - val_acc: 0.7960\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0424 - acc: 0.9858 - val_loss: 0.7009 - val_acc: 0.7760\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0395 - acc: 0.9862 - val_loss: 1.0382 - val_acc: 0.7080\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0329 - acc: 0.9916 - val_loss: 0.5948 - val_acc: 0.8240\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0327 - acc: 0.9893 - val_loss: 0.7889 - val_acc: 0.7880\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0518 - acc: 0.9809 - val_loss: 0.6765 - val_acc: 0.8160\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0467 - acc: 0.9862 - val_loss: 1.1055 - val_acc: 0.6520\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0332 - acc: 0.9933 - val_loss: 0.7134 - val_acc: 0.7600\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0401 - acc: 0.9916 - val_loss: 1.6376 - val_acc: 0.5560\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0374 - acc: 0.9924 - val_loss: 1.0894 - val_acc: 0.6640\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0275 - acc: 0.9938 - val_loss: 1.2603 - val_acc: 0.6760\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0178 - acc: 0.9942 - val_loss: 0.7763 - val_acc: 0.7960\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0142 - acc: 0.9973 - val_loss: 1.3491 - val_acc: 0.6480\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0090 - acc: 0.9982 - val_loss: 1.3402 - val_acc: 0.6480\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0089 - acc: 0.9987 - val_loss: 0.7109 - val_acc: 0.8200\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0146 - acc: 0.9951 - val_loss: 1.6615 - val_acc: 0.6040\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0064 - acc: 0.9987 - val_loss: 1.2802 - val_acc: 0.6800\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0047 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.7320\n",
      "CPU times: user 22.3 s, sys: 888 ms, total: 23.2 s\n",
      "Wall time: 26.7 s\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 0.918\n",
      "Test accuracy: 0.842\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_easy, Y_train_easy, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_easy, Y_test_easy, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1\n",
    "# conclusion: easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAF5CAYAAADZMYNPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0186225630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### plot validation history ############\n",
    "def plot_validation_history(his, fig_path):\n",
    "    train_loss = his.history['loss']\n",
    "    val_loss = his.history['val_loss']\n",
    "\n",
    "    # visualize training history\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.show()\n",
    "plot_validation_history(his, \"baby_model_small_mammal_adam_with_decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 8s - loss: 11.8785 - acc: 0.1698 - val_loss: 4.3823 - val_acc: 0.2400\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 0s - loss: 3.1697 - acc: 0.2062 - val_loss: 2.1511 - val_acc: 0.2200\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 0s - loss: 2.0042 - acc: 0.1987 - val_loss: 3.1137 - val_acc: 0.2360\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.7974 - acc: 0.2093 - val_loss: 2.2253 - val_acc: 0.2440\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.7107 - acc: 0.2169 - val_loss: 2.7176 - val_acc: 0.3200\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6572 - acc: 0.2409 - val_loss: 2.1684 - val_acc: 0.3640\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5723 - acc: 0.2787 - val_loss: 1.9279 - val_acc: 0.3880\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4427 - acc: 0.3507 - val_loss: 1.6986 - val_acc: 0.3920\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3691 - acc: 0.3787 - val_loss: 1.8006 - val_acc: 0.4840\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3432 - acc: 0.3831 - val_loss: 1.6150 - val_acc: 0.4120\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3062 - acc: 0.4102 - val_loss: 1.5656 - val_acc: 0.4960\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2693 - acc: 0.4347 - val_loss: 1.4663 - val_acc: 0.4720\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2623 - acc: 0.4333 - val_loss: 1.4637 - val_acc: 0.4840\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2266 - acc: 0.4640 - val_loss: 1.3108 - val_acc: 0.5080\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2063 - acc: 0.4693 - val_loss: 1.3093 - val_acc: 0.4960\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1735 - acc: 0.4853 - val_loss: 1.3043 - val_acc: 0.5600\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1491 - acc: 0.4978 - val_loss: 1.2050 - val_acc: 0.5440\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1121 - acc: 0.5329 - val_loss: 1.1846 - val_acc: 0.5720\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0901 - acc: 0.5360 - val_loss: 1.1553 - val_acc: 0.5760\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0479 - acc: 0.5773 - val_loss: 1.1212 - val_acc: 0.6240\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0639 - acc: 0.5809 - val_loss: 1.2634 - val_acc: 0.5800\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1087 - acc: 0.5547 - val_loss: 1.1624 - val_acc: 0.5480\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0301 - acc: 0.5867 - val_loss: 1.0482 - val_acc: 0.5920\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0132 - acc: 0.5982 - val_loss: 1.1338 - val_acc: 0.5840\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0028 - acc: 0.6036 - val_loss: 1.0460 - val_acc: 0.6080\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9465 - acc: 0.6271 - val_loss: 1.0425 - val_acc: 0.6360\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9124 - acc: 0.6418 - val_loss: 1.0115 - val_acc: 0.6120\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8937 - acc: 0.6449 - val_loss: 0.9623 - val_acc: 0.6440\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8620 - acc: 0.6613 - val_loss: 1.0069 - val_acc: 0.6240\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8263 - acc: 0.6804 - val_loss: 0.9335 - val_acc: 0.6520\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8140 - acc: 0.6880 - val_loss: 0.9299 - val_acc: 0.6440\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7719 - acc: 0.7053 - val_loss: 0.8882 - val_acc: 0.6600\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7659 - acc: 0.7147 - val_loss: 0.9043 - val_acc: 0.6400\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7469 - acc: 0.7138 - val_loss: 0.8536 - val_acc: 0.6680\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7224 - acc: 0.7298 - val_loss: 0.8530 - val_acc: 0.6960\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6816 - acc: 0.7489 - val_loss: 0.9208 - val_acc: 0.6560\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7263 - acc: 0.7227 - val_loss: 0.9405 - val_acc: 0.6360\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7005 - acc: 0.7280 - val_loss: 0.9139 - val_acc: 0.6480\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7398 - acc: 0.7284 - val_loss: 0.8654 - val_acc: 0.7040\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6438 - acc: 0.7604 - val_loss: 0.8268 - val_acc: 0.6760\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6273 - acc: 0.7702 - val_loss: 0.7906 - val_acc: 0.7120\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5638 - acc: 0.7867 - val_loss: 0.7451 - val_acc: 0.6960\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5873 - acc: 0.7858 - val_loss: 0.7705 - val_acc: 0.7120\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5660 - acc: 0.7867 - val_loss: 0.8176 - val_acc: 0.6920\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5500 - acc: 0.7964 - val_loss: 0.7728 - val_acc: 0.7160\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5382 - acc: 0.7996 - val_loss: 0.7742 - val_acc: 0.7120\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5535 - acc: 0.7902 - val_loss: 0.8205 - val_acc: 0.7080\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5186 - acc: 0.8107 - val_loss: 0.8053 - val_acc: 0.6960\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4906 - acc: 0.8187 - val_loss: 0.7068 - val_acc: 0.7360\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4844 - acc: 0.8200 - val_loss: 0.7300 - val_acc: 0.7240\n",
      "CPU times: user 24.5 s, sys: 1.21 s, total: 25.7 s\n",
      "Wall time: 32.1 s\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 0.799\n",
      "Test accuracy: 0.706\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 7s - loss: 3.8204 - acc: 0.1418 - val_loss: 2.1051 - val_acc: 0.1920\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.9090 - acc: 0.2244 - val_loss: 1.6694 - val_acc: 0.1920\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6305 - acc: 0.2431 - val_loss: 1.5792 - val_acc: 0.3240\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5445 - acc: 0.3036 - val_loss: 1.5059 - val_acc: 0.3480\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4800 - acc: 0.3538 - val_loss: 1.4876 - val_acc: 0.4240\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6014 - acc: 0.3200 - val_loss: 1.5971 - val_acc: 0.2520\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5333 - acc: 0.3156 - val_loss: 1.4762 - val_acc: 0.3960\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4199 - acc: 0.4067 - val_loss: 1.4503 - val_acc: 0.3520\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5021 - acc: 0.3747 - val_loss: 1.4785 - val_acc: 0.3720\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4397 - acc: 0.3862 - val_loss: 1.4866 - val_acc: 0.3600\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4551 - acc: 0.3791 - val_loss: 1.4516 - val_acc: 0.3360\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3843 - acc: 0.4031 - val_loss: 1.4427 - val_acc: 0.4000\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3237 - acc: 0.4413 - val_loss: 1.3362 - val_acc: 0.4520\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2900 - acc: 0.4484 - val_loss: 1.3212 - val_acc: 0.4720\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2639 - acc: 0.4693 - val_loss: 1.3609 - val_acc: 0.4200\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2419 - acc: 0.4849 - val_loss: 1.3170 - val_acc: 0.4360\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2310 - acc: 0.4911 - val_loss: 1.2670 - val_acc: 0.4640\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1747 - acc: 0.5191 - val_loss: 1.2392 - val_acc: 0.5080\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1362 - acc: 0.5516 - val_loss: 1.2177 - val_acc: 0.5200\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0960 - acc: 0.5636 - val_loss: 1.1920 - val_acc: 0.5160\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0539 - acc: 0.5787 - val_loss: 1.1993 - val_acc: 0.5280\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0421 - acc: 0.5956 - val_loss: 1.1697 - val_acc: 0.5400\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9806 - acc: 0.6249 - val_loss: 1.2207 - val_acc: 0.5320\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9778 - acc: 0.6160 - val_loss: 1.3741 - val_acc: 0.4560\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9864 - acc: 0.6147 - val_loss: 1.2386 - val_acc: 0.4760\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8913 - acc: 0.6636 - val_loss: 1.3235 - val_acc: 0.5360\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8436 - acc: 0.6831 - val_loss: 1.2436 - val_acc: 0.5280\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7877 - acc: 0.6956 - val_loss: 1.2567 - val_acc: 0.5240\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7365 - acc: 0.7262 - val_loss: 1.4481 - val_acc: 0.5120\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.7303 - acc: 0.7298 - val_loss: 1.3176 - val_acc: 0.5360\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6115 - acc: 0.7871 - val_loss: 1.4405 - val_acc: 0.5360\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5707 - acc: 0.8071 - val_loss: 1.5784 - val_acc: 0.5280\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6064 - acc: 0.7671 - val_loss: 1.8190 - val_acc: 0.5200\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.6600 - acc: 0.7356 - val_loss: 1.5664 - val_acc: 0.5360\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.5346 - acc: 0.8151 - val_loss: 1.5945 - val_acc: 0.5360\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4522 - acc: 0.8422 - val_loss: 1.7466 - val_acc: 0.5040\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.4035 - acc: 0.8573 - val_loss: 1.8324 - val_acc: 0.5720\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.3048 - acc: 0.8938 - val_loss: 2.2002 - val_acc: 0.5040\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.3073 - acc: 0.8960 - val_loss: 1.8950 - val_acc: 0.5360\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2441 - acc: 0.9244 - val_loss: 2.2172 - val_acc: 0.5360\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2428 - acc: 0.9196 - val_loss: 2.3289 - val_acc: 0.5280\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2047 - acc: 0.9271 - val_loss: 2.2950 - val_acc: 0.5560\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2077 - acc: 0.9240 - val_loss: 2.2992 - val_acc: 0.5120\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2212 - acc: 0.9196 - val_loss: 2.6537 - val_acc: 0.4920\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.2396 - acc: 0.9187 - val_loss: 2.3755 - val_acc: 0.5080\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1810 - acc: 0.9342 - val_loss: 2.5687 - val_acc: 0.4960\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1716 - acc: 0.9431 - val_loss: 2.8212 - val_acc: 0.5360\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.1449 - acc: 0.9480 - val_loss: 2.5692 - val_acc: 0.5200\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0952 - acc: 0.9729 - val_loss: 2.7147 - val_acc: 0.5400\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.0699 - acc: 0.9787 - val_loss: 3.0236 - val_acc: 0.5400\n",
      "CPU times: user 22 s, sys: 1.03 s, total: 23 s\n",
      "Wall time: 26.6 s\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 3.732\n",
      "Test accuracy: 0.438\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_small_mammals, Y_train_small_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/50\n",
      "2250/2250 [==============================] - 8s - loss: 6.4185 - acc: 0.1378 - val_loss: 2.5306 - val_acc: 0.2600\n",
      "Epoch 2/50\n",
      "2250/2250 [==============================] - 0s - loss: 2.0252 - acc: 0.1791 - val_loss: 2.4422 - val_acc: 0.1800\n",
      "Epoch 3/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.7918 - acc: 0.1916 - val_loss: 2.3093 - val_acc: 0.1920\n",
      "Epoch 4/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.7061 - acc: 0.2031 - val_loss: 2.5917 - val_acc: 0.2160\n",
      "Epoch 5/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6665 - acc: 0.2258 - val_loss: 2.2088 - val_acc: 0.3040\n",
      "Epoch 6/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6259 - acc: 0.2342 - val_loss: 2.0317 - val_acc: 0.3200\n",
      "Epoch 7/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5613 - acc: 0.2951 - val_loss: 2.0679 - val_acc: 0.3200\n",
      "Epoch 8/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4996 - acc: 0.3471 - val_loss: 1.6087 - val_acc: 0.4040\n",
      "Epoch 9/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.6043 - acc: 0.3107 - val_loss: 2.2637 - val_acc: 0.3480\n",
      "Epoch 10/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.5197 - acc: 0.3280 - val_loss: 1.5672 - val_acc: 0.3880\n",
      "Epoch 11/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4965 - acc: 0.3653 - val_loss: 2.0372 - val_acc: 0.3600\n",
      "Epoch 12/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4623 - acc: 0.3787 - val_loss: 1.7255 - val_acc: 0.3680\n",
      "Epoch 13/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4243 - acc: 0.3858 - val_loss: 1.6738 - val_acc: 0.3680\n",
      "Epoch 14/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.4062 - acc: 0.3947 - val_loss: 1.8111 - val_acc: 0.3600\n",
      "Epoch 15/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3992 - acc: 0.4013 - val_loss: 1.7213 - val_acc: 0.3720\n",
      "Epoch 16/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3775 - acc: 0.4089 - val_loss: 1.5354 - val_acc: 0.3760\n",
      "Epoch 17/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3569 - acc: 0.4231 - val_loss: 1.5408 - val_acc: 0.4120\n",
      "Epoch 18/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3262 - acc: 0.4351 - val_loss: 1.4878 - val_acc: 0.4080\n",
      "Epoch 19/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3179 - acc: 0.4502 - val_loss: 1.5120 - val_acc: 0.4880\n",
      "Epoch 20/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2884 - acc: 0.4547 - val_loss: 1.4989 - val_acc: 0.3600\n",
      "Epoch 21/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.3208 - acc: 0.4480 - val_loss: 1.5671 - val_acc: 0.4360\n",
      "Epoch 22/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2910 - acc: 0.4560 - val_loss: 1.3839 - val_acc: 0.4560\n",
      "Epoch 23/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2654 - acc: 0.4698 - val_loss: 1.3840 - val_acc: 0.5000\n",
      "Epoch 24/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2518 - acc: 0.4724 - val_loss: 1.3649 - val_acc: 0.4840\n",
      "Epoch 25/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2444 - acc: 0.4631 - val_loss: 1.3645 - val_acc: 0.4360\n",
      "Epoch 26/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2146 - acc: 0.5040 - val_loss: 1.3569 - val_acc: 0.4520\n",
      "Epoch 27/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2183 - acc: 0.4996 - val_loss: 1.3133 - val_acc: 0.5080\n",
      "Epoch 28/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1762 - acc: 0.5169 - val_loss: 1.2901 - val_acc: 0.5200\n",
      "Epoch 29/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1816 - acc: 0.5080 - val_loss: 1.2860 - val_acc: 0.5400\n",
      "Epoch 30/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1463 - acc: 0.5209 - val_loss: 1.2388 - val_acc: 0.4920\n",
      "Epoch 31/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1388 - acc: 0.5293 - val_loss: 1.2728 - val_acc: 0.5200\n",
      "Epoch 32/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1438 - acc: 0.5200 - val_loss: 1.3036 - val_acc: 0.5120\n",
      "Epoch 33/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1614 - acc: 0.5156 - val_loss: 1.2443 - val_acc: 0.5200\n",
      "Epoch 34/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1206 - acc: 0.5422 - val_loss: 1.3105 - val_acc: 0.5560\n",
      "Epoch 35/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1331 - acc: 0.5364 - val_loss: 1.1981 - val_acc: 0.5320\n",
      "Epoch 36/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0835 - acc: 0.5680 - val_loss: 1.1763 - val_acc: 0.5560\n",
      "Epoch 37/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0784 - acc: 0.5680 - val_loss: 1.3155 - val_acc: 0.5160\n",
      "Epoch 38/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.2171 - acc: 0.4831 - val_loss: 1.4223 - val_acc: 0.5040\n",
      "Epoch 39/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1493 - acc: 0.5236 - val_loss: 1.2307 - val_acc: 0.5360\n",
      "Epoch 40/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.1296 - acc: 0.5391 - val_loss: 1.2845 - val_acc: 0.5360\n",
      "Epoch 41/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0893 - acc: 0.5422 - val_loss: 1.1680 - val_acc: 0.5480\n",
      "Epoch 42/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0223 - acc: 0.5933 - val_loss: 1.1463 - val_acc: 0.5800\n",
      "Epoch 43/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0493 - acc: 0.5716 - val_loss: 1.1971 - val_acc: 0.5400\n",
      "Epoch 44/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0271 - acc: 0.5889 - val_loss: 1.1852 - val_acc: 0.5680\n",
      "Epoch 45/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0133 - acc: 0.5947 - val_loss: 1.2808 - val_acc: 0.4680\n",
      "Epoch 46/50\n",
      "2250/2250 [==============================] - 0s - loss: 1.0150 - acc: 0.5827 - val_loss: 1.2052 - val_acc: 0.5240\n",
      "Epoch 47/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9814 - acc: 0.6022 - val_loss: 1.1816 - val_acc: 0.5200\n",
      "Epoch 48/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9574 - acc: 0.6156 - val_loss: 1.1657 - val_acc: 0.5560\n",
      "Epoch 49/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.9290 - acc: 0.6227 - val_loss: 1.1298 - val_acc: 0.5880\n",
      "Epoch 50/50\n",
      "2250/2250 [==============================] - 0s - loss: 0.8791 - acc: 0.6493 - val_loss: 1.1261 - val_acc: 0.5800\n",
      "CPU times: user 24.5 s, sys: 1.14 s, total: 25.6 s\n",
      "Wall time: 32.1 s\n",
      "320/500 [==================>...........] - ETA: 0s\n",
      "Test loss: 1.311\n",
      "Test accuracy: 0.506\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_small_mammals, Y_train_small_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "2250/2250 [==============================] - 8s - loss: 5.7057 - acc: 0.1142 - val_loss: 2.4621 - val_acc: 0.1760\n",
      "Epoch 2/100\n",
      "2250/2250 [==============================] - 0s - loss: 2.1227 - acc: 0.2009 - val_loss: 2.5097 - val_acc: 0.2160\n",
      "Epoch 3/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.8761 - acc: 0.2227 - val_loss: 2.3451 - val_acc: 0.1920\n",
      "Epoch 4/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.8241 - acc: 0.2004 - val_loss: 2.3053 - val_acc: 0.2360\n",
      "Epoch 5/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.7756 - acc: 0.2076 - val_loss: 2.2286 - val_acc: 0.1920\n",
      "Epoch 6/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.7465 - acc: 0.2164 - val_loss: 2.0658 - val_acc: 0.2200\n",
      "Epoch 7/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.7104 - acc: 0.2289 - val_loss: 1.9769 - val_acc: 0.2320\n",
      "Epoch 8/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.7019 - acc: 0.2293 - val_loss: 1.9009 - val_acc: 0.2240\n",
      "Epoch 9/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.7050 - acc: 0.2333 - val_loss: 1.8627 - val_acc: 0.2360\n",
      "Epoch 10/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6946 - acc: 0.2244 - val_loss: 1.9675 - val_acc: 0.2520\n",
      "Epoch 11/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6794 - acc: 0.2444 - val_loss: 1.7801 - val_acc: 0.2480\n",
      "Epoch 12/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6370 - acc: 0.2582 - val_loss: 1.7420 - val_acc: 0.2480\n",
      "Epoch 13/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6353 - acc: 0.2658 - val_loss: 1.8092 - val_acc: 0.2280\n",
      "Epoch 14/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6155 - acc: 0.2760 - val_loss: 1.6934 - val_acc: 0.2160\n",
      "Epoch 15/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6295 - acc: 0.2667 - val_loss: 1.7294 - val_acc: 0.2280\n",
      "Epoch 16/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6151 - acc: 0.2738 - val_loss: 1.7189 - val_acc: 0.2560\n",
      "Epoch 17/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6063 - acc: 0.2711 - val_loss: 1.7758 - val_acc: 0.2360\n",
      "Epoch 18/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6255 - acc: 0.2707 - val_loss: 1.6806 - val_acc: 0.2480\n",
      "Epoch 19/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5906 - acc: 0.2840 - val_loss: 1.6891 - val_acc: 0.2400\n",
      "Epoch 20/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5857 - acc: 0.2987 - val_loss: 1.7098 - val_acc: 0.2600\n",
      "Epoch 21/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5523 - acc: 0.2911 - val_loss: 1.6385 - val_acc: 0.2640\n",
      "Epoch 22/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5436 - acc: 0.3213 - val_loss: 1.6839 - val_acc: 0.2440\n",
      "Epoch 23/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5410 - acc: 0.3249 - val_loss: 1.6603 - val_acc: 0.2760\n",
      "Epoch 24/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5136 - acc: 0.3302 - val_loss: 1.6531 - val_acc: 0.3040\n",
      "Epoch 25/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5074 - acc: 0.3516 - val_loss: 1.6684 - val_acc: 0.3240\n",
      "Epoch 26/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4885 - acc: 0.3702 - val_loss: 1.6903 - val_acc: 0.2680\n",
      "Epoch 27/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4819 - acc: 0.3467 - val_loss: 1.6558 - val_acc: 0.3360\n",
      "Epoch 28/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4285 - acc: 0.4053 - val_loss: 1.6125 - val_acc: 0.2840\n",
      "Epoch 29/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4320 - acc: 0.4009 - val_loss: 1.6628 - val_acc: 0.3120\n",
      "Epoch 30/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4069 - acc: 0.4129 - val_loss: 1.6845 - val_acc: 0.2920\n",
      "Epoch 31/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4197 - acc: 0.4209 - val_loss: 1.6001 - val_acc: 0.2960\n",
      "Epoch 32/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.3593 - acc: 0.4276 - val_loss: 1.6375 - val_acc: 0.3040\n",
      "Epoch 33/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.3391 - acc: 0.4573 - val_loss: 1.6779 - val_acc: 0.2840\n",
      "Epoch 34/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2996 - acc: 0.4609 - val_loss: 1.6585 - val_acc: 0.3120\n",
      "Epoch 35/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2623 - acc: 0.4991 - val_loss: 1.6932 - val_acc: 0.2600\n",
      "Epoch 36/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2705 - acc: 0.4924 - val_loss: 1.7503 - val_acc: 0.3040\n",
      "Epoch 37/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2343 - acc: 0.5098 - val_loss: 1.7142 - val_acc: 0.3440\n",
      "Epoch 38/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.1758 - acc: 0.5369 - val_loss: 1.7338 - val_acc: 0.2800\n",
      "Epoch 39/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.1570 - acc: 0.5284 - val_loss: 1.7171 - val_acc: 0.2760\n",
      "Epoch 40/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0841 - acc: 0.5809 - val_loss: 1.7470 - val_acc: 0.2960\n",
      "Epoch 41/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0410 - acc: 0.5853 - val_loss: 1.9187 - val_acc: 0.2840\n",
      "Epoch 42/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0084 - acc: 0.5991 - val_loss: 1.8586 - val_acc: 0.3240\n",
      "Epoch 43/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0077 - acc: 0.6049 - val_loss: 2.0483 - val_acc: 0.2880\n",
      "Epoch 44/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9331 - acc: 0.6364 - val_loss: 1.8675 - val_acc: 0.2680\n",
      "Epoch 45/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9316 - acc: 0.6409 - val_loss: 2.0039 - val_acc: 0.2840\n",
      "Epoch 46/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.8605 - acc: 0.6680 - val_loss: 2.1318 - val_acc: 0.2840\n",
      "Epoch 47/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.8333 - acc: 0.6809 - val_loss: 2.1919 - val_acc: 0.2920\n",
      "Epoch 48/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7560 - acc: 0.7058 - val_loss: 2.0664 - val_acc: 0.3080\n",
      "Epoch 49/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7260 - acc: 0.7231 - val_loss: 2.3589 - val_acc: 0.2720\n",
      "Epoch 50/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7243 - acc: 0.7231 - val_loss: 2.1981 - val_acc: 0.2880\n",
      "Epoch 51/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.6694 - acc: 0.7493 - val_loss: 2.2150 - val_acc: 0.3240\n",
      "Epoch 52/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.6006 - acc: 0.7796 - val_loss: 2.5236 - val_acc: 0.2880\n",
      "Epoch 53/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5482 - acc: 0.7960 - val_loss: 2.5437 - val_acc: 0.2960\n",
      "Epoch 54/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5196 - acc: 0.8004 - val_loss: 2.7723 - val_acc: 0.2440\n",
      "Epoch 55/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4903 - acc: 0.8231 - val_loss: 2.7244 - val_acc: 0.2480\n",
      "Epoch 56/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5218 - acc: 0.8151 - val_loss: 2.8159 - val_acc: 0.2720\n",
      "Epoch 57/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4881 - acc: 0.8311 - val_loss: 2.5294 - val_acc: 0.2960\n",
      "Epoch 58/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4845 - acc: 0.8324 - val_loss: 3.1099 - val_acc: 0.2680\n",
      "Epoch 59/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4399 - acc: 0.8431 - val_loss: 2.5240 - val_acc: 0.2720\n",
      "Epoch 60/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3940 - acc: 0.8564 - val_loss: 2.9279 - val_acc: 0.2800\n",
      "Epoch 61/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4112 - acc: 0.8556 - val_loss: 2.8061 - val_acc: 0.3200\n",
      "Epoch 62/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4117 - acc: 0.8480 - val_loss: 2.9961 - val_acc: 0.2720\n",
      "Epoch 63/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3754 - acc: 0.8760 - val_loss: 2.7815 - val_acc: 0.2880\n",
      "Epoch 64/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2887 - acc: 0.8991 - val_loss: 3.0532 - val_acc: 0.2680\n",
      "Epoch 65/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2663 - acc: 0.9036 - val_loss: 3.1354 - val_acc: 0.2920\n",
      "Epoch 66/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2444 - acc: 0.9160 - val_loss: 3.3573 - val_acc: 0.3080\n",
      "Epoch 67/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2315 - acc: 0.9200 - val_loss: 3.8086 - val_acc: 0.2440\n",
      "Epoch 68/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2397 - acc: 0.9120 - val_loss: 3.7112 - val_acc: 0.2520\n",
      "Epoch 69/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2223 - acc: 0.9236 - val_loss: 3.8813 - val_acc: 0.2560\n",
      "Epoch 70/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2306 - acc: 0.9164 - val_loss: 3.6596 - val_acc: 0.2520\n",
      "Epoch 71/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1953 - acc: 0.9329 - val_loss: 3.6592 - val_acc: 0.2640\n",
      "Epoch 72/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1630 - acc: 0.9409 - val_loss: 4.1846 - val_acc: 0.2640\n",
      "Epoch 73/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2035 - acc: 0.9244 - val_loss: 4.1221 - val_acc: 0.2440\n",
      "Epoch 74/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1876 - acc: 0.9387 - val_loss: 3.7324 - val_acc: 0.2800\n",
      "Epoch 75/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1993 - acc: 0.9360 - val_loss: 3.9401 - val_acc: 0.2640\n",
      "Epoch 76/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1955 - acc: 0.9347 - val_loss: 3.9588 - val_acc: 0.2800\n",
      "Epoch 77/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1892 - acc: 0.9364 - val_loss: 3.6801 - val_acc: 0.2800\n",
      "Epoch 78/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1654 - acc: 0.9480 - val_loss: 3.7122 - val_acc: 0.2800\n",
      "Epoch 79/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1796 - acc: 0.9369 - val_loss: 4.1780 - val_acc: 0.2440\n",
      "Epoch 80/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2036 - acc: 0.9347 - val_loss: 3.6447 - val_acc: 0.3040\n",
      "Epoch 81/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1660 - acc: 0.9436 - val_loss: 4.0144 - val_acc: 0.2440\n",
      "Epoch 82/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1305 - acc: 0.9582 - val_loss: 4.3016 - val_acc: 0.2720\n",
      "Epoch 83/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1253 - acc: 0.9573 - val_loss: 4.5077 - val_acc: 0.2320\n",
      "Epoch 84/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1106 - acc: 0.9596 - val_loss: 4.6610 - val_acc: 0.2760\n",
      "Epoch 85/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0891 - acc: 0.9747 - val_loss: 4.8749 - val_acc: 0.2320\n",
      "Epoch 86/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1170 - acc: 0.9618 - val_loss: 4.6101 - val_acc: 0.2120\n",
      "Epoch 87/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1351 - acc: 0.9596 - val_loss: 4.3118 - val_acc: 0.3000\n",
      "Epoch 88/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1940 - acc: 0.9373 - val_loss: 4.3571 - val_acc: 0.2560\n",
      "Epoch 89/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2049 - acc: 0.9373 - val_loss: 3.8063 - val_acc: 0.2760\n",
      "Epoch 90/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2178 - acc: 0.9280 - val_loss: 3.6966 - val_acc: 0.2720\n",
      "Epoch 91/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1589 - acc: 0.9507 - val_loss: 3.5157 - val_acc: 0.2600\n",
      "Epoch 92/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1335 - acc: 0.9587 - val_loss: 3.7681 - val_acc: 0.2680\n",
      "Epoch 93/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1045 - acc: 0.9636 - val_loss: 4.1624 - val_acc: 0.2360\n",
      "Epoch 94/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1004 - acc: 0.9689 - val_loss: 4.5641 - val_acc: 0.2680\n",
      "Epoch 95/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1356 - acc: 0.9551 - val_loss: 4.3362 - val_acc: 0.2880\n",
      "Epoch 96/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0899 - acc: 0.9751 - val_loss: 4.2575 - val_acc: 0.2880\n",
      "Epoch 97/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1135 - acc: 0.9578 - val_loss: 4.7830 - val_acc: 0.2760\n",
      "Epoch 98/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0905 - acc: 0.9676 - val_loss: 4.7697 - val_acc: 0.3000\n",
      "Epoch 99/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0670 - acc: 0.9773 - val_loss: 4.8659 - val_acc: 0.2800\n",
      "Epoch 100/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0683 - acc: 0.9760 - val_loss: 4.7193 - val_acc: 0.2960\n",
      "CPU times: user 37 s, sys: 2.99 s, total: 40 s\n",
      "Wall time: 48.4 s\n",
      "320/500 [==================>...........] - ETA: 0s\n",
      "Test loss: 3.995\n",
      "Test accuracy: 0.358\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 100\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_people, Y_train_people, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "2250/2250 [==============================] - 8s - loss: 6.9747 - acc: 0.1280 - val_loss: 2.4747 - val_acc: 0.2760\n",
      "Epoch 2/100\n",
      "2250/2250 [==============================] - 0s - loss: 2.0207 - acc: 0.2502 - val_loss: 2.0579 - val_acc: 0.4320\n",
      "Epoch 3/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.6293 - acc: 0.3596 - val_loss: 1.7955 - val_acc: 0.3880\n",
      "Epoch 4/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.5324 - acc: 0.3889 - val_loss: 1.6091 - val_acc: 0.4040\n",
      "Epoch 5/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.4553 - acc: 0.3791 - val_loss: 1.7530 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.3619 - acc: 0.4338 - val_loss: 1.4055 - val_acc: 0.5360\n",
      "Epoch 7/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2935 - acc: 0.4738 - val_loss: 1.5732 - val_acc: 0.5360\n",
      "Epoch 8/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2662 - acc: 0.4818 - val_loss: 1.3786 - val_acc: 0.5480\n",
      "Epoch 9/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.2057 - acc: 0.5191 - val_loss: 1.3055 - val_acc: 0.5920\n",
      "Epoch 10/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.1309 - acc: 0.5502 - val_loss: 1.2594 - val_acc: 0.5640\n",
      "Epoch 11/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.1042 - acc: 0.5782 - val_loss: 1.2139 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0513 - acc: 0.5942 - val_loss: 1.1291 - val_acc: 0.6120\n",
      "Epoch 13/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0355 - acc: 0.6009 - val_loss: 1.1748 - val_acc: 0.6200\n",
      "Epoch 14/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0162 - acc: 0.6182 - val_loss: 1.1772 - val_acc: 0.5720\n",
      "Epoch 15/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9985 - acc: 0.6240 - val_loss: 1.1291 - val_acc: 0.6200\n",
      "Epoch 16/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9315 - acc: 0.6458 - val_loss: 1.0304 - val_acc: 0.5920\n",
      "Epoch 17/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9631 - acc: 0.6324 - val_loss: 1.1218 - val_acc: 0.5920\n",
      "Epoch 18/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9306 - acc: 0.6502 - val_loss: 1.1660 - val_acc: 0.5720\n",
      "Epoch 19/100\n",
      "2250/2250 [==============================] - 0s - loss: 1.0388 - acc: 0.6120 - val_loss: 1.1111 - val_acc: 0.6080\n",
      "Epoch 20/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.9489 - acc: 0.6333 - val_loss: 1.1065 - val_acc: 0.5880\n",
      "Epoch 21/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.8929 - acc: 0.6622 - val_loss: 0.9608 - val_acc: 0.6440\n",
      "Epoch 22/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.8395 - acc: 0.6804 - val_loss: 1.0525 - val_acc: 0.6080\n",
      "Epoch 23/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.8053 - acc: 0.6996 - val_loss: 0.9556 - val_acc: 0.6240\n",
      "Epoch 24/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7699 - acc: 0.7129 - val_loss: 0.9371 - val_acc: 0.6600\n",
      "Epoch 25/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7498 - acc: 0.7178 - val_loss: 0.9170 - val_acc: 0.6400\n",
      "Epoch 26/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7324 - acc: 0.7204 - val_loss: 0.9382 - val_acc: 0.6360\n",
      "Epoch 27/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7320 - acc: 0.7129 - val_loss: 0.9494 - val_acc: 0.6520\n",
      "Epoch 28/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.7130 - acc: 0.7462 - val_loss: 0.9166 - val_acc: 0.6440\n",
      "Epoch 29/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.6642 - acc: 0.7547 - val_loss: 0.9736 - val_acc: 0.6520\n",
      "Epoch 30/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.6371 - acc: 0.7587 - val_loss: 1.0099 - val_acc: 0.6320\n",
      "Epoch 31/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.6319 - acc: 0.7516 - val_loss: 0.9774 - val_acc: 0.6080\n",
      "Epoch 32/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5832 - acc: 0.7729 - val_loss: 0.9225 - val_acc: 0.6440\n",
      "Epoch 33/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5567 - acc: 0.7902 - val_loss: 0.9151 - val_acc: 0.6600\n",
      "Epoch 34/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5241 - acc: 0.8022 - val_loss: 1.0271 - val_acc: 0.6200\n",
      "Epoch 35/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.5389 - acc: 0.7911 - val_loss: 0.9388 - val_acc: 0.6600\n",
      "Epoch 36/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4777 - acc: 0.8253 - val_loss: 0.9221 - val_acc: 0.6720\n",
      "Epoch 37/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4633 - acc: 0.8293 - val_loss: 0.8863 - val_acc: 0.6840\n",
      "Epoch 38/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4388 - acc: 0.8387 - val_loss: 0.8993 - val_acc: 0.6600\n",
      "Epoch 39/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4347 - acc: 0.8422 - val_loss: 1.0384 - val_acc: 0.6520\n",
      "Epoch 40/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.4224 - acc: 0.8418 - val_loss: 0.8923 - val_acc: 0.6760\n",
      "Epoch 41/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3750 - acc: 0.8569 - val_loss: 0.9989 - val_acc: 0.6400\n",
      "Epoch 42/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3548 - acc: 0.8538 - val_loss: 1.0295 - val_acc: 0.6440\n",
      "Epoch 43/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3529 - acc: 0.8613 - val_loss: 1.0277 - val_acc: 0.6560\n",
      "Epoch 44/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3805 - acc: 0.8520 - val_loss: 1.1040 - val_acc: 0.6880\n",
      "Epoch 45/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3665 - acc: 0.8547 - val_loss: 1.1244 - val_acc: 0.6480\n",
      "Epoch 46/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3413 - acc: 0.8742 - val_loss: 0.9841 - val_acc: 0.6760\n",
      "Epoch 47/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.3170 - acc: 0.8813 - val_loss: 1.1485 - val_acc: 0.6560\n",
      "Epoch 48/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2939 - acc: 0.8916 - val_loss: 1.1288 - val_acc: 0.6560\n",
      "Epoch 49/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2602 - acc: 0.9031 - val_loss: 1.1821 - val_acc: 0.6240\n",
      "Epoch 50/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2455 - acc: 0.9089 - val_loss: 1.0471 - val_acc: 0.6680\n",
      "Epoch 51/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1960 - acc: 0.9298 - val_loss: 1.1784 - val_acc: 0.6680\n",
      "Epoch 52/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1882 - acc: 0.9276 - val_loss: 1.2547 - val_acc: 0.6600\n",
      "Epoch 53/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1823 - acc: 0.9324 - val_loss: 1.3552 - val_acc: 0.6320\n",
      "Epoch 54/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1794 - acc: 0.9342 - val_loss: 1.2683 - val_acc: 0.6560\n",
      "Epoch 55/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1744 - acc: 0.9378 - val_loss: 1.3462 - val_acc: 0.6440\n",
      "Epoch 56/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1676 - acc: 0.9382 - val_loss: 1.3914 - val_acc: 0.6240\n",
      "Epoch 57/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1817 - acc: 0.9316 - val_loss: 1.3845 - val_acc: 0.6600\n",
      "Epoch 58/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1922 - acc: 0.9333 - val_loss: 1.1388 - val_acc: 0.6840\n",
      "Epoch 59/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1603 - acc: 0.9516 - val_loss: 1.3348 - val_acc: 0.6680\n",
      "Epoch 60/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1418 - acc: 0.9453 - val_loss: 1.6562 - val_acc: 0.6440\n",
      "Epoch 61/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1156 - acc: 0.9538 - val_loss: 1.6079 - val_acc: 0.6360\n",
      "Epoch 62/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1239 - acc: 0.9511 - val_loss: 1.5879 - val_acc: 0.6800\n",
      "Epoch 63/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1221 - acc: 0.9609 - val_loss: 1.5099 - val_acc: 0.6680\n",
      "Epoch 64/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1087 - acc: 0.9622 - val_loss: 1.5860 - val_acc: 0.6760\n",
      "Epoch 65/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1364 - acc: 0.9516 - val_loss: 1.4012 - val_acc: 0.6760\n",
      "Epoch 66/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1365 - acc: 0.9480 - val_loss: 1.5666 - val_acc: 0.6320\n",
      "Epoch 67/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1657 - acc: 0.9404 - val_loss: 1.4810 - val_acc: 0.6360\n",
      "Epoch 68/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1258 - acc: 0.9520 - val_loss: 1.3241 - val_acc: 0.6560\n",
      "Epoch 69/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0939 - acc: 0.9662 - val_loss: 1.5364 - val_acc: 0.6520\n",
      "Epoch 70/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1045 - acc: 0.9653 - val_loss: 1.4338 - val_acc: 0.6680\n",
      "Epoch 71/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1041 - acc: 0.9640 - val_loss: 1.6915 - val_acc: 0.6600\n",
      "Epoch 72/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0983 - acc: 0.9640 - val_loss: 1.6040 - val_acc: 0.6360\n",
      "Epoch 73/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1212 - acc: 0.9591 - val_loss: 1.8167 - val_acc: 0.6560\n",
      "Epoch 74/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1285 - acc: 0.9529 - val_loss: 1.4330 - val_acc: 0.6600\n",
      "Epoch 75/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0903 - acc: 0.9676 - val_loss: 1.5441 - val_acc: 0.6800\n",
      "Epoch 76/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0848 - acc: 0.9676 - val_loss: 1.5183 - val_acc: 0.6640\n",
      "Epoch 77/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0593 - acc: 0.9822 - val_loss: 1.8237 - val_acc: 0.6560\n",
      "Epoch 78/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0589 - acc: 0.9769 - val_loss: 1.7593 - val_acc: 0.6640\n",
      "Epoch 79/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0496 - acc: 0.9840 - val_loss: 1.7259 - val_acc: 0.6800\n",
      "Epoch 80/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0550 - acc: 0.9804 - val_loss: 1.7172 - val_acc: 0.6760\n",
      "Epoch 81/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0433 - acc: 0.9827 - val_loss: 1.7813 - val_acc: 0.6680\n",
      "Epoch 82/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0370 - acc: 0.9844 - val_loss: 1.7422 - val_acc: 0.6720\n",
      "Epoch 83/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0251 - acc: 0.9907 - val_loss: 2.0826 - val_acc: 0.6640\n",
      "Epoch 84/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0454 - acc: 0.9836 - val_loss: 2.0211 - val_acc: 0.6800\n",
      "Epoch 85/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0543 - acc: 0.9813 - val_loss: 1.9042 - val_acc: 0.6680\n",
      "Epoch 86/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0611 - acc: 0.9809 - val_loss: 1.9562 - val_acc: 0.6720\n",
      "Epoch 87/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0672 - acc: 0.9747 - val_loss: 1.9129 - val_acc: 0.6440\n",
      "Epoch 88/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1051 - acc: 0.9662 - val_loss: 1.8306 - val_acc: 0.6520\n",
      "Epoch 89/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1744 - acc: 0.9498 - val_loss: 1.8177 - val_acc: 0.6280\n",
      "Epoch 90/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.2095 - acc: 0.9240 - val_loss: 1.5942 - val_acc: 0.6560\n",
      "Epoch 91/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1780 - acc: 0.9418 - val_loss: 1.3561 - val_acc: 0.6520\n",
      "Epoch 92/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1807 - acc: 0.9409 - val_loss: 1.5536 - val_acc: 0.6240\n",
      "Epoch 93/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1225 - acc: 0.9591 - val_loss: 1.6805 - val_acc: 0.6240\n",
      "Epoch 94/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.1109 - acc: 0.9649 - val_loss: 1.6760 - val_acc: 0.6560\n",
      "Epoch 95/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0920 - acc: 0.9689 - val_loss: 1.6023 - val_acc: 0.6440\n",
      "Epoch 96/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0540 - acc: 0.9827 - val_loss: 1.6967 - val_acc: 0.6680\n",
      "Epoch 97/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0599 - acc: 0.9809 - val_loss: 1.6436 - val_acc: 0.6680\n",
      "Epoch 98/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0433 - acc: 0.9844 - val_loss: 1.7948 - val_acc: 0.6840\n",
      "Epoch 99/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0382 - acc: 0.9884 - val_loss: 1.7999 - val_acc: 0.6600\n",
      "Epoch 100/100\n",
      "2250/2250 [==============================] - 0s - loss: 0.0445 - acc: 0.9862 - val_loss: 1.8581 - val_acc: 0.6880\n",
      "CPU times: user 37.4 s, sys: 2.83 s, total: 40.2 s\n",
      "Wall time: 48.6 s\n",
      "320/500 [==================>...........] - ETA: 0s\n",
      "Test loss: 1.786\n",
      "Test accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 100\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_fish, Y_train_fish, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_fish, Y_test_fish, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 100\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train_fish, Y_train_fish, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test_fish, Y_test_fish, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 16s - loss: 4.5502 - acc: 0.0268 - val_loss: 4.2715 - val_acc: 0.0538\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 9s - loss: 4.0770 - acc: 0.0674 - val_loss: 4.0522 - val_acc: 0.0970\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 9s - loss: 3.8119 - acc: 0.1080 - val_loss: 3.8928 - val_acc: 0.1142\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 9s - loss: 3.5905 - acc: 0.1454 - val_loss: 3.5579 - val_acc: 0.1708\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 9s - loss: 3.3953 - acc: 0.1779 - val_loss: 3.4953 - val_acc: 0.1766\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 9s - loss: 3.2390 - acc: 0.2060 - val_loss: 3.3916 - val_acc: 0.1874\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 9s - loss: 3.0817 - acc: 0.2346 - val_loss: 3.2254 - val_acc: 0.2230\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.9682 - acc: 0.2562 - val_loss: 3.1442 - val_acc: 0.2404\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.8348 - acc: 0.2799 - val_loss: 3.0875 - val_acc: 0.2476\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.7430 - acc: 0.3004 - val_loss: 2.9629 - val_acc: 0.2666\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.6542 - acc: 0.3222 - val_loss: 3.0224 - val_acc: 0.2606\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.5516 - acc: 0.3406 - val_loss: 2.7938 - val_acc: 0.3008\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.4921 - acc: 0.3540 - val_loss: 2.8724 - val_acc: 0.2868\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.3904 - acc: 0.3723 - val_loss: 2.7711 - val_acc: 0.3036\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.3420 - acc: 0.3816 - val_loss: 2.7703 - val_acc: 0.3004\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.2627 - acc: 0.3998 - val_loss: 2.7689 - val_acc: 0.3120\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.2248 - acc: 0.4098 - val_loss: 2.7230 - val_acc: 0.3144\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.1585 - acc: 0.4223 - val_loss: 2.6201 - val_acc: 0.3294\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.0932 - acc: 0.4352 - val_loss: 2.5706 - val_acc: 0.3484\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 9s - loss: 2.0412 - acc: 0.4491 - val_loss: 2.5684 - val_acc: 0.3526\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.9875 - acc: 0.4581 - val_loss: 2.5018 - val_acc: 0.3598\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.9549 - acc: 0.4632 - val_loss: 2.5693 - val_acc: 0.3466\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.9241 - acc: 0.4747 - val_loss: 2.5526 - val_acc: 0.3552\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.8467 - acc: 0.4895 - val_loss: 2.5829 - val_acc: 0.3544\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.8256 - acc: 0.4928 - val_loss: 2.5572 - val_acc: 0.3548\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.7783 - acc: 0.5080 - val_loss: 2.5054 - val_acc: 0.3750\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.7431 - acc: 0.5139 - val_loss: 2.5496 - val_acc: 0.3532\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.7051 - acc: 0.5212 - val_loss: 2.5624 - val_acc: 0.3664\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.6756 - acc: 0.5286 - val_loss: 2.5176 - val_acc: 0.3734\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.6422 - acc: 0.5355 - val_loss: 2.5493 - val_acc: 0.3630\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.5808 - acc: 0.5526 - val_loss: 2.5624 - val_acc: 0.3640\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.5693 - acc: 0.5558 - val_loss: 2.6248 - val_acc: 0.3606\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.5310 - acc: 0.5601 - val_loss: 2.6346 - val_acc: 0.3594\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.5063 - acc: 0.5684 - val_loss: 2.5356 - val_acc: 0.3758\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.4847 - acc: 0.5722 - val_loss: 2.5813 - val_acc: 0.3662\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.4522 - acc: 0.5828 - val_loss: 2.5919 - val_acc: 0.3722\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.4264 - acc: 0.5898 - val_loss: 2.6868 - val_acc: 0.3484\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3844 - acc: 0.5992 - val_loss: 2.6423 - val_acc: 0.3708\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3743 - acc: 0.5986 - val_loss: 2.6945 - val_acc: 0.3602\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3670 - acc: 0.6014 - val_loss: 2.6742 - val_acc: 0.3692\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3317 - acc: 0.6083 - val_loss: 2.7317 - val_acc: 0.3676\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3108 - acc: 0.6158 - val_loss: 2.7759 - val_acc: 0.3634\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.3072 - acc: 0.6168 - val_loss: 2.6584 - val_acc: 0.3794\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.2673 - acc: 0.6272 - val_loss: 2.7263 - val_acc: 0.3654\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.2375 - acc: 0.6365 - val_loss: 2.7661 - val_acc: 0.3626\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.2447 - acc: 0.6316 - val_loss: 2.7585 - val_acc: 0.3700\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.2289 - acc: 0.6376 - val_loss: 2.7556 - val_acc: 0.3710\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.1840 - acc: 0.6472 - val_loss: 2.7855 - val_acc: 0.3668\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.1739 - acc: 0.6502 - val_loss: 2.7805 - val_acc: 0.3690\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 9s - loss: 1.1533 - acc: 0.6572 - val_loss: 2.7800 - val_acc: 0.3734\n",
      "CPU times: user 5min 6s, sys: 41.9 s, total: 5min 48s\n",
      "Wall time: 7min 52s\n",
      "10000/10000 [==============================] - 1s     \n",
      "\n",
      "Test loss: 2.729\n",
      "Test accuracy: 0.385\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GaussianNoise, MaxoutDense\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "# here are some settings for my network\n",
    "# batchsize 128 doesn't work\n",
    "batch_size = 512\n",
    "nb_epoch = 50\n",
    "# load data\n",
    "\n",
    "img_chhannels = 3\n",
    "img_size = 32\n",
    "img_size = 32\n",
    "nb_classes = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_size, img_size, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# let's train the model using SGD + momentum\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.99)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "%time his = model.fit(X_train, Y_train, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.1, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.8) batch_size = 256 -> train 0.89, val -> 0\n",
    "# validation_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncEMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00ef712dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### plot validation history ############\n",
    "def plot_validation_history(his, fig_path):\n",
    "    train_loss = his.history['loss']\n",
    "    val_loss = his.history['val_loss']\n",
    "\n",
    "    # visualize training history\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.show()\n",
    "plot_validation_history(his, \"baby_model_small_mammal_adam_with_decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s     \n",
      "\n",
      "people\n",
      "\n",
      "Test loss: 14.895\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "small mammals\n",
      "\n",
      "Test loss: 1.512\n",
      "Test accuracy: 0.474\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "medium mammal\n",
      "\n",
      "Test loss: 15.144\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "aquatic\n",
      "\n",
      "Test loss: 14.809\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "fish\n",
      "\n",
      "Test loss: 14.674\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# sample_images(X_test_people)\n",
    "# sample_images(X_test_small_mammals)\n",
    "# sample_images(X_test_medium_sized_mammals)\n",
    "# sample_images(X_test_aquatic_mammals)\n",
    "# sample_images(X_test_fish)\n",
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\npeople')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nsmall mammals')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nmedium mammal')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "print('\\naquatic')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_fish, Y_test_fish, verbose=1)\n",
    "print('\\nfish')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@TODO this is weird...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f32775fb748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### plot validation history ############\n",
    "def plot_validation_history(his, fig_path):\n",
    "    train_loss = his.history['loss']\n",
    "    val_loss = his.history['val_loss']\n",
    "\n",
    "    # visualize training history\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, color='blue', label='Train loss')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, color='red', label='Val loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.show()\n",
    "import lib\n",
    "plot_validation_history(his, \"baby_model_people_sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 2s - loss: 2.4091 - acc: 0.1720 - val_loss: 1.6095 - val_acc: 0.1900\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1930 - val_loss: 1.6095 - val_acc: 0.2040\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2105 - val_loss: 1.6095 - val_acc: 0.1940\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2105 - val_loss: 1.6095 - val_acc: 0.1920\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1905 - val_loss: 1.6095 - val_acc: 0.1940\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2005 - val_loss: 1.6095 - val_acc: 0.1940\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1850 - val_loss: 1.6095 - val_acc: 0.1980\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2070 - val_loss: 1.6095 - val_acc: 0.1980\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2030 - val_loss: 1.6095 - val_acc: 0.1960\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1980 - val_loss: 1.6095 - val_acc: 0.1940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2055 - val_loss: 1.6095 - val_acc: 0.2000\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1950 - val_loss: 1.6095 - val_acc: 0.2040\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.2060 - val_loss: 1.6095 - val_acc: 0.2040\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1965 - val_loss: 1.6094 - val_acc: 0.2100\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1920 - val_loss: 1.6094 - val_acc: 0.2040\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6095 - acc: 0.1980 - val_loss: 1.6094 - val_acc: 0.2060\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2095 - val_loss: 1.6094 - val_acc: 0.1980\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2030 - val_loss: 1.6094 - val_acc: 0.1940\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.1975 - val_loss: 1.6094 - val_acc: 0.1980\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2015 - val_loss: 1.6094 - val_acc: 0.1960\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.1965 - val_loss: 1.6094 - val_acc: 0.1960\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2030 - val_loss: 1.6094 - val_acc: 0.2020\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2075 - val_loss: 1.6094 - val_acc: 0.1980\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.1970 - val_loss: 1.6094 - val_acc: 0.2060\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2065 - val_loss: 1.6094 - val_acc: 0.2020\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.1975 - val_loss: 1.6094 - val_acc: 0.2020\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2070 - val_loss: 1.6094 - val_acc: 0.2020\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2010 - val_loss: 1.6094 - val_acc: 0.1980\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.2040 - val_loss: 1.6094 - val_acc: 0.1940\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 2s - loss: 1.6094 - acc: 0.1995 - val_loss: 1.6094 - val_acc: 0.1940\n",
      "CPU times: user 1min 15s, sys: 8.14 s, total: 1min 24s\n",
      "Wall time: 1min 22s\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 1.609\n",
      "Test accuracy: 0.206\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 30\n",
    "\n",
    "%time his_small_mammals = model_baby.fit(X_train_people, Y_train_people, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          verbose=True, \\\n",
    "          shuffle=True) \\\n",
    "# learncall back\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "# score = model_baby.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "# print('\\nTest loss: %.3f' % score[0])\n",
    "# print('Test accuracy: %.3f' % score[1])\n",
    "# print(model_baby.predict(X_test_medium_sized_mammals, batch_size=32, verbose=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s     \n",
      "\n",
      "people\n",
      "\n",
      "Test loss: 1.609\n",
      "Test accuracy: 0.206\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "small mammals\n",
      "\n",
      "Test loss: 13.664\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "medium mammal\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "aquatic\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "fish\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\npeople')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nsmall mammals')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nmedium mammal')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "print('\\naquatic')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_fish, Y_test_fish, verbose=1)\n",
    "print('\\nfish')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-537f8e470a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 1: Random initialize noise image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0minput_img_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_init_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcheck_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check_' is not defined"
     ]
    }
   ],
   "source": [
    "def random_init_image(img_width, img_height):\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "    return input_img_data\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width, img_height = X_train[0].shape[1], X_train[0].shape[2]\n",
    "\n",
    "\n",
    "# Step 1: Random initialize noise image\n",
    "input_img_data = random_init_image(img_width, img_height)\n",
    "check_(input_img_data)\n",
    "plt.figure()\n",
    "plt.imshow(toimage(input_img_data[0]))\n",
    "\n",
    "layer_tensor = model_baby.layer_tensors[2]\n",
    "layer_tensor\n",
    "\n",
    "layer_tensor = model_baby.layer_tensors[6]\n",
    "X_distorted = []\n",
    "\n",
    "for index, img in enumerate(X_train_people[:2]):\n",
    "    img_result = optimize_image(layer_tensor, input_img_data,\n",
    "                                num_iterations=10, step_size=6.0, tile_size=400,\n",
    "                                show_gradient=False)\n",
    "    # save_image(img_result, filename='deep_dream_cifar10/deepdream_cifar10_'+str(index)+'__without_tile,jpg')\n",
    "    X_distorted.append(img_result)\n",
    "    print(index+4000)\n",
    "    import pickle\n",
    "    if (index+1) % 500 == 0:\n",
    "        print(\"save 500\")\n",
    "        with open('dreamed_image_4000_'+str(index+4000)+'.p', 'wb') as f:\n",
    "            pickle.dump(X_distorted, f) \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "y_pred = model_baby.predict(X_test_medium_sized_mammals, batch_size=32, verbose=1)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, Y_test_class_medium_sized_mammals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.2 s, sys: 4.52 s, total: 46.7 s\n",
      "Wall time: 46.4 s\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 1.609\n",
      "Test accuracy: 0.200\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "%time his_medium_sized_mammals = model_baby.fit(X_train_medium_sized_mammals, Y_train_medium_sized_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          verbose=False, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s     \n",
      "\n",
      "people\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "500/500 [==============================] - 0s     \n",
      "\n",
      "small mammals\n",
      "\n",
      "Test loss: 14.357\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "medium mammal\n",
      "\n",
      "Test loss: 1.609\n",
      "Test accuracy: 0.200\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "aquatic\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n",
      "352/500 [====================>.........] - ETA: 0s\n",
      "fish\n",
      "\n",
      "Test loss: 16.118\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "score = model_baby.evaluate(X_test_people, Y_test_people, verbose=1)\n",
    "print('\\npeople')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_small_mammals, Y_test_small_mammals, verbose=1)\n",
    "print('\\nsmall mammals')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_medium_sized_mammals, Y_test_medium_sized_mammals, verbose=1)\n",
    "print('\\nmedium mammal')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "print('\\naquatic')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_test_fish, Y_test_fish, verbose=1)\n",
    "print('\\nfish')\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.9 s, sys: 9.07 s, total: 45 s\n",
      "Wall time: 45 s\n",
      "480/500 [===========================>..] - ETA: 0s\n",
      "Test loss: 0.028\n",
      "Test accuracy: 0.988\n",
      "2496/2500 [============================>.] - ETA: 0s\n",
      "Test loss: 0.169\n",
      "Test accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "%time his_aquatic_mammals = model_baby.fit(X_train_aquatic_mammals, Y_train_aquatic_mammals, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          verbose=False, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_test_aquatic_mammals, Y_test_aquatic_mammals, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])\n",
    "score = model_baby.evaluate(X_train_fish, Y_train_fish, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.6 s, sys: 11.7 s, total: 57.3 s\n",
      "Wall time: 57.3 s\n",
      "2496/2500 [============================>.] - ETA: 0s\n",
      "Test loss: 0.022\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "%time his_aquatic_mammals = model_baby.fit(X_train_fish, Y_train_fish, \\\n",
    "          batch_size=batch_size, \\\n",
    "          nb_epoch=nb_epoch, \\\n",
    "          validation_split=0.2, \\\n",
    "          callbacks=[early_stop], \\\n",
    "          verbose=False, \\\n",
    "          shuffle=True) \\\n",
    "\n",
    "\n",
    "# evaluate our model\n",
    "score = model_baby.evaluate(X_train_fish, Y_train_fish, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496/2500 [============================>.] - ETA: 0s\n",
      "Test loss: 0.086\n",
      "Test accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "# evaluate our model with all data of the five coarse category\n",
    "score = model_baby.evaluate(X_test_five, Y_test_five, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate our model with all data of the five coarse category\n",
    "score = model_baby.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 0.131\n",
      "Test accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "# evaluate our model with all data of the five coarse category\n",
    "score = model_baby.evaluate(X_test, Y_test, verbose=1)\n",
    "print('\\nTest loss: %.3f' % score[0])\n",
    "print('Test accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Some Predictions\n",
    "\n",
    "Note that the images look a little weird because their mean RGB values have been subtracted away."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:old-keras]",
   "language": "python",
   "name": "conda-env-old-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
